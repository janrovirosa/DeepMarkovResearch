{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# Go/No-Go Diagnostic Experiment\n",
    "\n",
    "**Goal:** Determine whether there is *learnable conditional signal* in our current setup,\n",
    "and whether \"marginal collapse\" is due to discretization + imbalance + objective rather than a true lack of signal.\n",
    "\n",
    "- **Part 1:** Sanity diagnostics (MI + smoothed baselines, NO neural nets)\n",
    "- **Part 2:** Bin-count ablation (`n_bins` in {25, 35, 40, 55})\n",
    "- **Part 3:** Minimal neural check (only if Part 2 shows promise)\n",
    "- **Part 4:** Final conclusion + save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os, json, warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "EPS = 1e-10\n",
    "\n",
    "print(\"Imports ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-data",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Copied from `TransitionProbMatrix_NEWDATA.ipynb` (cells `8b3d1378`, `414278a2`, `3c86ff39`).\n",
    "Same dataset, same split, same preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2368, n_features: 195, n_states: 55\n",
      "Train: 1657, Val: 355, Test: 356\n",
      "Train indices: [0, 1656]\n",
      "Val   indices: [1657, 2011]\n",
      "Test  indices: [2012, 2367]\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# DATA LOADING — from TransitionProbMatrix_NEWDATA.ipynb\n",
    "# ====================================================================\n",
    "train_df  = pd.read_csv(\"dataset/train_diagnostic.csv\")\n",
    "labels_df = pd.read_csv(\"dataset/label_diagnostic.csv\")\n",
    "\n",
    "# Compute forward percent change from Price\n",
    "train_df[\"Percent_change_forward\"] = (\n",
    "    train_df[\"Price\"].shift(-1) / train_df[\"Price\"] - 1\n",
    ") * 100.0\n",
    "\n",
    "# Drop last row (forward return undefined)\n",
    "train_df = train_df.iloc[:-1].copy()\n",
    "labels_df = labels_df.iloc[:-1].copy()\n",
    "\n",
    "# Drop Opinion column (NaN)\n",
    "train_df = train_df.drop(columns=[\"Opinion\"], errors=\"ignore\")\n",
    "\n",
    "# Feature columns\n",
    "drop_cols = [\"index\", \"Percent_change_forward\", \"Backward_Bin\"]\n",
    "feature_cols = [c for c in train_df.columns if c not in drop_cols]\n",
    "X_all = train_df[feature_cols].values.astype(np.float32)\n",
    "\n",
    "# States: 0-based\n",
    "s_curr_all = (train_df[\"Backward_Bin\"].values.astype(np.int64) - 1)\n",
    "y_all      = (labels_df[\"Forward_Bin\"].values.astype(np.int64) - 1)\n",
    "\n",
    "# Raw percent changes (for rebinning in Part 2)\n",
    "pct_backward_all = train_df[\"Percent_change_backward\"].values.astype(np.float64)\n",
    "pct_forward_all  = train_df[\"Percent_change_forward\"].values.astype(np.float64)\n",
    "\n",
    "n_samples, n_features = X_all.shape\n",
    "n_states = int(max(s_curr_all.max(), y_all.max()) + 1)\n",
    "\n",
    "# ====================================================================\n",
    "# TEMPORAL SPLIT: 70 / 15 / 15 (same as TransitionProbMatrix_NEWDATA)\n",
    "# ====================================================================\n",
    "T = n_samples\n",
    "train_end = int(0.7 * T)\n",
    "val_end   = int(0.85 * T)\n",
    "\n",
    "idx_train = np.arange(0,         train_end)\n",
    "idx_val   = np.arange(train_end, val_end)\n",
    "idx_test  = np.arange(val_end,   T)\n",
    "\n",
    "s_train, s_val, s_test = s_curr_all[idx_train], s_curr_all[idx_val], s_curr_all[idx_test]\n",
    "y_train, y_val, y_test = y_all[idx_train], y_all[idx_val], y_all[idx_test]\n",
    "\n",
    "X_train = X_all[idx_train]\n",
    "X_val   = X_all[idx_val]\n",
    "X_test  = X_all[idx_test]\n",
    "\n",
    "# Standardize features (train stats only)\n",
    "mean_feat = X_train.mean(axis=0, keepdims=True)\n",
    "std_feat  = X_train.std(axis=0, keepdims=True) + 1e-8\n",
    "X_train_std = (X_train - mean_feat) / std_feat\n",
    "X_val_std   = (X_val   - mean_feat) / std_feat\n",
    "X_test_std  = (X_test  - mean_feat) / std_feat\n",
    "\n",
    "print(f\"n_samples: {n_samples}, n_features: {n_features}, n_states: {n_states}\")\n",
    "print(f\"Train: {len(idx_train)}, Val: {len(idx_val)}, Test: {len(idx_test)}\")\n",
    "print(f\"Train indices: [{idx_train[0]}, {idx_train[-1]}]\")\n",
    "print(f\"Val   indices: [{idx_val[0]}, {idx_val[-1]}]\")\n",
    "print(f\"Test  indices: [{idx_test[0]}, {idx_test[-1]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-part1",
   "metadata": {},
   "source": [
    "## Part 1 — Sanity Diagnostics (NO neural nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-helpers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers defined.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ====================================================================\n",
    "\n",
    "def compute_marginal(y, n_classes):\n",
    "    \"\"\"Compute marginal distribution from integer labels.\"\"\"\n",
    "    counts = np.bincount(y, minlength=n_classes).astype(np.float64)\n",
    "    return counts / counts.sum()\n",
    "\n",
    "\n",
    "def compute_conditional(s, y, n_x, n_y, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Compute smoothed conditional P(y | x) via Laplace smoothing.\n",
    "    C_alpha[x, y] = C[x, y] + alpha\n",
    "    P(y|x) = C_alpha[x, y] / sum_y' C_alpha[x, y']\n",
    "    \n",
    "    Returns: P (n_x, n_y), raw_counts C (n_x, n_y)\n",
    "    \"\"\"\n",
    "    C = np.zeros((n_x, n_y), dtype=np.float64)\n",
    "    for si, yi in zip(s, y):\n",
    "        C[si, yi] += 1\n",
    "    C_alpha = C + alpha\n",
    "    P = C_alpha / C_alpha.sum(axis=1, keepdims=True)\n",
    "    return P, C\n",
    "\n",
    "\n",
    "def evaluate_distribution(pred_dist, y_true, n_classes, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate predicted distributions vs true labels.\n",
    "    pred_dist: (N, n_classes)  y_true: (N,)\n",
    "    Returns dict with: mean_ll, accuracy, severity.\n",
    "    \n",
    "    Severity definition matches TransitionProbMatrix_NEWDATA.ipynb:\n",
    "      severity = mean(|E_p[bin] - y_true|)\n",
    "    where E_p[bin] = sum_j p[j] * j\n",
    "    \"\"\"\n",
    "    N = len(y_true)\n",
    "    probs_true = pred_dist[np.arange(N), y_true]\n",
    "    mean_ll = np.log(probs_true + EPS).mean()\n",
    "    accuracy = (pred_dist.argmax(axis=1) == y_true).mean()\n",
    "    bins = np.arange(n_classes, dtype=np.float64)\n",
    "    expected_bins = (pred_dist * bins[np.newaxis, :]).sum(axis=1)\n",
    "    severity = np.abs(expected_bins - y_true.astype(np.float64)).mean()\n",
    "    return {\"label\": label, \"mean_ll\": mean_ll, \"accuracy\": accuracy,\n",
    "            \"severity\": severity, \"n\": N}\n",
    "\n",
    "\n",
    "def compute_mi(s, y, n_x, n_y, eps=1e-10):\n",
    "    \"\"\"Compute MI(X; Y) from integer arrays with epsilon smoothing.\"\"\"\n",
    "    C = np.zeros((n_x, n_y), dtype=np.float64)\n",
    "    for si, yi in zip(s, y):\n",
    "        C[si, yi] += 1\n",
    "    P_joint = (C + eps)\n",
    "    P_joint = P_joint / P_joint.sum()\n",
    "    P_x = P_joint.sum(axis=1)\n",
    "    P_y = P_joint.sum(axis=0)\n",
    "    mi = 0.0\n",
    "    for i in range(n_x):\n",
    "        for j in range(n_y):\n",
    "            if P_joint[i, j] > 0:\n",
    "                mi += P_joint[i, j] * np.log(P_joint[i, j] / (P_x[i] * P_y[j]))\n",
    "    return mi\n",
    "\n",
    "\n",
    "print(\"Helpers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-part1-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal: mode=bin 26, max_prob=0.0416, entropy=3.7083 nats\n",
      "\n",
      "State coverage (train): 53/55 occupied, 11 with <5 samples\n",
      "  Count distribution: min=1, median=35, max=69\n",
      "\n",
      "==========================================================================================\n",
      "PART 1: BASELINE RESULTS (n_states=55)\n",
      "==========================================================================================\n",
      "      label    alpha split   mean_ll  accuracy  severity   n\n",
      "   Marginal        -   val -3.682208  0.047887  8.101643 355\n",
      "Conditional 0.100000   val -4.125278  0.025352  8.185760 355\n",
      "Conditional 1.000000   val -3.882289  0.025352  8.144085 355\n",
      "   Marginal        -  test -3.692749  0.042135  8.266910 356\n",
      "Conditional 0.100000  test -4.184170  0.042135  8.294956 356\n",
      "Conditional 1.000000  test -3.891563  0.042135  8.317989 356\n",
      "==========================================================================================\n",
      "\n",
      "VAL: Best conditional - Marginal = -0.200081 nats  *** DOES NOT beat marginal ***\n",
      "\n",
      "TEST: Best conditional - Marginal = -0.198814 nats  *** DOES NOT beat marginal ***\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# PART 1: COMPUTE & EVALUATE BASELINES\n",
    "# ====================================================================\n",
    "\n",
    "# 1. Marginal baseline (train only)\n",
    "marginal_dist = compute_marginal(y_train, n_states)\n",
    "entropy = -np.sum(marginal_dist * np.log(marginal_dist + EPS))\n",
    "print(f\"Marginal: mode=bin {marginal_dist.argmax()}, \"\n",
    "      f\"max_prob={marginal_dist.max():.4f}, entropy={entropy:.4f} nats\")\n",
    "\n",
    "# 2. Smoothed conditional baselines\n",
    "P_cond_01, C_counts = compute_conditional(s_train, y_train, n_states, n_states, alpha=0.1)\n",
    "P_cond_10, _        = compute_conditional(s_train, y_train, n_states, n_states, alpha=1.0)\n",
    "\n",
    "# State coverage diagnostics\n",
    "state_counts_train = np.bincount(s_train, minlength=n_states)\n",
    "print(f\"\\nState coverage (train): {(state_counts_train > 0).sum()}/{n_states} occupied, \"\n",
    "      f\"{(state_counts_train < 5).sum()} with <5 samples\")\n",
    "print(f\"  Count distribution: min={state_counts_train[state_counts_train>0].min()}, \"\n",
    "      f\"median={np.median(state_counts_train[state_counts_train>0]):.0f}, \"\n",
    "      f\"max={state_counts_train.max()}\")\n",
    "\n",
    "# 3. Evaluate on VAL and TEST\n",
    "results_part1 = []\n",
    "\n",
    "for split_name, s_sp, y_sp in [(\"val\", s_val, y_val), (\"test\", s_test, y_test)]:\n",
    "    N = len(y_sp)\n",
    "    pred_m = np.tile(marginal_dist[np.newaxis, :], (N, 1))\n",
    "    r = evaluate_distribution(pred_m, y_sp, n_states, f\"Marginal\")\n",
    "    r[\"alpha\"] = \"-\"; r[\"split\"] = split_name\n",
    "    results_part1.append(r)\n",
    "\n",
    "    for alpha_val, P_cond in [(0.1, P_cond_01), (1.0, P_cond_10)]:\n",
    "        pred_c = P_cond[s_sp]\n",
    "        r = evaluate_distribution(pred_c, y_sp, n_states, f\"Conditional\")\n",
    "        r[\"alpha\"] = alpha_val; r[\"split\"] = split_name\n",
    "        results_part1.append(r)\n",
    "\n",
    "df1 = pd.DataFrame(results_part1)[[\"label\", \"alpha\", \"split\", \"mean_ll\", \"accuracy\", \"severity\", \"n\"]]\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"PART 1: BASELINE RESULTS (n_states=55)\")\n",
    "print(\"=\"*90)\n",
    "print(df1.to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Flag conditional vs marginal\n",
    "for sp in [\"val\", \"test\"]:\n",
    "    m_ll = df1[(df1[\"split\"]==sp) & (df1[\"label\"]==\"Marginal\")][\"mean_ll\"].values[0]\n",
    "    c_ll = df1[(df1[\"split\"]==sp) & (df1[\"label\"]==\"Conditional\")][\"mean_ll\"].max()\n",
    "    delta = c_ll - m_ll\n",
    "    print(f\"\\n{sp.upper()}: Best conditional - Marginal = {delta:+.6f} nats\"\n",
    "          f\"  {'BEATS marginal' if delta > 0 else '*** DOES NOT beat marginal ***'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-part1-mi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI(Backward_Bin, Forward_Bin) on TRAIN:\n",
      "  MI = 0.659427 nats = 0.951352 bits\n",
      "  Shuffled [0] = 0.617008 nats\n",
      "  Shuffled [1] = 0.637256 nats\n",
      "  Shuffled [2] = 0.628616 nats\n",
      "  Shuffled [3] = 0.638155 nats\n",
      "  Shuffled [4] = 0.630453 nats\n",
      "\n",
      "  Shuffled mean = 0.630298, std = 0.007611\n",
      "  Real MI / Shuffled mean = 1.0x\n",
      "  *** WARNING: MI is NOT significantly above noise floor! ***\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# PART 1: MUTUAL INFORMATION DIAGNOSTIC (TRAIN ONLY)\n",
    "# ====================================================================\n",
    "\n",
    "mi_real = compute_mi(s_train, y_train, n_states, n_states)\n",
    "mi_bits = mi_real / np.log(2)\n",
    "\n",
    "print(f\"MI(Backward_Bin, Forward_Bin) on TRAIN:\")\n",
    "print(f\"  MI = {mi_real:.6f} nats = {mi_bits:.6f} bits\")\n",
    "\n",
    "# Permutation test\n",
    "mi_shuffled = []\n",
    "for i in range(5):\n",
    "    y_perm = np.random.permutation(y_train)\n",
    "    mi_s = compute_mi(s_train, y_perm, n_states, n_states)\n",
    "    mi_shuffled.append(mi_s)\n",
    "    print(f\"  Shuffled [{i}] = {mi_s:.6f} nats\")\n",
    "\n",
    "mi_shuffled = np.array(mi_shuffled)\n",
    "print(f\"\\n  Shuffled mean = {mi_shuffled.mean():.6f}, std = {mi_shuffled.std():.6f}\")\n",
    "print(f\"  Real MI / Shuffled mean = {mi_real / mi_shuffled.mean():.1f}x\")\n",
    "\n",
    "if mi_real > 3 * mi_shuffled.mean():\n",
    "    print(\"  MI is significantly above noise floor.\")\n",
    "else:\n",
    "    print(\"  *** WARNING: MI is NOT significantly above noise floor! ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-part2",
   "metadata": {},
   "source": [
    "## Part 2 — Bin-Count Ablation\n",
    "\n",
    "**Original discretization:** Fixed-width bins hardcoded in `dataset.Rmd` (55 bins, variable widths: 0.2% center, 0.5% mid, 1% tails).\n",
    "\n",
    "**Ablation approach:**\n",
    "- For `n_bins=55`: use original pre-computed bins from CSV.\n",
    "- For `n_bins` in {25, 35, 40}: use quantile-based binning fit on **TRAIN forward returns only**, applied to all splits.\n",
    "\n",
    "The purpose is to test whether fewer bins increase learnable conditional structure. Quantile bins ensure roughly equal frequency per bin, reducing sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-part2-ablation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "n_bins = 25\n",
      "============================================================\n",
      "  Quantile bins: 25 actual (requested 25)\n",
      "  Avg samples/state (train): 66.3, min=66, states<5: 0\n",
      "  MI = 0.256094 nats (0.369466 bits)\n",
      "  alpha=0.1, test: marg_LL=-3.2188, cond_LL=-3.5423, delta=-0.3236\n",
      "  alpha=1.0, test: marg_LL=-3.2188, cond_LL=-3.3173, delta=-0.0985\n",
      "\n",
      "============================================================\n",
      "n_bins = 35\n",
      "============================================================\n",
      "  Quantile bins: 35 actual (requested 35)\n",
      "  Avg samples/state (train): 47.3, min=47, states<5: 0\n",
      "  MI = 0.471888 nats (0.680791 bits)\n",
      "  alpha=0.1, test: marg_LL=-3.5547, cond_LL=-4.1986, delta=-0.6439\n",
      "  alpha=1.0, test: marg_LL=-3.5547, cond_LL=-3.7045, delta=-0.1498\n",
      "\n",
      "============================================================\n",
      "n_bins = 40\n",
      "============================================================\n",
      "  Quantile bins: 40 actual (requested 40)\n",
      "  Avg samples/state (train): 41.4, min=41, states<5: 0\n",
      "  MI = 0.573988 nats (0.828090 bits)\n",
      "  alpha=0.1, test: marg_LL=-3.6886, cond_LL=-4.4660, delta=-0.7774\n",
      "  alpha=1.0, test: marg_LL=-3.6886, cond_LL=-3.8372, delta=-0.1486\n",
      "\n",
      "============================================================\n",
      "n_bins = 55\n",
      "============================================================\n",
      "  Using original fixed-width bins from dataset.Rmd\n",
      "  Avg samples/state (train): 31.3, min=1, states<5: 11\n",
      "  MI = 0.659427 nats (0.951352 bits)\n",
      "  alpha=0.1, test: marg_LL=-3.6927, cond_LL=-4.1842, delta=-0.4914\n",
      "  alpha=1.0, test: marg_LL=-3.6927, cond_LL=-3.8916, delta=-0.1988\n",
      "\n",
      "Ablation complete.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# PART 2: BIN-COUNT ABLATION\n",
    "# ====================================================================\n",
    "\n",
    "def rebin_quantile(pct_values, pct_train, n_bins):\n",
    "    \"\"\"Rebin percent changes using quantile edges fit on training data.\"\"\"\n",
    "    edges = np.quantile(pct_train, np.linspace(0, 1, n_bins + 1))\n",
    "    edges[0] = -np.inf\n",
    "    edges[-1] = np.inf\n",
    "    # Handle tied quantiles\n",
    "    edges = np.unique(edges)\n",
    "    actual_n = len(edges) - 1\n",
    "    bins = np.clip(np.digitize(pct_values, edges) - 1, 0, actual_n - 1)\n",
    "    return bins, edges, actual_n\n",
    "\n",
    "\n",
    "N_BINS_LIST = [25, 35, 40, 55]\n",
    "ablation_results = []\n",
    "ablation_mi = {}  # store MI per n_bins\n",
    "\n",
    "pct_fwd_train = pct_forward_all[idx_train]\n",
    "\n",
    "for n_bins in N_BINS_LIST:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"n_bins = {n_bins}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if n_bins == 55:\n",
    "        # Use original pre-computed bins from CSV\n",
    "        s_new = s_curr_all.copy()\n",
    "        y_new = y_all.copy()\n",
    "        actual_n = 55\n",
    "        method = \"original_fixed\"\n",
    "        print(\"  Using original fixed-width bins from dataset.Rmd\")\n",
    "    else:\n",
    "        # Quantile-based, fit on TRAIN forward returns only\n",
    "        y_new, edges, actual_n = rebin_quantile(pct_forward_all, pct_fwd_train, n_bins)\n",
    "        s_new, _, _ = rebin_quantile(pct_backward_all, pct_fwd_train, n_bins)\n",
    "        method = f\"quantile\"\n",
    "        print(f\"  Quantile bins: {actual_n} actual (requested {n_bins})\")\n",
    "\n",
    "    # Split\n",
    "    s_tr = s_new[idx_train]; s_va = s_new[idx_val]; s_te = s_new[idx_test]\n",
    "    y_tr = y_new[idx_train]; y_va = y_new[idx_val]; y_te = y_new[idx_test]\n",
    "\n",
    "    # State coverage\n",
    "    sc = np.bincount(s_tr, minlength=actual_n)\n",
    "    print(f\"  Avg samples/state (train): {sc[sc>0].mean():.1f}, \"\n",
    "          f\"min={sc[sc>0].min()}, states<5: {(sc<5).sum()}\")\n",
    "\n",
    "    # MI\n",
    "    mi = compute_mi(s_tr, y_tr, actual_n, actual_n)\n",
    "    ablation_mi[actual_n] = mi\n",
    "    print(f\"  MI = {mi:.6f} nats ({mi/np.log(2):.6f} bits)\")\n",
    "\n",
    "    # Marginal\n",
    "    marginal = compute_marginal(y_tr, actual_n)\n",
    "\n",
    "    # Conditional with both alphas\n",
    "    for alpha in [0.1, 1.0]:\n",
    "        P_cond, _ = compute_conditional(s_tr, y_tr, actual_n, actual_n, alpha=alpha)\n",
    "\n",
    "        for sp_name, s_sp, y_sp in [(\"val\", s_va, y_va), (\"test\", s_te, y_te)]:\n",
    "            N = len(y_sp)\n",
    "            r_m = evaluate_distribution(\n",
    "                np.tile(marginal[np.newaxis, :], (N, 1)), y_sp, actual_n)\n",
    "            r_c = evaluate_distribution(P_cond[s_sp], y_sp, actual_n)\n",
    "            delta = r_c[\"mean_ll\"] - r_m[\"mean_ll\"]\n",
    "\n",
    "            ablation_results.append({\n",
    "                \"n_bins\": actual_n, \"method\": method, \"alpha\": alpha,\n",
    "                \"split\": sp_name,\n",
    "                \"MI_nats\": mi, \"MI_bits\": mi / np.log(2),\n",
    "                \"marginal_LL\": r_m[\"mean_ll\"],\n",
    "                \"conditional_LL\": r_c[\"mean_ll\"],\n",
    "                \"delta_LL\": delta,\n",
    "                \"marginal_acc\": r_m[\"accuracy\"],\n",
    "                \"conditional_acc\": r_c[\"accuracy\"],\n",
    "                \"marginal_sev\": r_m[\"severity\"],\n",
    "                \"conditional_sev\": r_c[\"severity\"],\n",
    "            })\n",
    "\n",
    "            if sp_name == \"test\":\n",
    "                print(f\"  alpha={alpha}, test: marg_LL={r_m['mean_ll']:.4f}, \"\n",
    "                      f\"cond_LL={r_c['mean_ll']:.4f}, delta={delta:+.4f}\")\n",
    "\n",
    "print(\"\\nAblation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-part2-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PART 2: BIN-COUNT ABLATION SUMMARY (best alpha per n_bins)\n",
      "====================================================================================================\n",
      " n_bins split  best_alpha  MI_nats  marginal_LL  conditional_LL  delta_LL  cond_acc  cond_sev\n",
      "     25   val    1.000000 0.256094    -3.218704       -3.348023 -0.129319  0.042254  5.953382\n",
      "     25  test    1.000000 0.256094    -3.218758       -3.317298 -0.098540  0.050562  5.970979\n",
      "     35   val    1.000000 0.471888    -3.554669       -3.683649 -0.128980  0.050704  8.313137\n",
      "     35  test    1.000000 0.471888    -3.554692       -3.704510 -0.149818  0.022472  8.376967\n",
      "     40   val    1.000000 0.573988    -3.688670       -3.796610 -0.107940  0.016901  9.573489\n",
      "     40  test    1.000000 0.573988    -3.688565       -3.837156 -0.148592  0.022472  9.537670\n",
      "     55   val    1.000000 0.659427    -3.682208       -3.882289 -0.200081  0.025352  8.144085\n",
      "     55  test    1.000000 0.659427    -3.692749       -3.891563 -0.198814  0.042135  8.317989\n",
      "====================================================================================================\n",
      "\n",
      "Best n_bins on VAL: 40 (delta_LL = -0.107940, alpha = 1.0)\n",
      "Delta <= 0.01 nats: Conditional advantage is negligible.\n",
      "Part 3 will be SKIPPED (no evidence of learnable conditional signal).\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# PART 2 SUMMARY\n",
    "# ====================================================================\n",
    "df2 = pd.DataFrame(ablation_results)\n",
    "\n",
    "# Best alpha per (n_bins, split)\n",
    "summary2 = []\n",
    "for nb in sorted(df2[\"n_bins\"].unique()):\n",
    "    for sp in [\"val\", \"test\"]:\n",
    "        sub = df2[(df2[\"n_bins\"] == nb) & (df2[\"split\"] == sp)]\n",
    "        best = sub.loc[sub[\"delta_LL\"].idxmax()]\n",
    "        summary2.append({\n",
    "            \"n_bins\": int(best[\"n_bins\"]), \"split\": sp,\n",
    "            \"best_alpha\": best[\"alpha\"],\n",
    "            \"MI_nats\": best[\"MI_nats\"],\n",
    "            \"marginal_LL\": best[\"marginal_LL\"],\n",
    "            \"conditional_LL\": best[\"conditional_LL\"],\n",
    "            \"delta_LL\": best[\"delta_LL\"],\n",
    "            \"cond_acc\": best[\"conditional_acc\"],\n",
    "            \"cond_sev\": best[\"conditional_sev\"],\n",
    "        })\n",
    "\n",
    "df2_summary = pd.DataFrame(summary2)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"PART 2: BIN-COUNT ABLATION SUMMARY (best alpha per n_bins)\")\n",
    "print(\"=\"*100)\n",
    "print(df2_summary.to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Identify best n_bins for Part 3\n",
    "best_row_val = df2_summary[df2_summary[\"split\"] == \"val\"].sort_values(\n",
    "    \"delta_LL\", ascending=False).iloc[0]\n",
    "best_n_bins = int(best_row_val[\"n_bins\"])\n",
    "best_delta_val = best_row_val[\"delta_LL\"]\n",
    "best_alpha_val = best_row_val[\"best_alpha\"]\n",
    "\n",
    "proceed_to_part3 = best_delta_val > 0.01\n",
    "\n",
    "print(f\"\\nBest n_bins on VAL: {best_n_bins} \"\n",
    "      f\"(delta_LL = {best_delta_val:+.6f}, alpha = {best_alpha_val})\")\n",
    "if proceed_to_part3:\n",
    "    print(f\"Delta > 0.01 nats: PROCEEDING to Part 3 with n_bins={best_n_bins}\")\n",
    "else:\n",
    "    print(\"Delta <= 0.01 nats: Conditional advantage is negligible.\")\n",
    "    print(\"Part 3 will be SKIPPED (no evidence of learnable conditional signal).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-part3",
   "metadata": {},
   "source": [
    "## Part 3 — Minimal Neural Check\n",
    "\n",
    "Only runs if Part 2 showed a non-trivial conditional advantage (delta_LL > 0.01 on VAL).\n",
    "\n",
    "Trains two variants:\n",
    "- **(A)** Default sampling (current)\n",
    "- **(B)** State-balanced sampling (WeightedRandomSampler inversely proportional to current-state frequency)\n",
    "\n",
    "Uses `soft_gaussian_sigma1.0` (best-performing label strategy from prior experiments).\n",
    "\n",
    "Model: `TransitionNetStateHead` — copied from `TransitionProbMatrix_NEWDATA.ipynb` cell `6978c078`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-part3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPING Part 3: no conditional advantage found in Part 2.\n",
      "Final summary will be based on Parts 1-2 only.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# PART 3: MINIMAL NEURAL CHECK\n",
    "# ====================================================================\n",
    "\n",
    "if not proceed_to_part3:\n",
    "    print(\"SKIPPING Part 3: no conditional advantage found in Part 2.\")\n",
    "    print(\"Final summary will be based on Parts 1-2 only.\")\n",
    "    r_default = r_balanced = r_marginal_p3 = r_cond_p3 = None\n",
    "    df3 = pd.DataFrame()  # empty\n",
    "    n_states_p3 = None\n",
    "else:\n",
    "    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    N_EPOCHS_P3 = 30\n",
    "    PATIENCE = 8\n",
    "\n",
    "    # --- Re-discretize with best n_bins ---\n",
    "    if best_n_bins == 55:\n",
    "        s_p3 = s_curr_all.copy()\n",
    "        y_p3 = y_all.copy()\n",
    "        n_states_p3 = 55\n",
    "    else:\n",
    "        y_p3, _, n_states_p3 = rebin_quantile(pct_forward_all, pct_fwd_train, best_n_bins)\n",
    "        s_p3, _, _ = rebin_quantile(pct_backward_all, pct_fwd_train, best_n_bins)\n",
    "\n",
    "    s_tr3 = s_p3[idx_train]; s_va3 = s_p3[idx_val]; s_te3 = s_p3[idx_test]\n",
    "    y_tr3 = y_p3[idx_train]; y_va3 = y_p3[idx_val]; y_te3 = y_p3[idx_test]\n",
    "\n",
    "    print(f\"Part 3: n_bins={n_states_p3}, device={DEVICE}\")\n",
    "\n",
    "    # --- Model (from TransitionProbMatrix_NEWDATA.ipynb, cell 6978c078) ---\n",
    "    class TransitionNetStateHead(nn.Module):\n",
    "        def __init__(self, n_features, n_states, trunk_dims=(64, 128, 256, 128),\n",
    "                     trunk_out=64, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.n_features = n_features\n",
    "            self.n_states = n_states\n",
    "            self.trunk_out = trunk_out\n",
    "            layers = []\n",
    "            in_dim = n_features\n",
    "            for h in trunk_dims:\n",
    "                layers += [nn.Linear(in_dim, h), nn.GELU(), nn.Dropout(dropout)]\n",
    "                in_dim = h\n",
    "            layers += [nn.Linear(in_dim, trunk_out), nn.GELU()]\n",
    "            self.trunk = nn.Sequential(*layers)\n",
    "            self.head_W = nn.Parameter(torch.randn(n_states, trunk_out, n_states) * 0.01)\n",
    "            self.head_b = nn.Parameter(torch.zeros(n_states, n_states))\n",
    "\n",
    "        def forward(self, x, s_curr):\n",
    "            h = self.trunk(x)\n",
    "            W_s = self.head_W[s_curr]\n",
    "            b_s = self.head_b[s_curr]\n",
    "            return torch.bmm(h.unsqueeze(1), W_s).squeeze(1) + b_s\n",
    "\n",
    "    # --- Soft labels (from cell kq2127g0bk) ---\n",
    "    def create_soft_labels_batch(y_hard, n_st, sigma=1.0):\n",
    "        B = y_hard.shape[0]\n",
    "        dev = y_hard.device\n",
    "        j = torch.arange(n_st, device=dev, dtype=torch.float32).unsqueeze(0).expand(B, -1)\n",
    "        d2 = (j - y_hard.unsqueeze(1).float()) ** 2\n",
    "        u = torch.exp(-d2 / (2 * sigma ** 2))\n",
    "        return u / (u.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "    # --- Dataset ---\n",
    "    class TransitionDataset(Dataset):\n",
    "        def __init__(self, X, s, y):\n",
    "            self.X = torch.tensor(X, dtype=torch.float32)\n",
    "            self.s = torch.tensor(s, dtype=torch.long)\n",
    "            self.y = torch.tensor(y, dtype=torch.long)\n",
    "        def __len__(self): return len(self.X)\n",
    "        def __getitem__(self, i): return self.X[i], self.s[i], self.y[i]\n",
    "\n",
    "    # --- Train + evaluate function ---\n",
    "    def train_and_evaluate(s_tr, y_tr, s_va, y_va, s_te, y_te,\n",
    "                           n_st, sampler=None, label=\"default\"):\n",
    "        torch.manual_seed(SEED)\n",
    "        model = TransitionNetStateHead(n_features, n_st).to(DEVICE)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "        tr_ds = TransitionDataset(X_train_std, s_tr, y_tr)\n",
    "        va_ds = TransitionDataset(X_val_std, s_va, y_va)\n",
    "        te_ds = TransitionDataset(X_test_std, s_te, y_te)\n",
    "\n",
    "        tr_ld = DataLoader(tr_ds, batch_size=256,\n",
    "                           sampler=sampler, shuffle=(sampler is None))\n",
    "        va_ld = DataLoader(va_ds, batch_size=512, shuffle=False)\n",
    "        te_ld = DataLoader(te_ds, batch_size=512, shuffle=False)\n",
    "\n",
    "        best_vl, best_st, patience_ctr = float(\"inf\"), None, 0\n",
    "\n",
    "        for ep in range(1, N_EPOCHS_P3 + 1):\n",
    "            model.train()\n",
    "            for xb, sb, yb in tr_ld:\n",
    "                xb, sb, yb = xb.to(DEVICE), sb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                logits = model(xb, sb)\n",
    "                soft = create_soft_labels_batch(yb, n_st, sigma=1.0)\n",
    "                loss = F.kl_div(F.log_softmax(logits, 1), soft, reduction='batchmean')\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                opt.step()\n",
    "\n",
    "            model.eval()\n",
    "            vl, vn = 0.0, 0\n",
    "            with torch.no_grad():\n",
    "                for xb, sb, yb in va_ld:\n",
    "                    xb, sb, yb = xb.to(DEVICE), sb.to(DEVICE), yb.to(DEVICE)\n",
    "                    logits = model(xb, sb)\n",
    "                    soft = create_soft_labels_batch(yb, n_st, sigma=1.0)\n",
    "                    loss = F.kl_div(F.log_softmax(logits, 1), soft, reduction='batchmean')\n",
    "                    vl += loss.item() * len(xb); vn += len(xb)\n",
    "            vl /= vn\n",
    "\n",
    "            if vl < best_vl:\n",
    "                best_vl = vl\n",
    "                best_st = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                patience_ctr = 0\n",
    "            else:\n",
    "                patience_ctr += 1\n",
    "\n",
    "            if ep % 10 == 0 or ep == 1:\n",
    "                print(f\"  [{label}] ep {ep:02d}: val_loss={vl:.4f} (best={best_vl:.4f})\")\n",
    "            if patience_ctr >= PATIENCE:\n",
    "                print(f\"  [{label}] Early stop at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(best_st); model.to(DEVICE); model.eval()\n",
    "\n",
    "        all_p, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, sb, yb in te_ld:\n",
    "                xb, sb = xb.to(DEVICE), sb.to(DEVICE)\n",
    "                probs = F.softmax(model(xb, sb), dim=1).cpu().numpy()\n",
    "                all_p.append(probs); all_y.append(yb.numpy())\n",
    "        return evaluate_distribution(np.vstack(all_p), np.concatenate(all_y), n_st, label)\n",
    "\n",
    "    # --- (A) Default sampling ---\n",
    "    print(\"\\n--- Training: DEFAULT sampling ---\")\n",
    "    r_default = train_and_evaluate(\n",
    "        s_tr3, y_tr3, s_va3, y_va3, s_te3, y_te3, n_states_p3, label=\"neural_default\")\n",
    "\n",
    "    # --- (B) Balanced sampling ---\n",
    "    print(\"\\n--- Training: BALANCED sampling ---\")\n",
    "    sc3 = np.bincount(s_tr3, minlength=n_states_p3)\n",
    "    w = 1.0 / np.maximum(sc3, 1).astype(np.float64)\n",
    "    sw = w[s_tr3]; sw = sw / sw.sum()\n",
    "    sampler_b = WeightedRandomSampler(torch.DoubleTensor(sw), len(s_tr3), replacement=True)\n",
    "    r_balanced = train_and_evaluate(\n",
    "        s_tr3, y_tr3, s_va3, y_va3, s_te3, y_te3, n_states_p3,\n",
    "        sampler=sampler_b, label=\"neural_balanced\")\n",
    "\n",
    "    # --- Baselines at this n_bins ---\n",
    "    marg_p3 = compute_marginal(y_tr3, n_states_p3)\n",
    "    r_marginal_p3 = evaluate_distribution(\n",
    "        np.tile(marg_p3[np.newaxis, :], (len(y_te3), 1)),\n",
    "        y_te3, n_states_p3, \"marginal\")\n",
    "\n",
    "    P_cond_p3, _ = compute_conditional(\n",
    "        s_tr3, y_tr3, n_states_p3, n_states_p3, alpha=float(best_alpha_val))\n",
    "    r_cond_p3 = evaluate_distribution(\n",
    "        P_cond_p3[s_te3], y_te3, n_states_p3,\n",
    "        f\"conditional (a={best_alpha_val})\")\n",
    "\n",
    "    # --- Part 3 table ---\n",
    "    p3_rows = [r_marginal_p3, r_cond_p3, r_default, r_balanced]\n",
    "    df3 = pd.DataFrame(p3_rows)[[\"label\", \"mean_ll\", \"accuracy\", \"severity\"]]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"PART 3: NEURAL CHECK (n_bins={n_states_p3}, TEST)\")\n",
    "    print(\"=\"*90)\n",
    "    print(df3.to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    for r in [r_default, r_balanced]:\n",
    "        d = r[\"mean_ll\"] - r_marginal_p3[\"mean_ll\"]\n",
    "        print(f\"  {r['label']} vs marginal: delta = {d:+.6f} -> \"\n",
    "              f\"{'BEATS marginal' if d > 0 else 'does NOT beat marginal'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-part4",
   "metadata": {},
   "source": [
    "## Part 4 — Final Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-part4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "FINAL CONCLUSION\n",
      "==========================================================================================\n",
      "  [-] Conditional baseline does NOT beat marginal on TEST (best delta=-0.098540)\n",
      "      n_bins=25: delta_LL=-0.098540 [-]\n",
      "      n_bins=35: delta_LL=-0.149818 [-]\n",
      "      n_bins=40: delta_LL=-0.148592 [-]\n",
      "      n_bins=55: delta_LL=-0.198814 [-]\n",
      "\n",
      "  MI(X_t, Y_t) = 0.659427 nats (0.951352 bits)\n",
      "  Shuffled MI mean = 0.630298 nats\n",
      "  [-] MI is only 1.0x noise floor -> weak\n",
      "\n",
      "  Neural model: SKIPPED (no conditional signal found in Part 2)\n",
      "\n",
      "==========================================================================================\n",
      "RECOMMENDATION\n",
      "==========================================================================================\n",
      "  No conditional advantage found at any discretization tested.\n",
      "  RECOMMENDATION: REVISE the research claim / state design.\n",
      "  Consider: different state definitions, continuous targets, or\n",
      "  multi-variate state spaces.\n",
      "==========================================================================================\n",
      "\n",
      "All tables saved to results/diagnostics/ (timestamp: 20260213_155510)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# PART 4: FINAL CONCLUSION + SAVE\n",
    "# ====================================================================\n",
    "os.makedirs(\"results/diagnostics\", exist_ok=True)\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "df1.to_csv(f\"results/diagnostics/part1_baselines_{ts}.csv\", index=False)\n",
    "df2.to_csv(f\"results/diagnostics/part2_ablation_{ts}.csv\", index=False)\n",
    "df2_summary.to_csv(f\"results/diagnostics/part2_summary_{ts}.csv\", index=False)\n",
    "if len(df3) > 0:\n",
    "    df3.to_csv(f\"results/diagnostics/part3_neural_{ts}.csv\", index=False)\n",
    "\n",
    "# ====================================================================\n",
    "# FINAL SUMMARY\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"FINAL CONCLUSION\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# 1. Conditional vs marginal\n",
    "best_test = df2_summary[df2_summary[\"split\"] == \"test\"].sort_values(\n",
    "    \"delta_LL\", ascending=False).iloc[0]\n",
    "bt_delta = best_test[\"delta_LL\"]\n",
    "bt_nb = int(best_test[\"n_bins\"])\n",
    "\n",
    "if bt_delta > 0:\n",
    "    print(f\"  [+] Conditional baseline BEATS marginal on TEST \"\n",
    "          f\"(delta={bt_delta:+.6f} nats, n_bins={bt_nb})\")\n",
    "else:\n",
    "    print(f\"  [-] Conditional baseline does NOT beat marginal on TEST \"\n",
    "          f\"(best delta={bt_delta:+.6f})\")\n",
    "\n",
    "for nb in sorted(df2_summary[\"n_bins\"].unique()):\n",
    "    d = df2_summary[(df2_summary[\"n_bins\"]==nb) & (df2_summary[\"split\"]==\"test\")][\"delta_LL\"].values[0]\n",
    "    print(f\"      n_bins={nb}: delta_LL={d:+.6f} {'[+]' if d > 0 else '[-]'}\")\n",
    "\n",
    "# 2. MI\n",
    "print(f\"\\n  MI(X_t, Y_t) = {mi_real:.6f} nats ({mi_bits:.6f} bits)\")\n",
    "print(f\"  Shuffled MI mean = {mi_shuffled.mean():.6f} nats\")\n",
    "ratio = mi_real / mi_shuffled.mean() if mi_shuffled.mean() > 0 else float('inf')\n",
    "if ratio > 3:\n",
    "    print(f\"  [+] MI is {ratio:.1f}x noise floor -> statistically significant\")\n",
    "else:\n",
    "    print(f\"  [-] MI is only {ratio:.1f}x noise floor -> weak\")\n",
    "\n",
    "# 3. Neural model\n",
    "if r_default is not None:\n",
    "    dd = r_default[\"mean_ll\"] - r_marginal_p3[\"mean_ll\"]\n",
    "    db = r_balanced[\"mean_ll\"] - r_marginal_p3[\"mean_ll\"]\n",
    "    if dd > 0:\n",
    "        print(f\"\\n  [+] Neural (default) BEATS marginal (delta={dd:+.6f})\")\n",
    "    else:\n",
    "        print(f\"\\n  [-] Neural (default) does NOT beat marginal (delta={dd:+.6f})\")\n",
    "    if db > 0:\n",
    "        print(f\"  [+] Neural (balanced) BEATS marginal (delta={db:+.6f})\")\n",
    "    else:\n",
    "        print(f\"  [-] Neural (balanced) does NOT beat marginal (delta={db:+.6f})\")\n",
    "    if db > dd:\n",
    "        print(f\"  Balanced sampling helps: +{db - dd:.6f} nats\")\n",
    "    else:\n",
    "        print(f\"  Balanced sampling does NOT help: {db - dd:+.6f} nats\")\n",
    "else:\n",
    "    print(f\"\\n  Neural model: SKIPPED (no conditional signal found in Part 2)\")\n",
    "\n",
    "# 4. Recommendation\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "neural_beats = (r_default is not None and\n",
    "                max(r_default[\"mean_ll\"], r_balanced[\"mean_ll\"]) > r_marginal_p3[\"mean_ll\"])\n",
    "cond_beats = bt_delta > 0\n",
    "\n",
    "if neural_beats:\n",
    "    print(\"  The neural model BEATS marginal.\")\n",
    "    print(\"  RECOMMENDATION: PROCEED with further architecture improvements.\")\n",
    "    if r_balanced[\"mean_ll\"] > r_default[\"mean_ll\"]:\n",
    "        print(\"  Use balanced sampling as default going forward.\")\n",
    "elif cond_beats and not neural_beats:\n",
    "    print(\"  Conditional structure EXISTS (count-based baseline beats marginal),\")\n",
    "    print(\"  but the neural model fails to exploit it.\")\n",
    "    print(\"  RECOMMENDATION: Focus on improving the neural model (objective, capacity,\")\n",
    "    print(\"  regularization) rather than changing architecture or state design.\")\n",
    "else:\n",
    "    print(\"  No conditional advantage found at any discretization tested.\")\n",
    "    print(\"  RECOMMENDATION: REVISE the research claim / state design.\")\n",
    "    print(\"  Consider: different state definitions, continuous targets, or\")\n",
    "    print(\"  multi-variate state spaces.\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(f\"\\nAll tables saved to results/diagnostics/ (timestamp: {ts})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat453",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
