# Workflow Guide: Auto-Organized Experiment Comparison

## Overview

The notebook now **automatically organizes outputs** based on your configuration. You can run multiple experiments (hard labels, soft labels with different parameters) and compare them side-by-side without any manual file management.

---

## How It Works

### 1. **Automatic Output Organization**

When you run the training notebook, it:
1. Reads your soft-label configuration
2. Creates an experiment name (e.g., `hard_labels`, `soft_gaussian_sigma2.0`)
3. Creates a dedicated results folder: `results/{experiment_name}/`
4. Saves everything there:
   - `config.json` - Your experiment settings
   - `models/` - Model weights and transition matrices
   - `figures/` - Any plots you generate
   - `metrics/` - Training history (loss, accuracy, severity)

**Key benefit:** Different experiments don't overwrite each other!

---

## Workflow: Running Experiments

### **Step 1: Run Baseline (Hard Labels)**

1. Open [TransitionProbMatrix_NEWDATA.ipynb](TransitionProbMatrix_NEWDATA.ipynb)
2. Set configuration:
   ```python
   USE_SOFT_LABELS = False
   ```
3. Run all cells
4. Results saved to: `results/hard_labels/`

**Output structure:**
```
results/hard_labels/
â”œâ”€â”€ config.json
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_weights.pt
â”‚   â”œâ”€â”€ baseline_weights.pt
â”‚   â””â”€â”€ A_all_model.*
â””â”€â”€ metrics/
    â”œâ”€â”€ transition_model_history.json
    â””â”€â”€ baseline_model_history.json
```

---

### **Step 2: Run Soft Labels (Gaussian, Ïƒ=2.0)**

1. **Same notebook**, change configuration:
   ```python
   USE_SOFT_LABELS = True
   SOFT_LABEL_KERNEL = "gaussian"
   SOFT_LABEL_SIGMA = 2.0
   ```
2. Click "Restart Kernel & Run All Cells"
3. Results saved to: `results/soft_gaussian_sigma2.0/`

**New files created** (baseline files untouched):
```
results/soft_gaussian_sigma2.0/
â”œâ”€â”€ config.json
â”œâ”€â”€ models/
â””â”€â”€ metrics/
```

---

### **Step 3: Run More Experiments (Optional)**

Try different configurations:

**Tighter soft labels (Ïƒ=1.0):**
```python
USE_SOFT_LABELS = True
SOFT_LABEL_KERNEL = "gaussian"
SOFT_LABEL_SIGMA = 1.0
```
â†’ Results in `results/soft_gaussian_sigma1.0/`

**Triangular kernel:**
```python
USE_SOFT_LABELS = True
SOFT_LABEL_KERNEL = "triangular"
SOFT_LABEL_RADIUS = 3
```
â†’ Results in `results/soft_triangular_radius3/`

**Each experiment gets its own folder!**

---

### **Step 4: Compare All Experiments**

1. Open [CompareExperiments.ipynb](CompareExperiments.ipynb)
2. Run all cells
3. Automatically loads and compares all experiments in `results/`

**What you get:**
- Summary table with all metrics
- Bar charts comparing accuracy and severity
- Training curves overlayed
- Improvement analysis vs baseline
- Exported CSV and report files

---

## Directory Structure After Running All Experiments

```
DeepMarkovResearch/
â”œâ”€â”€ TransitionProbMatrix_NEWDATA.ipynb  # Main training notebook
â”œâ”€â”€ CompareExperiments.ipynb            # Comparison notebook
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ hard_labels/
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ model_weights.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ baseline_weights.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ A_all_model.pt
â”‚   â”‚   â”‚   â””â”€â”€ A_all_model.npy
â”‚   â”‚   â”œâ”€â”€ figures/  (empty, add plots here if needed)
â”‚   â”‚   â””â”€â”€ metrics/
â”‚   â”‚       â”œâ”€â”€ transition_model_history.json
â”‚   â”‚       â””â”€â”€ baseline_model_history.json
â”‚   â”‚
â”‚   â”œâ”€â”€ soft_gaussian_sigma2.0/
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â””â”€â”€ metrics/
â”‚   â”‚
â”‚   â”œâ”€â”€ soft_gaussian_sigma1.0/
â”‚   â”‚   â””â”€â”€ (same structure)
â”‚   â”‚
â”‚   â”œâ”€â”€ soft_triangular_radius3/
â”‚   â”‚   â””â”€â”€ (same structure)
â”‚   â”‚
â”‚   â”œâ”€â”€ experiment_summary.csv          # Generated by CompareExperiments
â”‚   â”œâ”€â”€ comparison_report.txt           # Generated by CompareExperiments
â”‚   â”œâ”€â”€ comparison_accuracy.png         # Generated by CompareExperiments
â”‚   â”œâ”€â”€ comparison_severity.png         # Generated by CompareExperiments
â”‚   â””â”€â”€ comparison_training_curves.png  # Generated by CompareExperiments
â”‚
â””â”€â”€ (other files...)
```

---

## Key Features

### âœ… **No Manual File Management**
- Don't need to rename files
- Don't need to move files around
- Don't need to duplicate notebooks

### âœ… **Reproducibility**
- Each experiment folder contains `config.json` with exact settings
- Can always recreate the experiment

### âœ… **Easy Comparison**
- Just run `CompareExperiments.ipynb`
- Automatically finds all experiments
- Generates publication-ready figures

### âœ… **Safe to Re-Run**
- Re-running an experiment overwrites only that experiment's folder
- Other experiments remain untouched

---

## Quick Reference: What Goes Where

| You want to... | Use this notebook | Look for results in |
|----------------|-------------------|---------------------|
| Train a model | `TransitionProbMatrix_NEWDATA.ipynb` | `results/{exp_name}/` |
| Compare experiments | `CompareExperiments.ipynb` | `results/comparison_*.png` |
| Load a trained model | Any notebook | `results/{exp_name}/models/model_weights.pt` |
| Check config of a run | Any notebook | `results/{exp_name}/config.json` |
| Get training history | Any notebook | `results/{exp_name}/metrics/*_history.json` |

---

## Typical Experiment Session

**Morning:** Run baseline
```python
# Set: USE_SOFT_LABELS = False
# Run all cells â†’ go get coffee â˜•
```

**Lunch:** Run soft labels
```python
# Set: USE_SOFT_LABELS = True, SOFT_LABEL_SIGMA = 2.0
# Run all cells â†’ go to lunch ğŸ”
```

**Afternoon:** Run more variations
```python
# Try SOFT_LABEL_SIGMA = 1.0, 3.0, etc.
# Each run: ~10-15 minutes
```

**End of day:** Compare everything
```
Open CompareExperiments.ipynb
Run all cells
Look at pretty charts ğŸ“Š
```

**For paper:**
```
Copy results/comparison_*.png to paper/figures/
Copy results/experiment_summary.csv to paper/tables/
```

---

## Tips & Tricks

### Naming Experiments
Experiment names are auto-generated:
- `USE_SOFT_LABELS = False` â†’ `hard_labels`
- `USE_SOFT_LABELS = True, SOFT_LABEL_KERNEL = "gaussian", SOFT_LABEL_SIGMA = 2.0` â†’ `soft_gaussian_sigma2.0`
- `USE_SOFT_LABELS = True, SOFT_LABEL_KERNEL = "triangular", SOFT_LABEL_RADIUS = 3` â†’ `soft_triangular_radius3`

**If you want custom names**, edit this section in the notebook:
```python
# In cell "Data Preparation"
if USE_SOFT_LABELS:
    if SOFT_LABEL_KERNEL == "gaussian":
        EXP_NAME = f"soft_gaussian_sigma{SOFT_LABEL_SIGMA}"
    else:
        EXP_NAME = f"soft_triangular_radius{SOFT_LABEL_RADIUS}"
else:
    EXP_NAME = "hard_labels"

# You can override it:
# EXP_NAME = "my_custom_name"
```

### Loading Results in Other Notebooks
```python
import json
import torch

# Load config
with open("results/soft_gaussian_sigma2.0/config.json") as f:
    config = json.load(f)

# Load model weights
model_state = torch.load("results/soft_gaussian_sigma2.0/models/model_weights.pt")

# Load metrics
with open("results/soft_gaussian_sigma2.0/metrics/transition_model_history.json") as f:
    history = json.load(f)

print(f"Test accuracy: {history['test_acc']*100:.2f}%")
```

### Archiving Old Experiments
```bash
# Keep results/ clean by archiving old runs
mkdir archive
mv results/old_experiment_name archive/
```

### Batch Running (Advanced)
Create a script to run multiple experiments sequentially:
```python
# run_experiments.py
configs = [
    {"USE_SOFT_LABELS": False},
    {"USE_SOFT_LABELS": True, "SOFT_LABEL_SIGMA": 1.0},
    {"USE_SOFT_LABELS": True, "SOFT_LABEL_SIGMA": 2.0},
    {"USE_SOFT_LABELS": True, "SOFT_LABEL_SIGMA": 3.0},
]

for config in configs:
    # Set config in notebook
    # Run notebook (using papermill or nbconvert)
    # Wait for completion
```

---

## FAQ

**Q: Do I need to create separate notebooks for each experiment?**
A: No! Use one notebook, change the config, re-run. Results auto-organize.

**Q: What if I forget which config I used for an experiment?**
A: Check `results/{exp_name}/config.json` - it has all settings.

**Q: Can I delete an experiment?**
A: Yes, just `rm -rf results/{exp_name}`. Other experiments unaffected.

**Q: How do I share results with collaborators?**
A: Zip the entire `results/` folder or individual experiment folders.

**Q: Can I run experiments in parallel on different machines?**
A: Yes! Just merge the `results/` folders afterward.

**Q: What if I want to save additional plots?**
A: Save them to `{FIGURES_DIR}/your_plot.png` - it's already defined in the notebook.

**Q: How do I know which experiment performed best?**
A: Run `CompareExperiments.ipynb` - it ranks them automatically.

---

## Comparison to Old Workflow

### âŒ **Old Way (Manual)**
1. Run notebook with hard labels
2. Manually rename output files: `model_weights.pt` â†’ `model_weights_hard.pt`
3. Change config to soft labels
4. Run notebook again
5. Manually rename: `model_weights.pt` â†’ `model_weights_soft.pt`
6. Write custom script to load both and compare
7. Forget which hyperparameters you used 3 days later

### âœ… **New Way (Automatic)**
1. Run notebook with hard labels â†’ saved to `results/hard_labels/`
2. Change config to soft labels, re-run â†’ saved to `results/soft_gaussian_sigma2.0/`
3. Open `CompareExperiments.ipynb`, run â†’ all comparisons generated
4. Check `config.json` anytime to see exact settings

**Time saved:** ~30 minutes per experiment cycle
**Sanity saved:** Priceless ğŸ˜Œ

---

## Next Steps

1. âœ… Run baseline experiment (`USE_SOFT_LABELS = False`)
2. âœ… Run soft-label experiment (`USE_SOFT_LABELS = True, SOFT_LABEL_SIGMA = 2.0`)
3. âœ… Open `CompareExperiments.ipynb` and generate comparisons
4. âœ… Review results, decide on best configuration
5. âœ… Run additional experiments if needed
6. âœ… Include comparison figures in your paper

**You're all set!** ğŸš€
