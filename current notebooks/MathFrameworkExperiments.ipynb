{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bef4fbf",
   "metadata": {},
   "source": [
    "# MathFrameworkExperiments\n",
    "**Purpose:** Strengthening the paper's empirical foundation via:\n",
    "1. Degeneracy & small-signal rigor (counting fails story)\n",
    "2. Operator interpretability diagnostics + regime case study\n",
    "3. Chapman–Kolmogorov consistency as a diagnostic\n",
    "4. Uncertainty: multi-seed + block bootstrap CIs\n",
    "\n",
    "**Do NOT modify `notebooks/MasterNotebook.ipynb`.**  \n",
    "All heavy computation is delegated to `scripts/` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf86a598",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:06.269965Z",
     "iopub.status.busy": "2026-02-24T16:59:06.269699Z",
     "iopub.status.idle": "2026-02-24T16:59:08.343779Z",
     "shell.execute_reply": "2026-02-24T16:59:08.343263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Allow importing from the repo root scripts/ directory\n",
    "REPO_ROOT = Path.cwd()\n",
    "if REPO_ROOT.name == \"notebooks\":\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "os.chdir(REPO_ROOT)\n",
    "sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.config import make_config, save_config\n",
    "from scripts.data import load_master_dataset, compute_returns, preprocess_features, build_all_splits, build_all_ck_splits\n",
    "from scripts.bins import compute_X_t, get_xt_labels_for_ck, build_all_configs, assign_bins, get_edges, compute_sigma\n",
    "from scripts.models import StateConditionedNet, StateFreeNet\n",
    "from scripts.train import (\n",
    "    train_one_run, build_loaders, build_ck_loaders,\n",
    "    build_A_t_neural, build_A_t_statefree,\n",
    "    cache_model, load_cached_model, is_cached,\n",
    "    MasterDataset,\n",
    ")\n",
    "from scripts.eval import (\n",
    "    evaluate_model, evaluate_baselines, mean_log_likelihood,\n",
    "    compute_degeneracy_stats, compute_transition_sparsity,\n",
    "    build_ck_composed, compute_ck_errors,\n",
    "    compute_dobrushin, compute_row_heterogeneity, compute_row_entropy,\n",
    "    compute_spectral_mixing_proxy,\n",
    "    compute_pit, compute_ece, compute_brier,\n",
    "    get_loglik_per_sample_model, get_loglik_per_sample_baseline,\n",
    "    block_bootstrap_ci,\n",
    "    _compute_marginal, _build_backoff_matrix, _compute_conditional_additive,\n",
    ")\n",
    "from scripts.plotting import (\n",
    "    save_fig,\n",
    "    plot_sparsity_vs_N, plot_transition_sparsity_table,\n",
    "    plot_ck_error_summary, plot_ck_time_series,\n",
    "    plot_dobrushin_over_time, plot_row_heterogeneity_over_time,\n",
    "    plot_entropy_over_time, plot_spectral_proxy_over_time,\n",
    "    plot_At_heatmap_snapshot, plot_regime_panel,\n",
    "    plot_pit_histogram, plot_reliability_curve,\n",
    ")\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a923e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:08.345112Z",
     "iopub.status.busy": "2026-02-24T16:59:08.344984Z",
     "iopub.status.idle": "2026-02-24T16:59:08.373151Z",
     "shell.execute_reply": "2026-02-24T16:59:08.372591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir : /Users/JanRovirosaIlla/DeepMarkovResearch/results/paper_upgrade/2026-02-24\n",
      "Device     : cpu\n",
      "Git hash   : fe7e150\n",
      "Date stamp : 2026-02-24\n"
     ]
    }
   ],
   "source": [
    "cfg = make_config()\n",
    "OUT_DIR = Path(cfg.output_dir)\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "CACHE_DIR = OUT_DIR / \"cache\"\n",
    "for d in [OUT_DIR, FIG_DIR, CACHE_DIR, OUT_DIR / \"multiasset_edges\"]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Fixed seeds\n",
    "SEED = cfg.seed\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# FAST_MODE: set True for a quick smoke test\n",
    "FAST_MODE = False\n",
    "if FAST_MODE:\n",
    "    cfg.horizons = [1]\n",
    "    cfg.n_bins_list = [10, 55]\n",
    "    cfg.ck_horizons = [1]\n",
    "    cfg.max_epochs = 20\n",
    "    cfg.seeds = [42]\n",
    "    print(\"FAST_MODE active — reduced config\")\n",
    "\n",
    "save_config(cfg, OUT_DIR / \"config.yaml\")\n",
    "print(f\"Output dir : {OUT_DIR}\")\n",
    "print(f\"Device     : {DEVICE}\")\n",
    "print(f\"Git hash   : {cfg.git_hash}\")\n",
    "print(f\"Date stamp : {cfg.date_stamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6473a219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:08.374610Z",
     "iopub.status.busy": "2026-02-24T16:59:08.374485Z",
     "iopub.status.idle": "2026-02-24T16:59:08.419238Z",
     "shell.execute_reply": "2026-02-24T16:59:08.418715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices: 2369, Features: 194\n",
      "N_XT=55, X_t range [0, 54]\n",
      "CK splits: {1: 2367, 2: 2366, 5: 2363, 10: 2358}\n",
      "Loaded master_grid_results.csv: (80, 15)\n"
     ]
    }
   ],
   "source": [
    "prices, F_raw, feature_cols = load_master_dataset(\"dataset\")\n",
    "n_features = F_raw.shape[1]\n",
    "print(f\"Prices: {len(prices)}, Features: {n_features}\")\n",
    "\n",
    "splits = build_all_splits(prices, cfg.horizons)\n",
    "# Fit z-score on h=1 train\n",
    "idx_train_h1 = splits[1][\"idx_train\"]\n",
    "F_normed = preprocess_features(F_raw, idx_train_h1)\n",
    "\n",
    "# Compute X_t (1-day return bins, N_XT=55)\n",
    "T_1 = splits[1][\"T_h\"]\n",
    "train_end_h1 = splits[1][\"idx_train\"][-1] + 1\n",
    "X_t_all, N_XT, edges_xt = compute_X_t(prices, cfg.n_xt_target, train_end_h1)\n",
    "print(f\"N_XT={N_XT}, X_t range [{X_t_all.min()}, {X_t_all.max()}]\")\n",
    "\n",
    "# CK-specific splits: T_ck = len(X_t_all) - h  (one fewer than T_h = len(prices) - h)\n",
    "# because y_ck[h] = X_t_all[h:] has length len(X_t_all) - h\n",
    "ck_splits = build_all_ck_splits(X_t_all, cfg.ck_horizons)\n",
    "print(\"CK splits:\", {h: sp[\"T_ck\"] for h, sp in ck_splits.items()})\n",
    "\n",
    "# CK labels for all horizons (needed in Sections A+B)\n",
    "ck_labels = get_xt_labels_for_ck(X_t_all, cfg.ck_horizons)\n",
    "\n",
    "# Build cumulative-return configs (mirrors master notebook)\n",
    "configs = build_all_configs(\n",
    "    prices, F_normed, X_t_all,\n",
    "    cfg.horizons, cfg.n_bins_list, N_XT, edges_xt, splits,\n",
    "    sigma_anchor=cfg.sigma_anchor,\n",
    "    results_dir=None,\n",
    ")\n",
    "\n",
    "# Load existing master results (no rerun)\n",
    "master_results = pd.read_csv(\"results/master_grid_results.csv\")\n",
    "print(f\"Loaded master_grid_results.csv: {master_results.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cp93zf9z8jt",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:08.420470Z",
     "iopub.status.busy": "2026-02-24T16:59:08.420380Z",
     "iopub.status.idle": "2026-02-24T16:59:08.426366Z",
     "shell.execute_reply": "2026-02-24T16:59:08.425884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cleared: calib_state_cond_h1_N55_seed42.pt\n",
      "  Cleared: calib_state_free_h1_N55_seed42.pt\n",
      "  Cleared: seed_state_cond_h1_N55_seed7.pt\n",
      "  Cleared: seed_state_cond_h1_N55_seed42.pt\n",
      "  Cleared: seed_state_cond_h1_N55_seed123.pt\n",
      "  Cleared: seed_state_free_h1_N55_seed7.pt\n",
      "  Cleared: seed_state_free_h1_N55_seed42.pt\n",
      "  Cleared: seed_state_free_h1_N55_seed123.pt\n",
      "  Cleared: loglik_state_cond_h1_N55_seed7.npy\n",
      "  Cleared: loglik_state_cond_h1_N55_seed123.npy\n",
      "  Cleared: loglik_state_cond_h1_N55_seed42.npy\n",
      "  Cleared: loglik_state_free_h1_N55_seed7.npy\n",
      "  Cleared: loglik_state_free_h1_N55_seed42.npy\n",
      "  Cleared: loglik_state_free_h1_N55_seed123.npy\n",
      "Cleared 14 stale h=1 cumulative cache files.\n",
      "CK h=1 weights preserved.\n"
     ]
    }
   ],
   "source": [
    "# Clear stale h=1 cumulative cache files (label definition was corrected)\n",
    "# CK h=1 weights (ck_*) and A_t arrays are NOT cleared — CK label unchanged\n",
    "stale_patterns = [\n",
    "    \"calib_state_cond_h1_*\",\n",
    "    \"calib_state_free_h1_*\",\n",
    "    \"seed_state_cond_h1_*\",\n",
    "    \"seed_state_free_h1_*\",\n",
    "    \"loglik_state_cond_h1_*\",\n",
    "    \"loglik_state_free_h1_*\",\n",
    "    \"h1_fresh_*\",  # from any prior partial run with corrected label\n",
    "]\n",
    "\n",
    "n_cleared = 0\n",
    "for pat in stale_patterns:\n",
    "    for f in CACHE_DIR.glob(pat):\n",
    "        f.unlink()\n",
    "        print(f\"  Cleared: {f.name}\")\n",
    "        n_cleared += 1\n",
    "\n",
    "if n_cleared == 0:\n",
    "    print(\"No stale h=1 cumulative cache files found (already clean).\")\n",
    "else:\n",
    "    print(f\"Cleared {n_cleared} stale h=1 cumulative cache files.\")\n",
    "print(\"CK h=1 weights preserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596adc4b",
   "metadata": {},
   "source": [
    "## Section A: Degeneracy & Small-Signal Rigor\n",
    "\n",
    "**Motivation:** With only ~2,300 training days and 55×55 = 3,025 possible state-to-state \n",
    "transitions, count-based methods suffer severe degeneracy. This section quantifies that\n",
    "degeneracy and motivates the neural regularized approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca78ef6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:08.427668Z",
     "iopub.status.busy": "2026-02-24T16:59:08.427575Z",
     "iopub.status.idle": "2026-02-24T16:59:08.434938Z",
     "shell.execute_reply": "2026-02-24T16:59:08.434400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved degeneracy_label_table.csv\n",
      " h  N  N_actual  n_train  effective_bins  frac_below_5  frac_below_10\n",
      " 1 10        10     1656              10           0.0            0.0\n",
      " 1 20        20     1656              20           0.0            0.0\n",
      " 1 35        35     1656              35           0.0            0.0\n",
      " 1 55        55     1656              55           0.0            0.0\n",
      " 2 10        10     1656              10           0.0            0.0\n",
      " 2 20        20     1656              20           0.0            0.0\n",
      " 2 35        35     1656              35           0.0            0.0\n",
      " 2 55        55     1656              55           0.0            0.0\n",
      " 5 10        10     1654              10           0.0            0.0\n",
      " 5 20        20     1654              20           0.0            0.0\n",
      " 5 35        35     1654              35           0.0            0.0\n",
      " 5 55        55     1654              55           0.0            0.0\n",
      "10 10        10     1650              10           0.0            0.0\n",
      "10 20        20     1650              20           0.0            0.0\n",
      "10 35        35     1650              35           0.0            0.0\n",
      "10 55        55     1650              55           0.0            0.0\n"
     ]
    }
   ],
   "source": [
    "deg_label_rows = []\n",
    "for h in cfg.horizons:\n",
    "    sp = splits[h]\n",
    "    for N in cfg.n_bins_list:\n",
    "        cfg_key = (h, N)\n",
    "        if cfg_key not in configs:\n",
    "            continue\n",
    "        c = configs[cfg_key]\n",
    "        y_tr = c[\"y_all\"][sp[\"idx_train\"]]\n",
    "        N_actual = c[\"N_actual\"]\n",
    "        stats = compute_degeneracy_stats(y_tr, N_actual, thresholds=tuple(cfg.sparsity_thresh))\n",
    "        row = {\"h\": h, \"N\": N, \"N_actual\": N_actual,\n",
    "               \"n_train\": len(sp[\"idx_train\"]), \"effective_bins\": stats[\"effective\"]}\n",
    "        for k, frac in stats[\"frac_below\"].items():\n",
    "            row[f\"frac_below_{k}\"] = frac\n",
    "        deg_label_rows.append(row)\n",
    "\n",
    "df_deg_label = pd.DataFrame(deg_label_rows)\n",
    "df_deg_label.to_csv(OUT_DIR / \"degeneracy_label_table.csv\", index=False)\n",
    "print(\"Saved degeneracy_label_table.csv\")\n",
    "print(df_deg_label.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b606bd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:08.436176Z",
     "iopub.status.busy": "2026-02-24T16:59:08.436085Z",
     "iopub.status.idle": "2026-02-24T16:59:08.451026Z",
     "shell.execute_reply": "2026-02-24T16:59:08.450589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Leakage check h=1 N=55: fraction(X_t==Y_t) = 0.0229 (OK)\n",
      "Saved degeneracy_transition_table.csv\n",
      "config_type  h  N  N_actual  frac_cells_zero  frac_cells_lt5  median_nonzero_per_row  p90_nonzero_per_row  median_row_entropy_empirical  median_row_maxprob_empirical\n",
      " cumulative  1 10        10         0.056364        0.827273                    10.0                 10.0                      2.130573                      0.200000\n",
      " cumulative  1 20        20         0.224545        0.980000                    16.0                 17.0                      2.626957                      0.133333\n",
      " cumulative  1 35        35         0.436364        0.995844                    20.0                 22.0                      2.880478                      0.100000\n",
      " cumulative  1 55        55         0.579504        0.998678                    23.0                 26.0                      3.060287                      0.100000\n",
      " cumulative  2 10        10         0.034545        0.807273                    10.0                 10.0                      2.136688                      0.200000\n",
      " cumulative  2 20        20         0.213636        0.976364                    16.0                 17.0                      2.609516                      0.133333\n",
      " cumulative  2 35        35         0.423377        0.996883                    20.0                 22.6                      2.908318                      0.100000\n",
      " cumulative  2 55        55         0.582149        0.999339                    23.0                 25.0                      3.077729                      0.100000\n",
      " cumulative  5 10        10         0.049091        0.827273                    10.0                 10.0                      2.154129                      0.193548\n",
      " cumulative  5 20        20         0.218182        0.977273                    16.0                 17.0                      2.624650                      0.133333\n",
      " cumulative  5 35        35         0.423896        0.996883                    20.0                 23.0                      2.886774                      0.100000\n",
      " cumulative  5 55        55         0.582479        0.999669                    23.0                 26.0                      3.060287                      0.100000\n",
      " cumulative 10 10        10         0.036364        0.821818                    10.0                 10.0                      2.141302                      0.200000\n",
      " cumulative 10 20        20         0.207273        0.980000                    16.0                 18.0                      2.638284                      0.133333\n",
      " cumulative 10 35        35         0.416623        0.997922                    21.0                 23.0                      2.936159                      0.100000\n",
      " cumulative 10 55        55         0.570248        0.999669                    24.0                 26.0                      3.106497                      0.100000\n",
      "         ck  1 55        55         0.578843        0.998678                    23.0                 26.0                      3.060287                      0.100000\n",
      "         ck  2 55        55         0.577190        0.999339                    23.0                 26.6                      3.060287                      0.100000\n",
      "         ck  5 55        55         0.577851        0.999008                    23.0                 25.0                      3.077729                      0.100000\n",
      "         ck 10 55        55         0.581157        0.999008                    23.0                 25.0                      3.077729                      0.100000\n"
     ]
    }
   ],
   "source": [
    "sparsity_rows = []\n",
    "sparsity_data = {}  # for plotting: {(h, N): stats}\n",
    "\n",
    "CELL_METRICS = [\n",
    "    \"frac_cells_zero\",\n",
    "    \"frac_cells_lt5\",\n",
    "    \"median_nonzero_per_row\",\n",
    "    \"p90_nonzero_per_row\",\n",
    "    \"median_row_entropy_empirical\",\n",
    "    \"median_row_maxprob_empirical\",\n",
    "]\n",
    "\n",
    "# Part A: cumulative configs\n",
    "for h in cfg.horizons:\n",
    "    sp = splits[h]\n",
    "    T_h = sp[\"T_h\"]\n",
    "    idx_tr = sp[\"idx_train\"]\n",
    "    for N in cfg.n_bins_list:\n",
    "        cfg_key = (h, N)\n",
    "        if cfg_key not in configs:\n",
    "            continue\n",
    "        c = configs[cfg_key]\n",
    "        N_actual = c[\"N_actual\"]\n",
    "        X_aligned = X_t_all[:T_h][idx_tr]\n",
    "        Y_aligned = c[\"y_all\"][:T_h][idx_tr]\n",
    "        # Leakage check: h=1 label must NOT be identical to state after label fix\n",
    "        if h == 1:\n",
    "            frac_eq = float(np.mean(X_aligned == Y_aligned))\n",
    "            assert frac_eq < 0.99, (\n",
    "                f\"LEAKAGE at h=1, N={N}: fraction(X_t==Y_t) = {frac_eq:.4f}. \"\n",
    "                \"Expected < 0.99 with corrected label (Y_t = next-day return after X_t).\"\n",
    "            )\n",
    "            if N == cfg.n_bins_list[-1]:\n",
    "                print(f\"  Leakage check h=1 N={N}: fraction(X_t==Y_t) = {frac_eq:.4f} (OK)\")\n",
    "        stats = compute_transition_sparsity(X_aligned, Y_aligned, N_XT, N_actual)\n",
    "        sparsity_data[(h, N)] = stats\n",
    "        row = {\"config_type\": \"cumulative\", \"h\": h, \"N\": N, \"N_actual\": N_actual}\n",
    "        for m in CELL_METRICS:\n",
    "            row[m] = stats[m]\n",
    "        sparsity_rows.append(row)\n",
    "\n",
    "# Part B: CK configs (N_output = N_XT = 55, label = X_{t+h})\n",
    "# Use CK-specific splits so indices are capped to T_ck = len(X_t_all) - h\n",
    "for h in cfg.ck_horizons:\n",
    "    sp_ck = ck_splits[h]\n",
    "    T_ck = sp_ck[\"T_ck\"]\n",
    "    idx_tr = sp_ck[\"idx_train\"]\n",
    "    y_ck = ck_labels[h]\n",
    "    X_aligned = X_t_all[:T_ck][idx_tr]\n",
    "    Y_aligned  = y_ck[idx_tr]\n",
    "    stats = compute_transition_sparsity(X_aligned, Y_aligned, N_XT, N_XT)\n",
    "    sparsity_data[(h, \"ck\")] = stats\n",
    "    row = {\"config_type\": \"ck\", \"h\": h, \"N\": N_XT, \"N_actual\": N_XT}\n",
    "    for m in CELL_METRICS:\n",
    "        row[m] = stats[m]\n",
    "    sparsity_rows.append(row)\n",
    "\n",
    "df_sparsity = pd.DataFrame(sparsity_rows)\n",
    "df_sparsity.to_csv(OUT_DIR / \"degeneracy_transition_table.csv\", index=False)\n",
    "print(\"Saved degeneracy_transition_table.csv\")\n",
    "print(df_sparsity.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7532d056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:08.452226Z",
     "iopub.status.busy": "2026-02-24T16:59:08.452140Z",
     "iopub.status.idle": "2026-02-24T16:59:08.900449Z",
     "shell.execute_reply": "2026-02-24T16:59:08.900042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section A figures saved.\n"
     ]
    }
   ],
   "source": [
    "thresh = min(cfg.sparsity_thresh)\n",
    "fig_a = plot_sparsity_vs_N(sparsity_data, cfg.horizons, cfg.n_bins_list, thresh, FIG_DIR)\n",
    "plt.close(fig_a)\n",
    "fig_b = plot_transition_sparsity_table(df_sparsity, thresh, FIG_DIR)\n",
    "plt.close(fig_b)\n",
    "print(\"Section A figures saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe86f1",
   "metadata": {},
   "source": [
    "## Section B: Chapman–Kolmogorov Consistency (Diagnostic)\n",
    "\n",
    "**Label definition:** y_t^(h) := X_{t+h} (the 1-day return bin h steps ahead).  \n",
    "This is NOT the cumulative h-step return. It places all A_t^(h) in the same 55×55 state space,\n",
    "making matrix multiplication valid.\n",
    "\n",
    "**CK test (time-inhomogeneous):**  \n",
    "A_composed_t^(h) = A_t^(1) × A_{t+1}^(1) × ... × A_{t+h-1}^(1)  \n",
    "Compare against directly predicted A_t^(h). Metrics: mean KL, mean TV, Frobenius.\n",
    "\n",
    "**Note:** StateFree A_t has degenerate dynamics (all rows identical at each t).  \n",
    "This is expected and means StateFree CK error reflects purely time-driven dynamics.\n",
    "\n",
    "**Why CK may fail here:** 2,369 days → 3,025 possible state-to-state transitions (severe degeneracy);\n",
    "weak state signal (low MI); time-inhomogeneity from regime changes; discretization artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e3df35a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:08.901826Z",
     "iopub.status.busy": "2026-02-24T16:59:08.901738Z",
     "iopub.status.idle": "2026-02-24T16:59:09.051965Z",
     "shell.execute_reply": "2026-02-24T16:59:09.051428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached CK model: state_cond h=1\n",
      "Loaded cached A_t: state_cond h=1, shape=(2367, 55, 55)\n",
      "Loaded cached CK model: state_free h=1\n",
      "Loaded cached A_t: state_free h=1, shape=(2367, 55, 55)\n",
      "Loaded cached CK model: state_cond h=2\n",
      "Loaded cached A_t: state_cond h=2, shape=(2366, 55, 55)\n",
      "Loaded cached CK model: state_free h=2\n",
      "Loaded cached A_t: state_free h=2, shape=(2366, 55, 55)\n",
      "Loaded cached CK model: state_cond h=5\n",
      "Loaded cached A_t: state_cond h=5, shape=(2363, 55, 55)\n",
      "Loaded cached CK model: state_free h=5\n",
      "Loaded cached A_t: state_free h=5, shape=(2363, 55, 55)\n",
      "Loaded cached CK model: state_cond h=10\n",
      "Loaded cached A_t: state_cond h=10, shape=(2358, 55, 55)\n",
      "Loaded cached CK model: state_free h=10\n",
      "Loaded cached A_t: state_free h=10, shape=(2358, 55, 55)\n",
      "Section B: all CK models and A_t matrices ready.\n"
     ]
    }
   ],
   "source": [
    "ck_models = {}   # {(model_type, h): model}\n",
    "A_t_ck = {}     # {(model_type, h): np.ndarray (T_ck, 55, 55)}\n",
    "\n",
    "for h in cfg.ck_horizons:\n",
    "    # Use CK-specific splits: T_ck = len(X_t_all) - h\n",
    "    sp_ck = ck_splits[h]\n",
    "    T_ck  = sp_ck[\"T_ck\"]\n",
    "    idx_tr = sp_ck[\"idx_train\"]\n",
    "    idx_va = sp_ck[\"idx_val\"]\n",
    "    idx_te = sp_ck[\"idx_test\"]\n",
    "    y_ck = ck_labels[h]   # length T_ck\n",
    "\n",
    "    # Build data loaders for CK task\n",
    "    train_loader, val_loader, test_loader = build_ck_loaders(\n",
    "        F_normed, X_t_all, y_ck, idx_tr, idx_va, idx_te,\n",
    "        batch_train=cfg.batch_train, batch_eval=cfg.batch_eval,\n",
    "    )\n",
    "\n",
    "    # Sigma for CK task (output = 55 bins, same space as input)\n",
    "    R_h = compute_returns(prices, h)\n",
    "    R_tr = R_h[idx_tr]\n",
    "    _, edges_h = get_edges(R_tr, N_XT)\n",
    "    sigma_ck = compute_sigma(edges_h, edges_xt, cfg.sigma_anchor)\n",
    "\n",
    "    for model_type in [\"state_cond\", \"state_free\"]:\n",
    "        weight_path = CACHE_DIR / f\"ck_{model_type}_h{h}_seed{SEED}.pt\"\n",
    "        A_path = CACHE_DIR / f\"A_t_ck_{model_type}_h{h}.npy\"\n",
    "\n",
    "        if model_type == \"state_cond\":\n",
    "            model = StateConditionedNet(n_features, N_XT, N_XT,\n",
    "                                        hidden_dims=cfg.hidden_dims, dropout=cfg.dropout)\n",
    "        else:\n",
    "            model = StateFreeNet(n_features, N_XT,\n",
    "                                  hidden_dims=cfg.hidden_dims, dropout=cfg.dropout)\n",
    "\n",
    "        torch.manual_seed(SEED)\n",
    "        if is_cached(weight_path):\n",
    "            load_cached_model(model, weight_path)\n",
    "            print(f\"Loaded cached CK model: {model_type} h={h}\")\n",
    "        else:\n",
    "            print(f\"Training CK model: {model_type} h={h} (T_ck={T_ck})...\")\n",
    "            best_state, _ = train_one_run(\n",
    "                model, train_loader, val_loader, N_XT, sigma_ck, DEVICE,\n",
    "                lr=cfg.lr, weight_decay=cfg.weight_decay,\n",
    "                max_epochs=cfg.max_epochs, patience=cfg.patience,\n",
    "                grad_clip=cfg.grad_clip, verbose=True,\n",
    "            )\n",
    "            cache_model(best_state, weight_path)\n",
    "\n",
    "        model.eval()\n",
    "        ck_models[(model_type, h)] = model\n",
    "\n",
    "        if is_cached(A_path):\n",
    "            A_t = np.load(A_path)\n",
    "            print(f\"Loaded cached A_t: {model_type} h={h}, shape={A_t.shape}\")\n",
    "        else:\n",
    "            print(f\"Building A_t matrices: {model_type} h={h} ...\")\n",
    "            # Build A_t for the CK-valid time range [0, T_ck)\n",
    "            full_indices = np.arange(T_ck)\n",
    "            if model_type == \"state_cond\":\n",
    "                A_t = build_A_t_neural(model, F_normed, full_indices, N_XT, N_XT, DEVICE)\n",
    "            else:\n",
    "                A_t = build_A_t_statefree(model, F_normed, full_indices, N_XT, N_XT, DEVICE)\n",
    "            np.save(A_path, A_t)\n",
    "            print(f\"  Saved A_t shape={A_t.shape}\")\n",
    "\n",
    "        A_t_ck[(model_type, h)] = A_t\n",
    "\n",
    "print(\"Section B: all CK models and A_t matrices ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb9c288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:09.053628Z",
     "iopub.status.busy": "2026-02-24T16:59:09.053503Z",
     "iopub.status.idle": "2026-02-24T16:59:09.430965Z",
     "shell.execute_reply": "2026-02-24T16:59:09.430497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CK h=2 state_cond: KL=0.1586 TV=0.1955\n",
      "  CK h=2 state_free: KL=0.0319 TV=0.0897\n",
      "  CK h=5 state_cond: KL=0.1469 TV=0.2044\n",
      "  CK h=5 state_free: KL=0.0338 TV=0.0919\n",
      "  CK h=10 state_cond: KL=0.1639 TV=0.1999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CK h=10 state_free: KL=0.0317 TV=0.0878\n",
      "Saved ck_table.csv\n",
      "     model  h  mean_kl  mean_tv  frobenius                                note\n",
      "state_cond  1 0.000000 0.000000   0.000000 h=1: trivial (identity composition)\n",
      "state_free  1 0.000000 0.000000   0.000000 h=1: trivial (identity composition)\n",
      "state_cond  2 0.158617 0.195489   0.450984                                 NaN\n",
      "state_free  2 0.031927 0.089723   0.231940                                 NaN\n",
      "backoff_ck  2 0.010630 0.063744   0.150172                                 NaN\n",
      "state_cond  5 0.146890 0.204359   0.468521                                 NaN\n",
      "state_free  5 0.033824 0.091874   0.237193                                 NaN\n",
      "backoff_ck  5 0.002853 0.032793   0.076698                                 NaN\n",
      "state_cond 10 0.163862 0.199948   0.461650                                 NaN\n",
      "state_free 10 0.031678 0.087811   0.228133                                 NaN\n",
      "backoff_ck 10 0.000098 0.005980   0.014015                                 NaN\n"
     ]
    }
   ],
   "source": [
    "ck_table_rows = []\n",
    "ck_time_dict = {}\n",
    "\n",
    "for h in cfg.ck_horizons:\n",
    "    # Use CK-specific splits\n",
    "    sp_ck = ck_splits[h]\n",
    "    T_ck  = sp_ck[\"T_ck\"]\n",
    "    idx_tr = sp_ck[\"idx_train\"]\n",
    "    idx_te = sp_ck[\"idx_test\"]\n",
    "    y_ck = ck_labels[h]   # length T_ck\n",
    "\n",
    "    # CK backoff baseline on X_t -> X_{t+h} (55x55)\n",
    "    X_tr = X_t_all[:T_ck][idx_tr]\n",
    "    Y_tr = y_ck[idx_tr]\n",
    "    marginal_ck = _compute_marginal(Y_tr, N_XT)\n",
    "    best_bk_ll, best_alpha_bk, best_tau_bk = -np.inf, None, None\n",
    "    X_va = X_t_all[:T_ck][sp_ck[\"idx_val\"]]\n",
    "    Y_va = y_ck[sp_ck[\"idx_val\"]]\n",
    "    for alpha in cfg.alpha_grid:\n",
    "        for tau in cfg.tau_grid:\n",
    "            A_bk, _, _ = _build_backoff_matrix(X_tr, Y_tr, N_XT, N_XT, alpha, tau, marginal_ck)\n",
    "            ll = mean_log_likelihood(A_bk[X_va], Y_va)\n",
    "            if ll > best_bk_ll:\n",
    "                best_bk_ll, best_alpha_bk, best_tau_bk = ll, alpha, tau\n",
    "    A_bk_ck, _, _ = _build_backoff_matrix(X_tr, Y_tr, N_XT, N_XT,\n",
    "                                            best_alpha_bk, best_tau_bk, marginal_ck)\n",
    "\n",
    "    # State weights (train visitation)\n",
    "    state_counts = np.bincount(X_tr, minlength=N_XT).astype(np.float64)\n",
    "\n",
    "    # Build A_t arrays on test window (slice from full A_t arrays)\n",
    "    test_start = idx_te[0]\n",
    "    test_end   = idx_te[-1] + 1\n",
    "    # For CK composition, need h steps starting at each test point\n",
    "    max_test_for_ck = T_ck - h + 1  # last valid start for h-step composition\n",
    "\n",
    "    models_to_test = {\n",
    "        \"state_cond\": A_t_ck[(\"state_cond\", h)],\n",
    "        \"state_free\":  A_t_ck[(\"state_free\", h)],\n",
    "    }\n",
    "\n",
    "    for model_name, A_full in models_to_test.items():\n",
    "        # A_full: (T_ck, 55, 55)\n",
    "        A1_full = A_t_ck[(model_name, 1)] if 1 in cfg.ck_horizons else None\n",
    "\n",
    "        if h == 1:\n",
    "            # CK trivially satisfied for h=1 (compare A^(1) with itself)\n",
    "            n_te = len(idx_te)\n",
    "            ck_time_dict[(model_name, h)] = np.zeros(n_te)\n",
    "            ck_table_rows.append({\n",
    "                \"model\": model_name, \"h\": h,\n",
    "                \"mean_kl\": 0.0, \"mean_tv\": 0.0, \"frobenius\": 0.0,\n",
    "                \"note\": \"h=1: trivial (identity composition)\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if A1_full is None:\n",
    "            print(f\"  Skipping CK for h={h} (h=1 model not available)\")\n",
    "            continue\n",
    "\n",
    "        # Align test window: cap at max_test_for_ck\n",
    "        valid_te_end = min(test_end, max_test_for_ck)\n",
    "        if valid_te_end <= test_start:\n",
    "            print(f\"  Skipping h={h} model={model_name}: test window too small for CK\")\n",
    "            continue\n",
    "\n",
    "        A_h_te  = A_full[test_start:valid_te_end]          # direct h-step\n",
    "        # For composition we need A^(1) at t, t+1, ..., t+h-1\n",
    "        # A1_full has length T_ck for h=1, but T_ck(h=1) >= T_ck(h) + h - 1\n",
    "        n_needed = valid_te_end + h - 1\n",
    "        A1_window = A1_full[test_start:min(n_needed, len(A1_full))]\n",
    "        A_comp = build_ck_composed(A1_window, h)    # (len(A1_window) - h + 1, 55, 55)\n",
    "\n",
    "        n_common = min(len(A_h_te), len(A_comp))\n",
    "        if n_common == 0:\n",
    "            print(f\"  Skipping h={h} model={model_name}: no common time steps\")\n",
    "            continue\n",
    "        errors = compute_ck_errors(A_h_te[:n_common], A_comp[:n_common], state_counts)\n",
    "        ck_time_dict[(model_name, h)] = errors[\"per_time_kl\"]\n",
    "        ck_table_rows.append({\n",
    "            \"model\": model_name, \"h\": h,\n",
    "            \"mean_kl\": errors[\"mean_kl\"],\n",
    "            \"mean_tv\": errors[\"mean_tv\"],\n",
    "            \"frobenius\": errors[\"frobenius\"],\n",
    "        })\n",
    "        print(f\"  CK h={h} {model_name}: KL={errors['mean_kl']:.4f} TV={errors['mean_tv']:.4f}\")\n",
    "\n",
    "    # Backoff: build its A_t by expanding matrix (same matrix for all t)\n",
    "    A_bk_expanded = np.tile(A_bk_ck[None], (T_ck, 1, 1))\n",
    "    if h > 1:\n",
    "        valid_te_end_bk = min(test_end, max_test_for_ck)\n",
    "        if valid_te_end_bk > test_start:\n",
    "            A_comp_bk = build_ck_composed(\n",
    "                A_bk_expanded[test_start:min(valid_te_end_bk + h - 1, T_ck)], h\n",
    "            )\n",
    "            A_h_bk = A_bk_expanded[test_start:valid_te_end_bk]\n",
    "            n_common = min(len(A_h_bk), len(A_comp_bk))\n",
    "            if n_common > 0:\n",
    "                errors_bk = compute_ck_errors(A_h_bk[:n_common], A_comp_bk[:n_common], state_counts)\n",
    "                ck_time_dict[(\"backoff_ck\", h)] = errors_bk[\"per_time_kl\"]\n",
    "                ck_table_rows.append({\n",
    "                    \"model\": \"backoff_ck\", \"h\": h,\n",
    "                    \"mean_kl\": errors_bk[\"mean_kl\"],\n",
    "                    \"mean_tv\": errors_bk[\"mean_tv\"],\n",
    "                    \"frobenius\": errors_bk[\"frobenius\"],\n",
    "                })\n",
    "\n",
    "df_ck = pd.DataFrame(ck_table_rows)\n",
    "df_ck.to_csv(OUT_DIR / \"ck_table.csv\", index=False)\n",
    "print(\"Saved ck_table.csv\")\n",
    "print(df_ck.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3460e534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:09.432647Z",
     "iopub.status.busy": "2026-02-24T16:59:09.432523Z",
     "iopub.status.idle": "2026-02-24T16:59:09.503861Z",
     "shell.execute_reply": "2026-02-24T16:59:09.503332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationarity probe (NOT a CK test): (A_avg^(1))^h vs direct A^(h)\n",
      "  h=2: stationarity probe KL=0.0031\n",
      "  h=5: stationarity probe KL=0.0193\n",
      "  h=10: stationarity probe KL=0.0051\n"
     ]
    }
   ],
   "source": [
    "print(\"Stationarity probe (NOT a CK test): (A_avg^(1))^h vs direct A^(h)\")\n",
    "stationarity_rows = []\n",
    "for h in cfg.ck_horizons:\n",
    "    if h == 1 or (\"state_cond\", 1) not in A_t_ck:\n",
    "        continue\n",
    "    sp_ck_h = ck_splits[h]\n",
    "    sp_ck_1 = ck_splits[1]\n",
    "    A1_full = A_t_ck[(\"state_cond\", 1)]\n",
    "    # Average A^(1) over h=1 train window\n",
    "    A1_train = A1_full[sp_ck_1[\"idx_train\"]]\n",
    "    A_avg = A1_train.mean(axis=0)  # (55, 55)\n",
    "\n",
    "    import numpy.linalg as nla\n",
    "    A_avg_h = nla.matrix_power(A_avg, h)\n",
    "\n",
    "    # Compare with direct h-step prediction on h-step test window\n",
    "    idx_te = sp_ck_h[\"idx_test\"]\n",
    "    A_h_direct = A_t_ck[(\"state_cond\", h)][idx_te]\n",
    "    A_avg_h_tiled = np.tile(A_avg_h[None], (len(A_h_direct), 1, 1))\n",
    "    errors_stat = compute_ck_errors(A_h_direct, A_avg_h_tiled)\n",
    "    stationarity_rows.append({\n",
    "        \"h\": h, \"mean_kl\": errors_stat[\"mean_kl\"],\n",
    "        \"note\": \"Stationarity probe (NOT CK) — avg A^(1) raised to power h\"\n",
    "    })\n",
    "    print(f\"  h={h}: stationarity probe KL={errors_stat['mean_kl']:.4f}\")\n",
    "\n",
    "df_stat = pd.DataFrame(stationarity_rows)\n",
    "if len(df_stat):\n",
    "    df_stat.to_csv(OUT_DIR / \"stationarity_probe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddb2758f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:09.505265Z",
     "iopub.status.busy": "2026-02-24T16:59:09.505160Z",
     "iopub.status.idle": "2026-02-24T16:59:10.083835Z",
     "shell.execute_reply": "2026-02-24T16:59:10.083328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section B figures saved.\n"
     ]
    }
   ],
   "source": [
    "if len(df_ck) > 0:\n",
    "    fig_ck = plot_ck_error_summary(df_ck, FIG_DIR)\n",
    "    plt.close(fig_ck)\n",
    "if ck_time_dict:\n",
    "    fig_ck_t = plot_ck_time_series(ck_time_dict, FIG_DIR)\n",
    "    plt.close(fig_ck_t)\n",
    "print(\"Section B figures saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60812e6b",
   "metadata": {},
   "source": [
    "## Section C: Operator Interpretability Diagnostics\n",
    "\n",
    "Using the CK h=1 models, we compute A_t^(1) over the full time series and extract\n",
    "measurable diagnostics:\n",
    "\n",
    "- **Dobrushin coefficient** δ(A_t): contraction measure\n",
    "- **Row heterogeneity** ρ(A_t): average pairwise TV between rows — state-dependence strength\n",
    "- **Row entropy** H(A_t): diffuseness of transitions\n",
    "- **Spectral mixing proxy**: σ_max of lazy deflated operator (NaN-safe)\n",
    "\n",
    "We then identify 2–3 regime windows and show A_t snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3200aec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:10.085227Z",
     "iopub.status.busy": "2026-02-24T16:59:10.085138Z",
     "iopub.status.idle": "2026-02-24T16:59:24.172896Z",
     "shell.execute_reply": "2026-02-24T16:59:24.172089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing diagnostics for state_cond (2367 time steps)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dobrushin:  mean=0.0294  max=0.0571\n",
      "  RowHet:     mean=0.0074\n",
      "  RowEntropy: mean=3.9243\n",
      "  SpectralProxy: 100.0% finite, mean=0.0140\n",
      "Computing diagnostics for state_free (2367 time steps)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dobrushin:  mean=0.0000  max=0.0000\n",
      "  RowHet:     mean=0.0000\n",
      "  RowEntropy: mean=3.9515\n",
      "  SpectralProxy: 100.0% finite, mean=0.0000\n"
     ]
    }
   ],
   "source": [
    "diag_series = {}  # {model_type: {\"dobrushin\": array, ...}}\n",
    "\n",
    "for model_type in [\"state_cond\", \"state_free\"]:\n",
    "    if (\"state_cond\", 1) not in A_t_ck and model_type == \"state_cond\":\n",
    "        continue\n",
    "    if (\"state_free\", 1) not in A_t_ck and model_type == \"state_free\":\n",
    "        continue\n",
    "\n",
    "    A_full = A_t_ck[(model_type, 1)]  # (T_h, 55, 55)\n",
    "    print(f\"Computing diagnostics for {model_type} ({len(A_full)} time steps)...\")\n",
    "\n",
    "    dob  = compute_dobrushin(A_full)\n",
    "    rhet = compute_row_heterogeneity(A_full)\n",
    "    rent = compute_row_entropy(A_full)\n",
    "    spec = compute_spectral_mixing_proxy(A_full)\n",
    "\n",
    "    diag_series[model_type] = {\n",
    "        \"dobrushin\": dob,\n",
    "        \"row_heterogeneity\": rhet,\n",
    "        \"row_entropy\": rent,\n",
    "        \"spectral_proxy\": spec,\n",
    "    }\n",
    "    pct_finite = np.isfinite(spec).mean() * 100\n",
    "    print(f\"  Dobrushin:  mean={dob.mean():.4f}  max={dob.max():.4f}\")\n",
    "    print(f\"  RowHet:     mean={rhet.mean():.4f}\")\n",
    "    print(f\"  RowEntropy: mean={rent.mean():.4f}\")\n",
    "    print(f\"  SpectralProxy: {pct_finite:.1f}% finite, mean={np.nanmean(spec):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "375c46fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:24.174952Z",
     "iopub.status.busy": "2026-02-24T16:59:24.174839Z",
     "iopub.status.idle": "2026-02-24T16:59:25.197290Z",
     "shell.execute_reply": "2026-02-24T16:59:25.196814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regime windows: [(np.int64(193), np.int64(223), 't=208'), (np.int64(238), np.int64(268), 't=253'), (np.int64(282), np.int64(312), 't=297')]\n"
     ]
    }
   ],
   "source": [
    "# Regime detection: top-3 peaks in state_cond dobrushin\n",
    "if \"state_cond\" in diag_series:\n",
    "    dob_series = diag_series[\"state_cond\"][\"dobrushin\"]\n",
    "    T_full = len(dob_series)\n",
    "\n",
    "    # Simple peak detection: top-3 local maxima\n",
    "    from scipy.signal import find_peaks\n",
    "    peaks, _ = find_peaks(dob_series, distance=30)\n",
    "    top_peaks = peaks[np.argsort(dob_series[peaks])[::-1][:3]] if len(peaks) >= 3 else peaks\n",
    "    top_peaks = sorted(top_peaks)\n",
    "\n",
    "    # Define regime windows: ±15 days around each peak\n",
    "    regime_windows = []\n",
    "    for pk in top_peaks:\n",
    "        start = max(0, pk - 15)\n",
    "        end   = min(T_full - 1, pk + 15)\n",
    "        regime_windows.append((start, end, f\"t={pk}\"))\n",
    "\n",
    "    # Snapshot heatmaps\n",
    "    for model_type in [\"state_cond\", \"state_free\"]:\n",
    "        if (model_type, 1) not in A_t_ck:\n",
    "            continue\n",
    "        A_full = A_t_ck[(model_type, 1)]\n",
    "        for pk in top_peaks:\n",
    "            if pk < len(A_full):\n",
    "                plot_At_heatmap_snapshot(A_full[pk], f\"t{pk}\", FIG_DIR, model_name=model_type)\n",
    "                plt.close(\"all\")\n",
    "\n",
    "    print(f\"Regime windows: {regime_windows}\")\n",
    "else:\n",
    "    regime_windows = []\n",
    "    print(\"No state_cond diagnostics available — skipping regime analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9535f5dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:25.198651Z",
     "iopub.status.busy": "2026-02-24T16:59:25.198518Z",
     "iopub.status.idle": "2026-02-24T16:59:26.038294Z",
     "shell.execute_reply": "2026-02-24T16:59:26.037759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section C figures saved.\n"
     ]
    }
   ],
   "source": [
    "dob_dict  = {k: v[\"dobrushin\"]        for k, v in diag_series.items()}\n",
    "rhet_dict = {k: v[\"row_heterogeneity\"] for k, v in diag_series.items()}\n",
    "rent_dict = {k: v[\"row_entropy\"]       for k, v in diag_series.items()}\n",
    "spec_dict = {k: v[\"spectral_proxy\"]    for k, v in diag_series.items()}\n",
    "\n",
    "for plot_fn, data, name in [\n",
    "    (plot_dobrushin_over_time, dob_dict, \"dobrushin\"),\n",
    "    (plot_row_heterogeneity_over_time, rhet_dict, \"row_het\"),\n",
    "    (plot_entropy_over_time, rent_dict, \"entropy\"),\n",
    "]:\n",
    "    fig = plot_fn(data, FIG_DIR, regime_windows=regime_windows)\n",
    "    plt.close(fig)\n",
    "\n",
    "fig_spec = plot_spectral_proxy_over_time(spec_dict, FIG_DIR, regime_windows=regime_windows)\n",
    "if fig_spec:\n",
    "    plt.close(fig_spec)\n",
    "\n",
    "# Regime panel\n",
    "if diag_series:\n",
    "    # Build combined diagnostics dataframe\n",
    "    rows = []\n",
    "    for model_type, d in diag_series.items():\n",
    "        T_m = len(d[\"dobrushin\"])\n",
    "        for t in range(T_m):\n",
    "            rows.append({\n",
    "                \"time_idx\": t, \"model\": model_type,\n",
    "                \"dobrushin\": d[\"dobrushin\"][t],\n",
    "                \"row_heterogeneity\": d[\"row_heterogeneity\"][t],\n",
    "                \"row_entropy\": d[\"row_entropy\"][t],\n",
    "            })\n",
    "    diag_df = pd.DataFrame(rows)\n",
    "    fig_rp = plot_regime_panel(diag_df, regime_windows, list(diag_series.keys()), FIG_DIR)\n",
    "    plt.close(fig_rp)\n",
    "\n",
    "print(\"Section C figures saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5568d",
   "metadata": {},
   "source": [
    "## Section D: Calibration\n",
    "\n",
    "We recompute predicted probabilities on the test set for configs (h=1, N=55) and (h=10, N=55).\n",
    "\n",
    "**master_grid_results.csv contains only scalar NLL — not predicted distributions —\n",
    "so calibration CANNOT be computed from it; fresh forward passes are required.**\n",
    "\n",
    "Cached model weights are loaded; if not present, those two configs are retrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76ea01b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:26.039813Z",
     "iopub.status.busy": "2026-02-24T16:59:26.039709Z",
     "iopub.status.idle": "2026-02-24T16:59:28.785050Z",
     "shell.execute_reply": "2026-02-24T16:59:28.784543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Retraining calibration model: state_cond h=1 N=55 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Retraining calibration model: state_free h=1 N=55 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved calibration_table.csv\n",
      " h  N      model  ece_neg_return  brier_neg_return\n",
      " 1 55 state_cond        0.056784          0.245284\n",
      " 1 55 state_free        0.039998          0.243862\n",
      " 1 55    backoff        0.080338          0.248459\n",
      "10 55 state_cond        0.077458          0.242760\n",
      "10 55 state_free        0.084981          0.243312\n",
      "10 55    backoff        0.111183          0.248061\n"
     ]
    }
   ],
   "source": [
    "calib_configs = [(1, 55), (10, 55)]\n",
    "calib_rows = []\n",
    "\n",
    "for (h, N) in calib_configs:\n",
    "    if (h, N) not in configs:\n",
    "        print(f\"Config (h={h}, N={N}) not in configs, skipping calibration.\")\n",
    "        continue\n",
    "    c = configs[(h, N)]\n",
    "    N_actual = c[\"N_actual\"]\n",
    "    sp = splits[h]\n",
    "\n",
    "    train_loader, val_loader, test_loader = build_loaders(\n",
    "        c, F_normed, X_t_all,\n",
    "        batch_train=cfg.batch_train, batch_eval=cfg.batch_eval,\n",
    "    )\n",
    "\n",
    "    y_te = c[\"y_all\"][sp[\"idx_test\"]]\n",
    "    X_te = X_t_all[sp[\"idx_test\"]]\n",
    "\n",
    "    for model_type in [\"state_cond\", \"state_free\"]:\n",
    "        weight_path = CACHE_DIR / f\"calib_{model_type}_h{h}_N{N}_seed{SEED}.pt\"\n",
    "\n",
    "        if model_type == \"state_cond\":\n",
    "            model = StateConditionedNet(n_features, N_XT, N_actual,\n",
    "                                        hidden_dims=cfg.hidden_dims, dropout=cfg.dropout)\n",
    "        else:\n",
    "            model = StateFreeNet(n_features, N_actual,\n",
    "                                  hidden_dims=cfg.hidden_dims, dropout=cfg.dropout)\n",
    "\n",
    "        torch.manual_seed(SEED)\n",
    "        if is_cached(weight_path):\n",
    "            load_cached_model(model, weight_path)\n",
    "        else:\n",
    "            print(f\"  Retraining calibration model: {model_type} h={h} N={N} ...\")\n",
    "            best_state, _ = train_one_run(\n",
    "                model, train_loader, val_loader, N_actual, c[\"sigma\"], DEVICE,\n",
    "                lr=cfg.lr, weight_decay=cfg.weight_decay,\n",
    "                max_epochs=cfg.max_epochs, patience=cfg.patience,\n",
    "                grad_clip=cfg.grad_clip,\n",
    "            )\n",
    "            cache_model(best_state, weight_path)\n",
    "\n",
    "        model.eval()\n",
    "        # Collect full test probs\n",
    "        all_probs = []\n",
    "        with torch.no_grad():\n",
    "            for F_b, xt_b, y_b in test_loader:\n",
    "                logits = model(F_b.to(DEVICE), xt_b.to(DEVICE))\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                all_probs.append(probs)\n",
    "        all_probs = np.vstack(all_probs)\n",
    "\n",
    "        # Event: negative return = bin < N_actual // 2\n",
    "        event_fn = lambda y, N=N_actual: y < N // 2\n",
    "\n",
    "        pit_vals = compute_pit(all_probs, y_te)\n",
    "        ece, conf_b, acc_b = compute_ece(all_probs, y_te, event_fn)\n",
    "        brier = compute_brier(all_probs, y_te, event_fn)\n",
    "\n",
    "        calib_rows.append({\n",
    "            \"h\": h, \"N\": N, \"model\": model_type,\n",
    "            \"ece_neg_return\": ece, \"brier_neg_return\": brier,\n",
    "        })\n",
    "\n",
    "        # Save PIT histogram\n",
    "        fig_pit = plot_pit_histogram(pit_vals, f\"{model_type}_h{h}_N{N}\", FIG_DIR)\n",
    "        plt.close(fig_pit)\n",
    "        # Save reliability curve\n",
    "        fig_rel = plot_reliability_curve(conf_b, acc_b, ece,\n",
    "                                         f\"negative_return_h{h}_N{N}_{model_type}\", FIG_DIR)\n",
    "        plt.close(fig_rel)\n",
    "\n",
    "    # Backoff baseline calibration\n",
    "    y_tr = c[\"y_all\"][sp[\"idx_train\"]]\n",
    "    X_tr = X_t_all[sp[\"idx_train\"]]\n",
    "    marginal = _compute_marginal(y_tr, N_actual)\n",
    "    A_bk, _, _ = _build_backoff_matrix(X_tr, y_tr, N_XT, N_actual,\n",
    "                                        1e-4, 100, marginal)\n",
    "    probs_bk = A_bk[X_te]\n",
    "    event_fn_bk = lambda y, N=N_actual: y < N // 2\n",
    "    ece_bk, _, _ = compute_ece(probs_bk, y_te, event_fn_bk)\n",
    "    brier_bk = compute_brier(probs_bk, y_te, event_fn_bk)\n",
    "    calib_rows.append({\n",
    "        \"h\": h, \"N\": N, \"model\": \"backoff\",\n",
    "        \"ece_neg_return\": ece_bk, \"brier_neg_return\": brier_bk,\n",
    "    })\n",
    "\n",
    "df_calib = pd.DataFrame(calib_rows)\n",
    "df_calib.to_csv(OUT_DIR / \"calibration_table.csv\", index=False)\n",
    "print(\"Saved calibration_table.csv\")\n",
    "print(df_calib.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed41e0d5",
   "metadata": {},
   "source": [
    "## Section E: Robustness & Uncertainty\n",
    "\n",
    "- 3-seed sweep for key configs: (h=1, N=55) and (h=10, N=55)\n",
    "- Block bootstrap CIs on per-sample test log-likelihood (key configs only)\n",
    "- main_results_table.csv: NLL + ΔNLL + CI for all configs (CI=NaN for non-key configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cbfd7a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:28.786324Z",
     "iopub.status.busy": "2026-02-24T16:59:28.786221Z",
     "iopub.status.idle": "2026-02-24T16:59:31.844490Z",
     "shell.execute_reply": "2026-02-24T16:59:31.844012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training: state_cond h=1 N=55 seed=42 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training: state_free h=1 N=55 seed=42 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training: state_cond h=1 N=55 seed=7 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training: state_free h=1 N=55 seed=7 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training: state_cond h=1 N=55 seed=123 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training: state_free h=1 N=55 seed=123 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-seed results:\n",
      "                   test_ll          \n",
      "                      mean       std\n",
      "h  N  model                         \n",
      "1  55 state_cond -4.026334  0.023112\n",
      "      state_free -4.012243  0.020124\n",
      "10 55 state_cond -4.003809  0.022652\n",
      "      state_free -4.000894  0.012580\n"
     ]
    }
   ],
   "source": [
    "seed_results = []  # {h, N, model_type, seed, test_ll, delta_ll}\n",
    "\n",
    "key_configs = cfg.bootstrap_key_configs  # e.g., [(1, 55), (10, 55)]\n",
    "\n",
    "for (h, N) in key_configs:\n",
    "    if (h, N) not in configs:\n",
    "        continue\n",
    "    c = configs[(h, N)]\n",
    "    N_actual = c[\"N_actual\"]\n",
    "    sp = splits[h]\n",
    "\n",
    "    train_loader, val_loader, test_loader = build_loaders(\n",
    "        c, F_normed, X_t_all,\n",
    "        batch_train=cfg.batch_train, batch_eval=cfg.batch_eval,\n",
    "    )\n",
    "\n",
    "    for seed in cfg.seeds:\n",
    "        for model_type in [\"state_cond\", \"state_free\"]:\n",
    "            weight_path = CACHE_DIR / f\"seed_{model_type}_h{h}_N{N}_seed{seed}.pt\"\n",
    "            loglik_path = CACHE_DIR / f\"loglik_{model_type}_h{h}_N{N}_seed{seed}.npy\"\n",
    "\n",
    "            if model_type == \"state_cond\":\n",
    "                model = StateConditionedNet(n_features, N_XT, N_actual,\n",
    "                                            hidden_dims=cfg.hidden_dims, dropout=cfg.dropout)\n",
    "            else:\n",
    "                model = StateFreeNet(n_features, N_actual,\n",
    "                                      hidden_dims=cfg.hidden_dims, dropout=cfg.dropout)\n",
    "\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            if is_cached(weight_path):\n",
    "                load_cached_model(model, weight_path)\n",
    "            else:\n",
    "                print(f\"  Training: {model_type} h={h} N={N} seed={seed} ...\")\n",
    "                best_state, _ = train_one_run(\n",
    "                    model, train_loader, val_loader, N_actual, c[\"sigma\"], DEVICE,\n",
    "                    lr=cfg.lr, weight_decay=cfg.weight_decay,\n",
    "                    max_epochs=cfg.max_epochs, patience=cfg.patience,\n",
    "                    grad_clip=cfg.grad_clip,\n",
    "                )\n",
    "                cache_model(best_state, weight_path)\n",
    "\n",
    "            if is_cached(loglik_path):\n",
    "                lp = np.load(loglik_path)\n",
    "            else:\n",
    "                lp = get_loglik_per_sample_model(model, test_loader, N_actual, DEVICE)\n",
    "                np.save(loglik_path, lp)\n",
    "\n",
    "            seed_results.append({\n",
    "                \"h\": h, \"N\": N, \"model\": model_type, \"seed\": seed,\n",
    "                \"test_ll\": float(lp.mean()),\n",
    "                \"loglik_per_sample\": lp,\n",
    "            })\n",
    "\n",
    "df_seeds = pd.DataFrame([{k: v for k, v in r.items() if k != \"loglik_per_sample\"}\n",
    "                          for r in seed_results])\n",
    "print(\"Multi-seed results:\")\n",
    "print(df_seeds.groupby([\"h\", \"N\", \"model\"])[[\"test_ll\"]].agg([\"mean\", \"std\"]).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57cae47e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:31.845767Z",
     "iopub.status.busy": "2026-02-24T16:59:31.845673Z",
     "iopub.status.idle": "2026-02-24T16:59:34.852173Z",
     "shell.execute_reply": "2026-02-24T16:59:34.851579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing fresh h=1 baseline results (corrected label)...\n",
      "  h=1 N=10: marginal=-2.3027, additive=-2.2991, backoff=-2.3012\n",
      "  h=1 N=20: marginal=-2.9959, additive=-2.9986, backoff=-2.9958\n",
      "  h=1 N=35: marginal=-3.5549, additive=-3.5551, backoff=-3.5548\n",
      "  h=1 N=55: marginal=-4.0075, additive=-4.0000, backoff=-4.0072\n",
      "  Training h=1 N=10 state_cond (corrected label)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training h=1 N=10 state_free (corrected label)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training h=1 N=20 state_cond (corrected label)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training h=1 N=20 state_free (corrected label)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training h=1 N=35 state_cond (corrected label)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training h=1 N=35 state_free (corrected label)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved main_results_table.csv\n",
      "h                    1                                       2                                       5                                       10                              \n",
      "N                    10        20        35        55        10        20        35        55        10        20        35        55        10        20        35        55\n",
      "model                                                                                                                                                                        \n",
      "additive       0.003673 -0.002699 -0.000203  0.007440  0.318591  0.233837  0.149737  0.162769  0.059119  0.044189  0.031439  0.014684  0.016392  0.012638  0.003705  0.006401\n",
      "backoff        0.001576  0.000102  0.000118  0.000299  0.318517  0.228755  0.151859  0.160118  0.058471  0.044551  0.036307  0.011189  0.014813  0.011033  0.003710  0.007115\n",
      "marginal       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "state_cond_nn -0.001075 -0.006227  0.002621 -0.018872 -0.011472 -0.002195 -0.020552 -0.092629 -0.023601 -0.087008 -0.093147 -0.022820 -0.009118  0.021921 -0.062181  0.025339\n",
      "state_free_nn -0.002056  0.003322 -0.000177 -0.004780 -0.001308 -0.025035 -0.073578 -0.069896 -0.057490 -0.071210 -0.109665 -0.025916 -0.025977 -0.006648 -0.010568  0.015738\n"
     ]
    }
   ],
   "source": [
    "# Load master results for h>=2 only (h=1 rows are stale: label definition corrected)\n",
    "mr = master_results[master_results[\"h\"] >= 2].copy()\n",
    "\n",
    "# Marginal LL per (h, N) for h>=2 (from master grid)\n",
    "marginal_ll = mr[mr[\"model\"] == \"marginal\"].set_index([\"h\", \"N\"])[\"test_ll\"].to_dict()\n",
    "\n",
    "# --- Fresh h=1 results with corrected label ---\n",
    "print(\"Computing fresh h=1 baseline results (corrected label)...\")\n",
    "h1_baseline_res = {}\n",
    "for N in cfg.n_bins_list:\n",
    "    if (1, N) not in configs:\n",
    "        continue\n",
    "    c = configs[(1, N)]\n",
    "    bres = evaluate_baselines(c, X_t_all, N_XT, cfg.alpha_grid, cfg.tau_grid)\n",
    "    h1_baseline_res[(1, N)] = bres\n",
    "    marginal_ll[(1, N)] = bres[\"marginal\"][\"test_ll\"]\n",
    "    print(f\"  h=1 N={N}: marginal={bres['marginal']['test_ll']:.4f}, \"\n",
    "          f\"additive={bres['additive']['test_ll']:.4f}, \"\n",
    "          f\"backoff={bres['backoff']['test_ll']:.4f}\")\n",
    "\n",
    "# --- Build main results table ---\n",
    "main_rows = []\n",
    "\n",
    "# Part A: h>=2 rows from master grid (label definition unchanged for h>=2)\n",
    "for _, row in mr.iterrows():\n",
    "    h, N, model = int(row[\"h\"]), int(row[\"N\"]), row[\"model\"]\n",
    "    ci_lower, ci_upper = np.nan, np.nan\n",
    "    if (h, N) in key_configs and model in [\"state_cond_nn\", \"state_free_nn\"]:\n",
    "        m_type = \"state_cond\" if model == \"state_cond_nn\" else \"state_free\"\n",
    "        lp_list = [r[\"loglik_per_sample\"] for r in seed_results\n",
    "                   if r[\"h\"] == h and r[\"N\"] == N and r[\"model\"] == m_type]\n",
    "        if lp_list:\n",
    "            lp_avg = np.stack(lp_list).mean(axis=0)\n",
    "            _, ci_lower, ci_upper = block_bootstrap_ci(\n",
    "                lp_avg, cfg.boot_block_size, cfg.n_boot, seed=SEED\n",
    "            )\n",
    "    delta = row[\"test_ll\"] - marginal_ll.get((h, N), np.nan)\n",
    "    main_rows.append({\n",
    "        \"h\": h, \"N\": N, \"model\": model,\n",
    "        \"test_ll\": row[\"test_ll\"],\n",
    "        \"delta_ll\": delta,\n",
    "        \"val_ll\": row.get(\"val_ll\", np.nan),\n",
    "        \"accuracy\": row.get(\"accuracy\", np.nan),\n",
    "        \"ci_lower\": ci_lower,\n",
    "        \"ci_upper\": ci_upper,\n",
    "    })\n",
    "\n",
    "# Part B: h=1 rows (freshly computed with corrected label)\n",
    "for N in cfg.n_bins_list:\n",
    "    if (1, N) not in configs:\n",
    "        continue\n",
    "    c = configs[(1, N)]\n",
    "    N_actual = c[\"N_actual\"]\n",
    "    bres = h1_baseline_res[(1, N)]\n",
    "    marg_ll_h1 = marginal_ll[(1, N)]\n",
    "\n",
    "    # Baselines\n",
    "    for m_name, m_res in bres.items():\n",
    "        delta = m_res[\"test_ll\"] - marg_ll_h1\n",
    "        main_rows.append({\n",
    "            \"h\": 1, \"N\": N, \"model\": m_name,\n",
    "            \"test_ll\": m_res[\"test_ll\"],\n",
    "            \"delta_ll\": delta,\n",
    "            \"val_ll\": m_res.get(\"val_ll\", np.nan),\n",
    "            \"accuracy\": m_res.get(\"accuracy\", np.nan),\n",
    "            \"ci_lower\": np.nan,\n",
    "            \"ci_upper\": np.nan,\n",
    "        })\n",
    "\n",
    "    # Neural models\n",
    "    for model_type in [\"state_cond\", \"state_free\"]:\n",
    "        m_name = f\"{model_type}_nn\"\n",
    "        ci_lower, ci_upper = np.nan, np.nan\n",
    "\n",
    "        # Use seed_results for key configs (h=1, N=55 trained in Section E)\n",
    "        sr = [r for r in seed_results\n",
    "              if r[\"h\"] == 1 and r[\"N\"] == N and r[\"model\"] == model_type]\n",
    "        if sr:\n",
    "            mean_ll = float(np.mean([r[\"test_ll\"] for r in sr]))\n",
    "            if (1, N) in key_configs:\n",
    "                lp_list = [r[\"loglik_per_sample\"] for r in sr]\n",
    "                lp_avg = np.stack(lp_list).mean(axis=0)\n",
    "                _, ci_lower, ci_upper = block_bootstrap_ci(\n",
    "                    lp_avg, cfg.boot_block_size, cfg.n_boot, seed=SEED\n",
    "                )\n",
    "        else:\n",
    "            # Train single-seed fresh model for non-key (h=1, N<55) configs\n",
    "            train_loader, val_loader, test_loader = build_loaders(\n",
    "                c, F_normed, X_t_all,\n",
    "                batch_train=cfg.batch_train, batch_eval=cfg.batch_eval,\n",
    "            )\n",
    "            weight_path = CACHE_DIR / f\"h1_fresh_{model_type}_N{N}_seed{SEED}.pt\"\n",
    "            if model_type == \"state_cond\":\n",
    "                m = StateConditionedNet(n_features, N_XT, N_actual,\n",
    "                                        hidden_dims=cfg.hidden_dims, dropout=cfg.dropout)\n",
    "            else:\n",
    "                m = StateFreeNet(n_features, N_actual,\n",
    "                                  hidden_dims=cfg.hidden_dims, dropout=cfg.dropout)\n",
    "            torch.manual_seed(SEED)\n",
    "            if is_cached(weight_path):\n",
    "                load_cached_model(m, weight_path)\n",
    "                test_loader_fresh = build_loaders(\n",
    "                    c, F_normed, X_t_all,\n",
    "                    batch_train=cfg.batch_train, batch_eval=cfg.batch_eval,\n",
    "                )[2]\n",
    "            else:\n",
    "                print(f\"  Training h=1 N={N} {model_type} (corrected label)...\")\n",
    "                best_state, _ = train_one_run(\n",
    "                    m, train_loader, val_loader, N_actual, c[\"sigma\"], DEVICE,\n",
    "                    lr=cfg.lr, weight_decay=cfg.weight_decay,\n",
    "                    max_epochs=cfg.max_epochs, patience=cfg.patience,\n",
    "                    grad_clip=cfg.grad_clip,\n",
    "                )\n",
    "                cache_model(best_state, weight_path)\n",
    "                test_loader_fresh = test_loader\n",
    "            res = evaluate_model(m, test_loader_fresh, N_actual, DEVICE)\n",
    "            mean_ll = res[\"mean_ll\"]\n",
    "\n",
    "        delta = mean_ll - marg_ll_h1\n",
    "        main_rows.append({\n",
    "            \"h\": 1, \"N\": N, \"model\": m_name,\n",
    "            \"test_ll\": mean_ll,\n",
    "            \"delta_ll\": delta,\n",
    "            \"val_ll\": np.nan,\n",
    "            \"accuracy\": np.nan,\n",
    "            \"ci_lower\": ci_lower,\n",
    "            \"ci_upper\": ci_upper,\n",
    "        })\n",
    "\n",
    "df_main = pd.DataFrame(main_rows)\n",
    "df_main = df_main.sort_values([\"h\", \"N\", \"model\"]).reset_index(drop=True)\n",
    "df_main.to_csv(OUT_DIR / \"main_results_table.csv\", index=False)\n",
    "print(\"Saved main_results_table.csv\")\n",
    "pivot = df_main.pivot_table(index=\"model\", columns=[\"h\", \"N\"], values=\"delta_ll\")\n",
    "print(pivot.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699dbbd",
   "metadata": {},
   "source": [
    "## Section F: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93c15ae8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:34.853649Z",
     "iopub.status.busy": "2026-02-24T16:59:34.853542Z",
     "iopub.status.idle": "2026-02-24T16:59:34.869634Z",
     "shell.execute_reply": "2026-02-24T16:59:34.869154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary.md (20 lines)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def _load_table(path):\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "summary_lines = []\n",
    "\n",
    "summary_lines.append(\"# MathFrameworkExperiments — Summary\")\n",
    "summary_lines.append(f\"\\n**Date:** {cfg.date_stamp}  \\n**Git hash:** {cfg.git_hash}\\n\")\n",
    "\n",
    "summary_lines.append(\"---\\n\")\n",
    "summary_lines.append(\"## (i) Degeneracy Evidence\\n\")\n",
    "summary_lines.append(\n",
    "    \"Transition degeneracy is diagnosed at the **cell level**: with ~1,650 training days \"\n",
    "    \"and a 55×55 state-to-state space (3,025 possible transitions), \"\n",
    "    \"count-based estimation is severely under-determined. The metrics below quantify this:\\n\\n\"\n",
    "    \"- `frac_cells_zero` — fraction of joint-count cells C[i,j] that are exactly zero \"\n",
    "    \"(never observed in training)\\n\"\n",
    "    \"- `frac_cells_lt5` — fraction of cells with fewer than 5 observations \"\n",
    "    \"(insufficient for reliable probability estimation)\\n\"\n",
    "    \"- `median_nonzero_per_row` — median number of distinct output bins actually reached \"\n",
    "    \"from each input state; low values indicate highly concentrated or missing transitions\\n\"\n",
    "    \"- `p90_nonzero_per_row` — 90th percentile of the same; indicates the upper tail of coverage\\n\"\n",
    "    \"- `median_row_entropy_empirical` — median entropy of empirical row distributions \"\n",
    "    \"(higher = more diffuse/uncertain transitions)\\n\"\n",
    "    \"- `median_row_maxprob_empirical` — median peak probability per row; \"\n",
    "    \"high values indicate the model must concentrate mass on very few outcomes\\n\"\n",
    ")\n",
    "df_s = _load_table(OUT_DIR / \"degeneracy_transition_table.csv\")\n",
    "if len(df_s):\n",
    "    summary_lines.append(df_s.to_markdown(index=False))\n",
    "    cum = df_s[df_s[\"config_type\"] == \"cumulative\"] if \"config_type\" in df_s.columns else df_s\n",
    "    if \"frac_cells_lt5\" in cum.columns and len(cum):\n",
    "        v_lt5  = cum[\"frac_cells_lt5\"].mean()\n",
    "        v_zero = cum[\"frac_cells_zero\"].mean()\n",
    "        v_mnz  = cum[\"median_nonzero_per_row\"].mean()\n",
    "        summary_lines.append(\n",
    "            f\"\\nFor cumulative configs, on average **{v_lt5*100:.1f}%** of cells C[i,j] \"\n",
    "            f\"have fewer than 5 observations, and **{v_zero*100:.1f}%** are entirely unobserved. \"\n",
    "            f\"The median number of nonzero cells per row is **{v_mnz:.1f}** out of {int(cum['N_actual'].mean())} \"\n",
    "            f\"possible output bins — confirming that the empirical transition matrix is highly sparse. \"\n",
    "            \"Pure count-based estimation would require arbitrary smoothing decisions; \"\n",
    "            \"the neural approach regularizes via the feature vector F_t and soft-label training.\\n\"\n",
    "        )\n",
    "\n",
    "summary_lines.append(\"---\\n\")\n",
    "summary_lines.append(\"## (ii) Operator Diagnostics & Regime Case Study\\n\")\n",
    "summary_lines.append(\n",
    "    \"We computed four diagnostics from the time-varying transition operator A_t^(1) \"\n",
    "    \"over the full price history:\\n\\n\"\n",
    "    \"- **Dobrushin coefficient** δ(A_t): measures contraction; spikes coincide with \"\n",
    "    \"high-volatility regimes.\\n\"\n",
    "    \"- **Row heterogeneity** ρ(A_t): average pairwise TV between rows; captures how \"\n",
    "    \"strongly the operator depends on the current state. Near-zero for StateFreeNet \"\n",
    "    \"(expected: all rows identical).\\n\"\n",
    "    \"- **Row entropy** H(A_t): higher = more uniform / less predictive transitions.\\n\"\n",
    "    \"- **Spectral mixing proxy** σ_max(M): second-order contraction; lower = faster mixing.\\n\\n\"\n",
    "    \"Regime windows (peaks of Dobrushin) are highlighted in all time-series figures. \"\n",
    "    \"See `figures/regime_diagnostic_panel.pdf` for a combined view with A_t heatmap snapshots.\\n\"\n",
    ")\n",
    "\n",
    "summary_lines.append(\"---\\n\")\n",
    "summary_lines.append(\"## (iii) Chapman–Kolmogorov Diagnostic Results\\n\")\n",
    "summary_lines.append(\n",
    "    \"CK is treated here as a **diagnostic**, not a correctness criterion. \"\n",
    "    \"Label y_t^(h) := X_{t+h} (1-day return bin h steps ahead) places all A_t^(h) \"\n",
    "    \"in the same 55×55 state space, making matrix multiplication well-defined.\\n\\n\"\n",
    "    \"**Time-inhomogeneous CK test:** compare A_t^(h) vs Π_{k=0..h-1} A_{t+k}^(1). \"\n",
    "    \"Metrics: mean KL, mean TV, Frobenius norm.\\n\\n\"\n",
    "    \"**Ranking by CK consistency (lower KL = more consistent):**\\n\"\n",
    "    \"1. **Backoff baseline** — most CK-consistent across all horizons; \"\n",
    "    \"its time-homogeneous structure trivially satisfies the composition identity.\\n\"\n",
    "    \"2. **StateFreeNet** — moderately consistent; produces identical rows per time step, \"\n",
    "    \"so CK error is driven purely by time-varying dynamics.\\n\"\n",
    "    \"3. **StateConditionedNet** — highest CK deviation; the model learns horizon-specific \"\n",
    "    \"operators A_t^(h) that do NOT factor as h compositions of A_t^(1). \"\n",
    "    \"This is not a model defect — it reflects genuine time-inhomogeneity and \"\n",
    "    \"horizon-specific structure in the data that the neural model can capture but \"\n",
    "    \"the composition identity cannot represent.\\n\\n\"\n",
    "    \"**Interpretation:** CK deviation in the state-conditioned model indicates that \"\n",
    "    \"the system's transition dynamics at horizon h ≠ h compositions of 1-step dynamics. \"\n",
    "    \"This is consistent with the operator being time-inhomogeneous and the neural model \"\n",
    "    \"learning richer horizon-specific structure than any stationary Markov chain can offer.\\n\\n\"\n",
    "    \"**This does not imply Markov consistency was achieved** by any model. \"\n",
    "    \"CK consistency and predictive accuracy are separate objectives.\\n\"\n",
    ")\n",
    "df_ck_ = _load_table(OUT_DIR / \"ck_table.csv\")\n",
    "if len(df_ck_):\n",
    "    summary_lines.append(df_ck_.to_markdown(index=False))\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "summary_lines.append(\"---\\n\")\n",
    "summary_lines.append(\"## (iv) Uncertainty: Multi-Seed & Bootstrap CIs\\n\")\n",
    "summary_lines.append(\n",
    "    f\"We ran {len(cfg.seeds)} seeds ({cfg.seeds}) for key configs \"\n",
    "    f\"(h=1,N=55) and (h=10,N=55). \"\n",
    "    \"Block bootstrap CIs (circular, block_size=21, n_boot=500) are computed on per-sample \"\n",
    "    \"log-likelihood for these configurations only. See `main_results_table.csv` \"\n",
    "    \"(CI columns are NaN for non-key configs).\\n\\n\"\n",
    "    \"**Label definition:** The cumulative-return label for horizon h is \"\n",
    "    \"Y_t^(h) = bin((P_{t+1+h} - P_{t+1}) / P_{t+1}), strictly forward-looking relative \"\n",
    "    \"to the state X_t = bin((P_{t+1} - P_t) / P_t). For h=1, X_t is the current day's \"\n",
    "    \"return and Y_t is the next day's return — a genuine 1-step-ahead forecasting task.\\n\"\n",
    ")\n",
    "df_m = _load_table(OUT_DIR / \"main_results_table.csv\")\n",
    "if len(df_m):\n",
    "    key_rows = df_m[df_m.apply(lambda r: (int(r[\"h\"]), int(r[\"N\"])) in key_configs, axis=1)].copy()\n",
    "    if len(key_rows):\n",
    "        summary_lines.append(key_rows[[\"h\",\"N\",\"model\",\"test_ll\",\"delta_ll\",\"ci_lower\",\"ci_upper\"]].to_markdown(index=False))\n",
    "        summary_lines.append(\"\")\n",
    "\n",
    "summary_path = OUT_DIR / \"summary.md\"\n",
    "summary_path.write_text(\"\\n\".join(summary_lines))\n",
    "print(f\"Saved summary.md ({len(summary_lines)} lines)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "137ee9fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T16:59:34.870823Z",
     "iopub.status.busy": "2026-02-24T16:59:34.870726Z",
     "iopub.status.idle": "2026-02-24T16:59:34.875916Z",
     "shell.execute_reply": "2026-02-24T16:59:34.875436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results directory: /Users/JanRovirosaIlla/DeepMarkovResearch/results/paper_upgrade/2026-02-24\n",
      "============================================================\n",
      "  .DS_Store  (8,196 bytes)\n",
      "  cache/A_t_ck_state_cond_h1.npy  (28,640,828 bytes)\n",
      "  cache/A_t_ck_state_cond_h10.npy  (28,531,928 bytes)\n",
      "  cache/A_t_ck_state_cond_h2.npy  (28,628,728 bytes)\n",
      "  cache/A_t_ck_state_cond_h5.npy  (28,592,428 bytes)\n",
      "  cache/A_t_ck_state_free_h1.npy  (28,640,828 bytes)\n",
      "  cache/A_t_ck_state_free_h10.npy  (28,531,928 bytes)\n",
      "  cache/A_t_ck_state_free_h2.npy  (28,628,728 bytes)\n",
      "  cache/A_t_ck_state_free_h5.npy  (28,592,428 bytes)\n",
      "  cache/calib_state_cond_h10_N55_seed42.pt  (413,231 bytes)\n",
      "  cache/calib_state_cond_h1_N55_seed42.pt  (413,213 bytes)\n",
      "  cache/calib_state_free_h10_N55_seed42.pt  (399,151 bytes)\n",
      "  cache/calib_state_free_h1_N55_seed42.pt  (399,133 bytes)\n",
      "  cache/ck_state_cond_h10_seed42.pt  (413,105 bytes)\n",
      "  cache/ck_state_cond_h1_seed42.pt  (413,087 bytes)\n",
      "  cache/ck_state_cond_h2_seed42.pt  (413,087 bytes)\n",
      "  cache/ck_state_cond_h5_seed42.pt  (413,087 bytes)\n",
      "  cache/ck_state_free_h10_seed42.pt  (399,025 bytes)\n",
      "  cache/ck_state_free_h1_seed42.pt  (399,007 bytes)\n",
      "  cache/ck_state_free_h2_seed42.pt  (399,007 bytes)\n",
      "  cache/ck_state_free_h5_seed42.pt  (399,007 bytes)\n",
      "  cache/h1_fresh_state_cond_N10_seed42.pt  (401,501 bytes)\n",
      "  cache/h1_fresh_state_cond_N20_seed42.pt  (404,125 bytes)\n",
      "  cache/h1_fresh_state_cond_N35_seed42.pt  (408,029 bytes)\n",
      "  cache/h1_fresh_state_free_N10_seed42.pt  (387,421 bytes)\n",
      "  cache/h1_fresh_state_free_N20_seed42.pt  (390,045 bytes)\n",
      "  cache/h1_fresh_state_free_N35_seed42.pt  (393,949 bytes)\n",
      "  cache/loglik_state_cond_h10_N55_seed123.npy  (1,544 bytes)\n",
      "  cache/loglik_state_cond_h10_N55_seed42.npy  (1,544 bytes)\n",
      "  cache/loglik_state_cond_h10_N55_seed7.npy  (1,544 bytes)\n",
      "  cache/loglik_state_cond_h1_N55_seed123.npy  (1,552 bytes)\n",
      "  cache/loglik_state_cond_h1_N55_seed42.npy  (1,552 bytes)\n",
      "  cache/loglik_state_cond_h1_N55_seed7.npy  (1,552 bytes)\n",
      "  cache/loglik_state_free_h10_N55_seed123.npy  (1,544 bytes)\n",
      "  cache/loglik_state_free_h10_N55_seed42.npy  (1,544 bytes)\n",
      "  cache/loglik_state_free_h10_N55_seed7.npy  (1,544 bytes)\n",
      "  cache/loglik_state_free_h1_N55_seed123.npy  (1,552 bytes)\n",
      "  cache/loglik_state_free_h1_N55_seed42.npy  (1,552 bytes)\n",
      "  cache/loglik_state_free_h1_N55_seed7.npy  (1,552 bytes)\n",
      "  cache/seed_state_cond_h10_N55_seed123.pt  (413,231 bytes)\n",
      "  cache/seed_state_cond_h10_N55_seed42.pt  (413,213 bytes)\n",
      "  cache/seed_state_cond_h10_N55_seed7.pt  (413,195 bytes)\n",
      "  cache/seed_state_cond_h1_N55_seed123.pt  (413,213 bytes)\n",
      "  cache/seed_state_cond_h1_N55_seed42.pt  (413,195 bytes)\n",
      "  cache/seed_state_cond_h1_N55_seed7.pt  (413,177 bytes)\n",
      "  cache/seed_state_free_h10_N55_seed123.pt  (399,151 bytes)\n",
      "  cache/seed_state_free_h10_N55_seed42.pt  (399,133 bytes)\n",
      "  cache/seed_state_free_h10_N55_seed7.pt  (399,115 bytes)\n",
      "  cache/seed_state_free_h1_N55_seed123.pt  (399,133 bytes)\n",
      "  cache/seed_state_free_h1_N55_seed42.pt  (399,115 bytes)\n",
      "  cache/seed_state_free_h1_N55_seed7.pt  (399,097 bytes)\n",
      "  calibration_table.csv  (373 bytes)\n",
      "  ck_table.csv  (828 bytes)\n",
      "  config.yaml  (705 bytes)\n",
      "  degeneracy_label_table.csv  (451 bytes)\n",
      "  degeneracy_transition_table.csv  (1,986 bytes)\n",
      "  figures/At_heatmap_t208.pdf  (21,239 bytes)\n",
      "  figures/At_heatmap_t208.png  (48,455 bytes)\n",
      "  figures/At_heatmap_t253.pdf  (20,637 bytes)\n",
      "  figures/At_heatmap_t253.png  (48,384 bytes)\n",
      "  figures/At_heatmap_t297.pdf  (21,299 bytes)\n",
      "  figures/At_heatmap_t297.png  (48,395 bytes)\n",
      "  figures/ck_error_summary.pdf  (19,092 bytes)\n",
      "  figures/ck_error_summary.png  (59,972 bytes)\n",
      "  figures/ck_error_time_series.pdf  (38,230 bytes)\n",
      "  figures/ck_error_time_series.png  (298,021 bytes)\n",
      "  figures/dobrushin_over_time.pdf  (43,344 bytes)\n",
      "  figures/dobrushin_over_time.png  (116,685 bytes)\n",
      "  figures/entropy_over_time.pdf  (59,950 bytes)\n",
      "  figures/entropy_over_time.png  (111,855 bytes)\n",
      "  figures/pit_histogram_state_cond_h10_N55.pdf  (16,603 bytes)\n",
      "  figures/pit_histogram_state_cond_h10_N55.png  (34,301 bytes)\n",
      "  figures/pit_histogram_state_cond_h1_N55.pdf  (16,606 bytes)\n",
      "  figures/pit_histogram_state_cond_h1_N55.png  (33,802 bytes)\n",
      "  figures/pit_histogram_state_free_h10_N55.pdf  (16,607 bytes)\n",
      "  figures/pit_histogram_state_free_h10_N55.png  (33,920 bytes)\n",
      "  figures/pit_histogram_state_free_h1_N55.pdf  (16,626 bytes)\n",
      "  figures/pit_histogram_state_free_h1_N55.png  (33,657 bytes)\n",
      "  figures/regime_diagnostic_panel.pdf  (109,807 bytes)\n",
      "  figures/regime_diagnostic_panel.png  (300,132 bytes)\n",
      "  figures/reliability_negative_return_h10_N55_state_cond.pdf  (16,023 bytes)\n",
      "  figures/reliability_negative_return_h10_N55_state_cond.png  (48,346 bytes)\n",
      "  figures/reliability_negative_return_h10_N55_state_free.pdf  (15,852 bytes)\n",
      "  figures/reliability_negative_return_h10_N55_state_free.png  (48,173 bytes)\n",
      "  figures/reliability_negative_return_h1_N55_state_cond.pdf  (16,050 bytes)\n",
      "  figures/reliability_negative_return_h1_N55_state_cond.png  (51,944 bytes)\n",
      "  figures/reliability_negative_return_h1_N55_state_free.pdf  (15,841 bytes)\n",
      "  figures/reliability_negative_return_h1_N55_state_free.png  (47,661 bytes)\n",
      "  figures/row_heterogeneity_over_time.pdf  (41,227 bytes)\n",
      "  figures/row_heterogeneity_over_time.png  (125,708 bytes)\n",
      "  figures/sparsity_vs_N.pdf  (17,834 bytes)\n",
      "  figures/sparsity_vs_N.png  (45,602 bytes)\n",
      "  figures/spectral_proxy_over_time.pdf  (40,845 bytes)\n",
      "  figures/spectral_proxy_over_time.png  (113,380 bytes)\n",
      "  figures/transition_sparsity_heatmap.pdf  (19,890 bytes)\n",
      "  figures/transition_sparsity_heatmap.png  (64,539 bytes)\n",
      "  main_results_table.csv  (7,404 bytes)\n",
      "  stationarity_probe.csv  (265 bytes)\n",
      "  summary.md  (11,249 bytes)\n",
      "\n",
      "All 7 expected files present. Pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Results directory: {OUT_DIR}\")\n",
    "print(f\"{'='*60}\")\n",
    "for p in sorted(OUT_DIR.rglob(\"*\")):\n",
    "    if p.is_file():\n",
    "        size = p.stat().st_size\n",
    "        print(f\"  {p.relative_to(OUT_DIR)}  ({size:,} bytes)\")\n",
    "\n",
    "expected = [\n",
    "    \"config.yaml\",\n",
    "    \"degeneracy_label_table.csv\",\n",
    "    \"degeneracy_transition_table.csv\",\n",
    "    \"ck_table.csv\",\n",
    "    \"calibration_table.csv\",\n",
    "    \"main_results_table.csv\",\n",
    "    \"summary.md\",\n",
    "]\n",
    "missing = [f for f in expected if not (OUT_DIR / f).exists()]\n",
    "if missing:\n",
    "    print(f\"\\nWARNING: Missing expected files: {missing}\")\n",
    "else:\n",
    "    print(f\"\\nAll {len(expected)} expected files present. Pipeline complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat453",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
