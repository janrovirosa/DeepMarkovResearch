{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-0",
   "metadata": {},
   "source": [
    "# Multi-Step Evaluation Framework\n",
    "\n",
    "Evaluates time-varying Markov transition model on **k-step distributional predictions**.\n",
    "\n",
    "**Key idea:** The model produces time-varying transition matrices $A_t$ of shape $(n, n)$. For k-step prediction:\n",
    "\n",
    "$$\\pi_{t+k} = \\pi_t \\cdot A_t \\cdot A_{t+1} \\cdots A_{t+k-1}$$\n",
    "\n",
    "We compare these predicted distributions against empirical observations using KL, JSD, TV, and severity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "K_VALUES = [1, 2, 3, 5, 10]         # Prediction horizons to evaluate\n",
    "EXPERIMENT = \"hard_labels\"           # Change to compare different experiments\n",
    "BOOTSTRAP_N = 1000                   # Bootstrap resamples for CIs\n",
    "EPS = 1e-10                          # Smoothing for KL divergence\n",
    "\n",
    "print(f\"Evaluating experiment: {EXPERIMENT}\")\n",
    "print(f\"K-step horizons: {K_VALUES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-1",
   "metadata": {},
   "source": [
    "## 1. Load Data and Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD DATASET\n",
    "# ============================================\n",
    "train_df  = pd.read_csv(\"dataset/train_diagnostic.csv\")\n",
    "labels_df = pd.read_csv(\"dataset/label_diagnostic.csv\")\n",
    "\n",
    "# Drop last row (forward return undefined) and Opinion column\n",
    "train_df[\"Percent_change_forward\"] = (\n",
    "    train_df[\"Price\"].shift(-1) / train_df[\"Price\"] - 1\n",
    ") * 100.0\n",
    "train_df = train_df.iloc[:-1].copy()\n",
    "labels_df = labels_df.iloc[:-1].copy()\n",
    "train_df = train_df.drop(columns=[\"Opinion\"], errors=\"ignore\")\n",
    "\n",
    "# States: 0-based\n",
    "s_curr_all = (train_df[\"Backward_Bin\"].values.astype(np.int64) - 1)\n",
    "y_all      = (labels_df[\"Forward_Bin\"].values.astype(np.int64) - 1)\n",
    "\n",
    "n_samples = len(s_curr_all)\n",
    "n_states = int(max(s_curr_all.max(), y_all.max()) + 1)\n",
    "\n",
    "# Temporal split: 70 / 15 / 15 (must match training notebook)\n",
    "train_end = int(0.7 * n_samples)\n",
    "val_end   = int(0.85 * n_samples)\n",
    "\n",
    "idx_train = np.arange(0, train_end)\n",
    "idx_val   = np.arange(train_end, val_end)\n",
    "idx_test  = np.arange(val_end, n_samples)\n",
    "\n",
    "s_train = s_curr_all[idx_train]\n",
    "s_test  = s_curr_all[idx_test]\n",
    "y_train = y_all[idx_train]\n",
    "y_test  = y_all[idx_test]\n",
    "\n",
    "print(f\"n_samples: {n_samples}, n_states: {n_states}\")\n",
    "print(f\"Train: {len(idx_train)}, Val: {len(idx_val)}, Test: {len(idx_test)}\")\n",
    "print(f\"Test indices: [{idx_test[0]}, {idx_test[-1]}]\")\n",
    "\n",
    "# ============================================\n",
    "# LOAD TRANSITION MATRICES\n",
    "# ============================================\n",
    "results_dir = Path(\"results\")\n",
    "\n",
    "# Load primary experiment\n",
    "A_all = torch.load(results_dir / EXPERIMENT / \"models\" / \"A_all_model.pt\",\n",
    "                   map_location=\"cpu\", weights_only=True)\n",
    "A_all_np = A_all.numpy()\n",
    "\n",
    "print(f\"\\nA_all shape: {A_all.shape}  (expected: ({n_samples}, {n_states}, {n_states}))\")\n",
    "print(f\"Row sums check (sample): {A_all_np[0].sum(axis=1)[:5]}  (should be ~1.0)\")\n",
    "\n",
    "# Load all available experiments for comparison\n",
    "all_experiments = {}\n",
    "for exp_path in results_dir.iterdir():\n",
    "    a_file = exp_path / \"models\" / \"A_all_model.pt\"\n",
    "    if a_file.exists():\n",
    "        all_experiments[exp_path.name] = torch.load(\n",
    "            a_file, map_location=\"cpu\", weights_only=True\n",
    "        ).numpy()\n",
    "        print(f\"  Loaded: {exp_path.name} -> shape {all_experiments[exp_path.name].shape}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(all_experiments)} experiments total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-2",
   "metadata": {},
   "source": [
    "## 2. k-Step Distribution Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_k_step_distribution(A_all, k_start, s_0, k, n_states):\n",
    "    \"\"\"\n",
    "    Propagate distribution k steps forward using time-varying matrices.\n",
    "    \n",
    "    pi_{t+k} = pi_t * A_t * A_{t+1} * ... * A_{t+k-1}\n",
    "    \n",
    "    Args:\n",
    "        A_all: (T, n, n) array of transition matrices\n",
    "        k_start: starting time index\n",
    "        s_0: initial state (integer)\n",
    "        k: number of steps to propagate\n",
    "        n_states: number of states\n",
    "    \n",
    "    Returns:\n",
    "        pi: (n_states,) probability distribution over states at t+k\n",
    "    \"\"\"\n",
    "    pi = np.zeros(n_states, dtype=np.float64)\n",
    "    pi[s_0] = 1.0\n",
    "    \n",
    "    for step in range(k):\n",
    "        t = k_start + step\n",
    "        pi = pi @ A_all[t]  # (n,) @ (n, n) -> (n,)\n",
    "    \n",
    "    return pi\n",
    "\n",
    "\n",
    "def compute_k_step_matrix(A_all, k_start, k):\n",
    "    \"\"\"\n",
    "    Compute the k-step transition matrix: A_t * A_{t+1} * ... * A_{t+k-1}\n",
    "    \n",
    "    Returns: (n_states, n_states) matrix\n",
    "    \"\"\"\n",
    "    M = A_all[k_start].copy().astype(np.float64)\n",
    "    for step in range(1, k):\n",
    "        t = k_start + step\n",
    "        M = M @ A_all[t]\n",
    "    return M\n",
    "\n",
    "\n",
    "# Sanity check: k=1 should give same row as A_all[t]\n",
    "t_check = idx_test[0]\n",
    "s_check = s_test[0]\n",
    "pi_1 = predict_k_step_distribution(A_all_np, t_check, s_check, k=1, n_states=n_states)\n",
    "direct = A_all_np[t_check, s_check, :]\n",
    "print(f\"k=1 sanity check: max diff = {np.abs(pi_1 - direct).max():.2e}\")\n",
    "print(f\"Row sum: {pi_1.sum():.6f}\")\n",
    "\n",
    "# k=2 check: matrix product rows sum to 1\n",
    "M2 = compute_k_step_matrix(A_all_np, t_check, k=2)\n",
    "print(f\"k=2 matrix row sums: min={M2.sum(axis=1).min():.6f}, max={M2.sum(axis=1).max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-3",
   "metadata": {},
   "source": [
    "## 3. Empirical k-Step Transition Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_k_step(s_curr_all, y_all, idx_set, k, n_states):\n",
    "    \"\"\"\n",
    "    Compute empirical k-step transition counts from data.\n",
    "    \n",
    "    For each sample i in idx_set, the current state is s_curr_all[i]\n",
    "    and the state k steps later is y_all[i + k - 1] (since y_all[i] is already 1 step ahead).\n",
    "    \n",
    "    For k=1: state at i -> y_all[i] (next state, which is already stored)\n",
    "    For k=2: state at i -> y_all[i+1] (state 2 steps later)\n",
    "    For k=j: state at i -> y_all[i+j-1]\n",
    "    \n",
    "    But we need to be careful: y_all[i] = state at time i+1.\n",
    "    So state k steps after time i = s_curr_all[i + k] if it exists,\n",
    "    but equivalently = y_all[i + k - 1].\n",
    "    \n",
    "    We use s_curr_all for the \"arrival\" state to be consistent.\n",
    "    \n",
    "    Returns:\n",
    "        P_emp: (n_states, n_states) empirical transition matrix\n",
    "        counts: (n_states,) number of samples per starting state\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((n_states, n_states), dtype=np.float64)\n",
    "    state_counts = np.zeros(n_states, dtype=np.int64)\n",
    "    \n",
    "    for i in range(len(idx_set)):\n",
    "        t = idx_set[i]\n",
    "        s_start = s_curr_all[t]\n",
    "        \n",
    "        # The state k steps later\n",
    "        # y_all[t] is the state at t+1, so state at t+k = y_all[t+k-1]\n",
    "        target_idx = t + k - 1\n",
    "        if target_idx >= len(y_all):\n",
    "            continue\n",
    "        \n",
    "        # But we also need target_idx to be in test range for fair comparison\n",
    "        # Actually, y_all[target_idx] gives us the forward bin at time target_idx,\n",
    "        # which is the state at time target_idx + 1 = t + k.\n",
    "        # For k=1: y_all[t] = state at t+1. Correct.\n",
    "        # For k=2: y_all[t+1] = state at t+2. Correct.\n",
    "        s_end = y_all[target_idx]\n",
    "        \n",
    "        transition_counts[s_start, s_end] += 1\n",
    "        state_counts[s_start] += 1\n",
    "    \n",
    "    # Normalize rows\n",
    "    P_emp = np.zeros_like(transition_counts)\n",
    "    for s in range(n_states):\n",
    "        if state_counts[s] > 0:\n",
    "            P_emp[s] = transition_counts[s] / state_counts[s]\n",
    "    \n",
    "    return P_emp, state_counts\n",
    "\n",
    "\n",
    "# Compute empirical matrices for all k values\n",
    "empirical_matrices = {}\n",
    "empirical_counts = {}\n",
    "\n",
    "for k in K_VALUES:\n",
    "    P_emp, counts = compute_empirical_k_step(s_curr_all, y_all, idx_test, k, n_states)\n",
    "    empirical_matrices[k] = P_emp\n",
    "    empirical_counts[k] = counts\n",
    "    \n",
    "    valid_states = (counts >= 5).sum()\n",
    "    total_samples = counts.sum()\n",
    "    print(f\"k={k:2d}: {total_samples} valid transitions, \"\n",
    "          f\"{valid_states}/{n_states} states with >=5 samples, \"\n",
    "          f\"sparse states: {(counts > 0).sum() - valid_states}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-4",
   "metadata": {},
   "source": [
    "## 4. Distributional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q, eps=EPS):\n",
    "    \"\"\"KL(p || q) with smoothing.\"\"\"\n",
    "    p = np.asarray(p, dtype=np.float64) + eps\n",
    "    q = np.asarray(q, dtype=np.float64) + eps\n",
    "    p = p / p.sum()\n",
    "    q = q / q.sum()\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "\n",
    "def jensen_shannon_divergence(p, q, eps=EPS):\n",
    "    \"\"\"JSD(p, q) = 0.5 * KL(p||m) + 0.5 * KL(q||m), m = (p+q)/2.\"\"\"\n",
    "    p = np.asarray(p, dtype=np.float64) + eps\n",
    "    q = np.asarray(q, dtype=np.float64) + eps\n",
    "    p = p / p.sum()\n",
    "    q = q / q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * np.sum(p * np.log(p / m)) + 0.5 * np.sum(q * np.log(q / m))\n",
    "\n",
    "\n",
    "def total_variation(p, q):\n",
    "    \"\"\"TV(p, q) = 0.5 * ||p - q||_1.\"\"\"\n",
    "    p = np.asarray(p, dtype=np.float64)\n",
    "    q = np.asarray(q, dtype=np.float64)\n",
    "    p_sum = p.sum()\n",
    "    q_sum = q.sum()\n",
    "    if p_sum > 0:\n",
    "        p = p / p_sum\n",
    "    if q_sum > 0:\n",
    "        q = q / q_sum\n",
    "    return 0.5 * np.abs(p - q).sum()\n",
    "\n",
    "\n",
    "def mean_bin_distance(p_model, q_empirical, n_states):\n",
    "    \"\"\"\n",
    "    Expected absolute bin error under the empirical distribution,\n",
    "    using the model's expected bin as the prediction.\n",
    "    \n",
    "    severity = sum_j |E_model[bin] - j| * q(j)\n",
    "    \"\"\"\n",
    "    bins = np.arange(n_states, dtype=np.float64)\n",
    "    p = np.asarray(p_model, dtype=np.float64)\n",
    "    q = np.asarray(q_empirical, dtype=np.float64)\n",
    "    \n",
    "    p_sum = p.sum()\n",
    "    q_sum = q.sum()\n",
    "    if p_sum > 0:\n",
    "        p = p / p_sum\n",
    "    if q_sum > 0:\n",
    "        q = q / q_sum\n",
    "    \n",
    "    expected_bin_model = (p * bins).sum()\n",
    "    severity = np.sum(np.abs(expected_bin_model - bins) * q)\n",
    "    return severity\n",
    "\n",
    "\n",
    "def compute_all_metrics(p_model, q_empirical, n_states):\n",
    "    \"\"\"Compute all four metrics between model and empirical distributions.\"\"\"\n",
    "    return {\n",
    "        \"KL\": kl_divergence(q_empirical, p_model),  # KL(empirical || model)\n",
    "        \"JSD\": jensen_shannon_divergence(p_model, q_empirical),\n",
    "        \"TV\": total_variation(p_model, q_empirical),\n",
    "        \"Severity\": mean_bin_distance(p_model, q_empirical, n_states),\n",
    "    }\n",
    "\n",
    "\n",
    "# Quick test\n",
    "p_test = np.zeros(n_states); p_test[27] = 1.0\n",
    "q_test = np.zeros(n_states); q_test[27] = 0.5; q_test[28] = 0.5\n",
    "metrics_test = compute_all_metrics(p_test, q_test, n_states)\n",
    "print(\"Metric test (delta_27 vs 0.5*delta_27 + 0.5*delta_28):\")\n",
    "for name, val in metrics_test.items():\n",
    "    print(f\"  {name}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-5",
   "metadata": {},
   "source": [
    "## 5. Evaluate Multi-Step Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multistep(A_all_np, s_curr_all, y_all, idx_test, k_values, n_states,\n",
    "                       empirical_matrices, empirical_counts, label=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate k-step predictions for a given set of transition matrices.\n",
    "    \n",
    "    Returns:\n",
    "        results: dict of k -> {metric_name -> {per_state: array, weighted_avg: float, mean: float, std: float}}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        P_emp = empirical_matrices[k]\n",
    "        counts = empirical_counts[k]\n",
    "        \n",
    "        # Compute model k-step matrix averaged over test times\n",
    "        # For each test starting point, compute the k-step prediction\n",
    "        # Then average the predicted distributions per starting state\n",
    "        model_k_step = np.zeros((n_states, n_states), dtype=np.float64)\n",
    "        model_counts = np.zeros(n_states, dtype=np.int64)\n",
    "        \n",
    "        for i in range(len(idx_test)):\n",
    "            t = idx_test[i]\n",
    "            s_0 = s_curr_all[t]\n",
    "            \n",
    "            # Check we have enough future matrices\n",
    "            if t + k > len(A_all_np):\n",
    "                continue\n",
    "            # Check the target exists in y_all\n",
    "            if t + k - 1 >= len(y_all):\n",
    "                continue\n",
    "            \n",
    "            pi_k = predict_k_step_distribution(A_all_np, t, s_0, k, n_states)\n",
    "            model_k_step[s_0] += pi_k\n",
    "            model_counts[s_0] += 1\n",
    "        \n",
    "        # Normalize to get average predicted distribution per state\n",
    "        for s in range(n_states):\n",
    "            if model_counts[s] > 0:\n",
    "                model_k_step[s] /= model_counts[s]\n",
    "        \n",
    "        # Compute per-state metrics (only for states with sufficient data)\n",
    "        metric_names = [\"KL\", \"JSD\", \"TV\", \"Severity\"]\n",
    "        per_state = {m: np.full(n_states, np.nan) for m in metric_names}\n",
    "        valid_mask = np.zeros(n_states, dtype=bool)\n",
    "        \n",
    "        for s in range(n_states):\n",
    "            if counts[s] >= 5 and model_counts[s] > 0:\n",
    "                valid_mask[s] = True\n",
    "                m = compute_all_metrics(model_k_step[s], P_emp[s], n_states)\n",
    "                for name in metric_names:\n",
    "                    per_state[name][s] = m[name]\n",
    "        \n",
    "        # Aggregate\n",
    "        total_weight = counts[valid_mask].sum()\n",
    "        weights = counts[valid_mask].astype(np.float64) / total_weight if total_weight > 0 else None\n",
    "        \n",
    "        k_results = {}\n",
    "        for name in metric_names:\n",
    "            vals = per_state[name][valid_mask]\n",
    "            k_results[name] = {\n",
    "                \"per_state\": per_state[name],\n",
    "                \"weighted_avg\": float(np.average(vals, weights=weights)) if weights is not None and len(vals) > 0 else np.nan,\n",
    "                \"mean\": float(np.nanmean(vals)) if len(vals) > 0 else np.nan,\n",
    "                \"std\": float(np.nanstd(vals)) if len(vals) > 0 else np.nan,\n",
    "            }\n",
    "        \n",
    "        k_results[\"n_valid_states\"] = int(valid_mask.sum())\n",
    "        k_results[\"model_matrix\"] = model_k_step\n",
    "        results[k] = k_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Evaluate the primary experiment\n",
    "results_primary = evaluate_multistep(\n",
    "    A_all_np, s_curr_all, y_all, idx_test, K_VALUES, n_states,\n",
    "    empirical_matrices, empirical_counts, label=EXPERIMENT\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MULTI-STEP EVALUATION: {EXPERIMENT}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'k':>3} | {'KL':>8} | {'JSD':>8} | {'TV':>8} | {'Severity':>10} | {'Valid States':>12}\")\n",
    "print(f\"{'-'*3}-+-{'-'*8}-+-{'-'*8}-+-{'-'*8}-+-{'-'*10}-+-{'-'*12}\")\n",
    "for k in K_VALUES:\n",
    "    r = results_primary[k]\n",
    "    print(f\"{k:3d} | {r['KL']['weighted_avg']:8.4f} | {r['JSD']['weighted_avg']:8.4f} | \"\n",
    "          f\"{r['TV']['weighted_avg']:8.4f} | {r['Severity']['weighted_avg']:10.4f} | {r['n_valid_states']:12d}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-6",
   "metadata": {},
   "source": [
    "## 6. Baseline Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BASELINE 1: Stationary Empirical Matrix^k\n",
    "# ============================================\n",
    "# Compute empirical 1-step transition matrix from TRAINING data only\n",
    "A_emp_train = np.zeros((n_states, n_states), dtype=np.float64)\n",
    "for t in idx_train:\n",
    "    s_from = s_curr_all[t]\n",
    "    s_to = y_all[t]\n",
    "    A_emp_train[s_from, s_to] += 1\n",
    "\n",
    "# Normalize rows (with smoothing for empty rows)\n",
    "row_sums = A_emp_train.sum(axis=1, keepdims=True)\n",
    "A_emp_train = np.where(row_sums > 0, A_emp_train / row_sums, 1.0 / n_states)\n",
    "\n",
    "# For k-step: raise to power k\n",
    "def stationary_k_step(A_emp, k, n_samples_needed):\n",
    "    \"\"\"Create A_all-shaped array where every time step uses A_emp^k.\"\"\"\n",
    "    A_k = np.linalg.matrix_power(A_emp, 1)  # Start with A_emp\n",
    "    # We'll build the product directly per-step to match the evaluate_multistep interface\n",
    "    # Just repeat the same matrix for all time steps\n",
    "    return np.tile(A_emp[np.newaxis, :, :], (n_samples_needed, 1, 1))\n",
    "\n",
    "A_stationary = stationary_k_step(A_emp_train, 1, n_samples)\n",
    "\n",
    "results_stationary = evaluate_multistep(\n",
    "    A_stationary, s_curr_all, y_all, idx_test, K_VALUES, n_states,\n",
    "    empirical_matrices, empirical_counts, label=\"Stationary\"\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# BASELINE 2: Marginal (ignore current state)\n",
    "# ============================================\n",
    "# Marginal distribution of states in training data\n",
    "marginal_dist = np.bincount(y_train, minlength=n_states).astype(np.float64)\n",
    "marginal_dist = marginal_dist / marginal_dist.sum()\n",
    "\n",
    "# Create A_all where every row is the marginal distribution (state-independent)\n",
    "A_marginal = np.tile(marginal_dist[np.newaxis, np.newaxis, :], (n_samples, n_states, 1))\n",
    "\n",
    "results_marginal = evaluate_multistep(\n",
    "    A_marginal, s_curr_all, y_all, idx_test, K_VALUES, n_states,\n",
    "    empirical_matrices, empirical_counts, label=\"Marginal\"\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# EVALUATE ALL SAVED EXPERIMENTS\n",
    "# ============================================\n",
    "results_all_experiments = {}\n",
    "for exp_name, A_exp in all_experiments.items():\n",
    "    results_all_experiments[exp_name] = evaluate_multistep(\n",
    "        A_exp, s_curr_all, y_all, idx_test, K_VALUES, n_states,\n",
    "        empirical_matrices, empirical_counts, label=exp_name\n",
    "    )\n",
    "\n",
    "# Add baselines\n",
    "results_all_experiments[\"Stationary (train)\"] = results_stationary\n",
    "results_all_experiments[\"Marginal\"] = results_marginal\n",
    "\n",
    "# ============================================\n",
    "# COMPARISON TABLE\n",
    "# ============================================\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"MULTI-STEP COMPARISON: ALL MODELS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "for metric_name in [\"JSD\", \"TV\", \"Severity\"]:\n",
    "    print(f\"\\n--- {metric_name} (weighted avg) ---\")\n",
    "    header = f\"{'Model':<30}\"\n",
    "    for k in K_VALUES:\n",
    "        header += f\" | k={k:>2}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    for model_name, model_results in results_all_experiments.items():\n",
    "        row = f\"{model_name:<30}\"\n",
    "        for k in K_VALUES:\n",
    "            val = model_results[k][metric_name][\"weighted_avg\"]\n",
    "            row += f\" | {val:5.4f}\"\n",
    "        print(row)\n",
    "\n",
    "print(f\"\\n{'='*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-7",
   "metadata": {},
   "source": [
    "## 7. Chapman-Kolmogorov Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chapman_kolmogorov_test(A_all_np, s_curr_all, y_all, idx_test, n_states, k_values=[2, 3, 5]):\n",
    "    \"\"\"\n",
    "    Chapman-Kolmogorov consistency test.\n",
    "    \n",
    "    For each test time t and each k:\n",
    "    - Model k-step matrix: A_t * A_{t+1} * ... * A_{t+k-1}\n",
    "    - Empirical k-step: direct observed transitions starting at t\n",
    "    \n",
    "    Since we can't get a full empirical matrix at a single time t,\n",
    "    we instead compare the model's k-step prediction for the actual\n",
    "    starting state against the observed outcome.\n",
    "    \n",
    "    We measure:\n",
    "    1. Frobenius norm between model 2-step matrix and product of model 1-step matrices (self-consistency)\n",
    "    2. Log-likelihood of observed k-step transitions under model predictions\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        log_likelihoods = []\n",
    "        predicted_probs = []\n",
    "        true_states = []\n",
    "        \n",
    "        for i in range(len(idx_test)):\n",
    "            t = idx_test[i]\n",
    "            s_0 = s_curr_all[t]\n",
    "            \n",
    "            # Check boundaries\n",
    "            if t + k > len(A_all_np) or t + k - 1 >= len(y_all):\n",
    "                continue\n",
    "            \n",
    "            # Model's k-step prediction\n",
    "            pi_k = predict_k_step_distribution(A_all_np, t, s_0, k, n_states)\n",
    "            \n",
    "            # True state k steps later\n",
    "            s_true = y_all[t + k - 1]\n",
    "            \n",
    "            # Log-likelihood of the true state under model prediction\n",
    "            ll = np.log(pi_k[s_true] + EPS)\n",
    "            log_likelihoods.append(ll)\n",
    "            predicted_probs.append(pi_k)\n",
    "            true_states.append(s_true)\n",
    "        \n",
    "        log_likelihoods = np.array(log_likelihoods)\n",
    "        true_states = np.array(true_states)\n",
    "        predicted_probs = np.array(predicted_probs)\n",
    "        \n",
    "        # Accuracy: most likely state == true state\n",
    "        pred_argmax = predicted_probs.argmax(axis=1)\n",
    "        accuracy = (pred_argmax == true_states).mean()\n",
    "        \n",
    "        # Expected bin error (severity)\n",
    "        bins = np.arange(n_states, dtype=np.float64)\n",
    "        expected_bins = (predicted_probs * bins[np.newaxis, :]).sum(axis=1)\n",
    "        severity = np.abs(expected_bins - true_states.astype(np.float64)).mean()\n",
    "        \n",
    "        results[k] = {\n",
    "            \"mean_ll\": float(log_likelihoods.mean()),\n",
    "            \"std_ll\": float(log_likelihoods.std()),\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"severity\": float(severity),\n",
    "            \"n_samples\": len(log_likelihoods),\n",
    "            \"log_likelihoods\": log_likelihoods,\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Self-consistency: compare model's multi-step product vs direct product\n",
    "# (These should be identical by construction, but this verifies numerical stability)\n",
    "print(\"=\" * 60)\n",
    "print(\"CHAPMAN-KOLMOGOROV CONSISTENCY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Frobenius norm between A_t*A_{t+1} (2-step product) computed two ways\n",
    "frobenius_deviations = {k: [] for k in [2, 3, 5]}\n",
    "\n",
    "for i in range(0, min(len(idx_test) - 10, 200)):\n",
    "    t = idx_test[i]\n",
    "    for k in [2, 3, 5]:\n",
    "        if t + k > len(A_all_np):\n",
    "            continue\n",
    "        \n",
    "        # Method 1: Sequential matrix product\n",
    "        M_seq = compute_k_step_matrix(A_all_np, t, k)\n",
    "        \n",
    "        # Method 2: For CK test, compare k-step model matrix against\n",
    "        # empirical k-step transitions from test data around time t\n",
    "        # Since empirical at single time t isn't available, we compare\n",
    "        # the row for the actual starting state\n",
    "        s_0 = s_curr_all[t]\n",
    "        pi_model = M_seq[s_0]\n",
    "        \n",
    "        # Row sum deviation from 1\n",
    "        row_sum_dev = np.abs(M_seq.sum(axis=1) - 1.0).max()\n",
    "        frobenius_deviations[k].append(row_sum_dev)\n",
    "\n",
    "print(\"\\nNumerical stability (max row-sum deviation from 1.0):\")\n",
    "for k in [2, 3, 5]:\n",
    "    devs = np.array(frobenius_deviations[k])\n",
    "    print(f\"  k={k}: mean={devs.mean():.2e}, max={devs.max():.2e}\")\n",
    "\n",
    "# Per-sample CK test: log-likelihood of true k-step outcomes\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Per-sample k-step prediction quality:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "ck_results_all = {}\n",
    "for exp_name, A_exp in all_experiments.items():\n",
    "    if exp_name in [\"Stationary (train)\", \"Marginal\"]:\n",
    "        continue\n",
    "    ck_results_all[exp_name] = chapman_kolmogorov_test(\n",
    "        A_exp, s_curr_all, y_all, idx_test, n_states, k_values=[1, 2, 3, 5, 10]\n",
    "    )\n",
    "\n",
    "# Add baselines\n",
    "ck_results_all[\"Stationary (train)\"] = chapman_kolmogorov_test(\n",
    "    A_stationary, s_curr_all, y_all, idx_test, n_states, k_values=[1, 2, 3, 5, 10]\n",
    ")\n",
    "ck_results_all[\"Marginal\"] = chapman_kolmogorov_test(\n",
    "    A_marginal, s_curr_all, y_all, idx_test, n_states, k_values=[1, 2, 3, 5, 10]\n",
    ")\n",
    "\n",
    "print(f\"\\n{'Model':<30} | {'k':>3} | {'Mean LL':>8} | {'Accuracy':>8} | {'Severity':>8} | {'N':>5}\")\n",
    "print(\"-\" * 80)\n",
    "for exp_name, ck_res in ck_results_all.items():\n",
    "    for k in [1, 2, 3, 5, 10]:\n",
    "        if k in ck_res:\n",
    "            r = ck_res[k]\n",
    "            print(f\"{exp_name:<30} | {k:3d} | {r['mean_ll']:8.4f} | {r['accuracy']:8.4f} | \"\n",
    "                  f\"{r['severity']:8.2f} | {r['n_samples']:5d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-8",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PLOT 1: Divergence vs k (all models)\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax_idx, metric_name in enumerate([\"JSD\", \"TV\", \"Severity\"]):\n",
    "    ax = axes[ax_idx]\n",
    "    \n",
    "    for model_name, model_results in results_all_experiments.items():\n",
    "        means = [model_results[k][metric_name][\"weighted_avg\"] for k in K_VALUES]\n",
    "        stds = [model_results[k][metric_name][\"std\"] for k in K_VALUES]\n",
    "        \n",
    "        style = \"--\" if model_name in [\"Stationary (train)\", \"Marginal\"] else \"-\"\n",
    "        alpha = 0.6 if model_name in [\"Stationary (train)\", \"Marginal\"] else 0.9\n",
    "        \n",
    "        label = model_name.replace(\"_\", \" \").title()\n",
    "        ax.plot(K_VALUES, means, style, marker=\"o\", label=label, alpha=alpha, linewidth=2)\n",
    "        ax.fill_between(K_VALUES,\n",
    "                       [m - s for m, s in zip(means, stds)],\n",
    "                       [m + s for m, s in zip(means, stds)],\n",
    "                       alpha=0.15)\n",
    "    \n",
    "    ax.set_xlabel(\"Prediction Horizon k\", fontsize=12)\n",
    "    ax.set_ylabel(metric_name, fontsize=12)\n",
    "    ax.set_title(f\"{metric_name} vs Prediction Horizon\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.legend(fontsize=8, loc=\"best\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xticks(K_VALUES)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"results/multistep_divergence_vs_k.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# PLOT 2: Heatmaps - Model vs Empirical k-step matrices\n",
    "# ============================================\n",
    "k_plot = [1, 3, 5, 10]\n",
    "fig, axes = plt.subplots(2, len(k_plot), figsize=(5 * len(k_plot), 10))\n",
    "\n",
    "for col, k in enumerate(k_plot):\n",
    "    if k not in results_primary:\n",
    "        continue\n",
    "    \n",
    "    model_mat = results_primary[k][\"model_matrix\"]\n",
    "    emp_mat = empirical_matrices[k]\n",
    "    \n",
    "    vmax = max(model_mat.max(), emp_mat.max())\n",
    "    \n",
    "    sns.heatmap(model_mat, ax=axes[0, col], cmap=\"viridis\", vmin=0, vmax=vmax, cbar=True)\n",
    "    axes[0, col].set_title(f\"Model k={k}\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[0, col].set_xlabel(\"Next State\")\n",
    "    if col == 0:\n",
    "        axes[0, col].set_ylabel(\"Current State\")\n",
    "    \n",
    "    sns.heatmap(emp_mat, ax=axes[1, col], cmap=\"viridis\", vmin=0, vmax=vmax, cbar=True)\n",
    "    axes[1, col].set_title(f\"Empirical k={k}\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[1, col].set_xlabel(\"Next State\")\n",
    "    if col == 0:\n",
    "        axes[1, col].set_ylabel(\"Current State\")\n",
    "\n",
    "plt.suptitle(f\"Model vs Empirical k-Step Transition Matrices ({EXPERIMENT})\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"results/multistep_heatmaps.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# PLOT 3: Per-sample log-likelihood vs k\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Severity vs k\n",
    "for exp_name, ck_res in ck_results_all.items():\n",
    "    ks = sorted(ck_res.keys())\n",
    "    sevs = [ck_res[k][\"severity\"] for k in ks]\n",
    "    style = \"--\" if exp_name in [\"Stationary (train)\", \"Marginal\"] else \"-\"\n",
    "    label = exp_name.replace(\"_\", \" \").title()\n",
    "    axes[0].plot(ks, sevs, style, marker=\"o\", label=label, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel(\"Prediction Horizon k\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Mean Severity (bins)\", fontsize=12)\n",
    "axes[0].set_title(\"Per-Sample Severity vs Horizon\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Mean log-likelihood vs k\n",
    "for exp_name, ck_res in ck_results_all.items():\n",
    "    ks = sorted(ck_res.keys())\n",
    "    lls = [ck_res[k][\"mean_ll\"] for k in ks]\n",
    "    style = \"--\" if exp_name in [\"Stationary (train)\", \"Marginal\"] else \"-\"\n",
    "    label = exp_name.replace(\"_\", \" \").title()\n",
    "    axes[1].plot(ks, lls, style, marker=\"o\", label=label, linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel(\"Prediction Horizon k\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Mean Log-Likelihood\", fontsize=12)\n",
    "axes[1].set_title(\"Per-Sample Log-Likelihood vs Horizon\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"results/multistep_persample_metrics.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# PLOT 4: Per-state JSD for the primary experiment\n",
    "# ============================================\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "for k in K_VALUES:\n",
    "    jsd_per_state = results_primary[k][\"JSD\"][\"per_state\"]\n",
    "    valid = ~np.isnan(jsd_per_state)\n",
    "    ax.plot(np.where(valid)[0], jsd_per_state[valid], \"o-\", label=f\"k={k}\", alpha=0.7, markersize=4)\n",
    "\n",
    "ax.set_xlabel(\"State\", fontsize=12)\n",
    "ax.set_ylabel(\"Jensen-Shannon Divergence\", fontsize=12)\n",
    "ax.set_title(f\"Per-State JSD by Prediction Horizon ({EXPERIMENT})\", fontsize=13, fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"results/multistep_perstate_jsd.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-9",
   "metadata": {},
   "source": [
    "## 9. Statistical Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, statistic=np.mean, n_boot=BOOTSTRAP_N, ci=0.95):\n",
    "    \"\"\"Compute bootstrap confidence interval.\"\"\"\n",
    "    boot_stats = []\n",
    "    for _ in range(n_boot):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        boot_stats.append(statistic(sample))\n",
    "    boot_stats = np.sort(boot_stats)\n",
    "    lower = boot_stats[int((1 - ci) / 2 * n_boot)]\n",
    "    upper = boot_stats[int((1 + ci) / 2 * n_boot)]\n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"STATISTICAL VALIDATION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# For each k, compare models using per-sample log-likelihoods\n",
    "for k in K_VALUES:\n",
    "    print(f\"\\n--- k = {k} ---\")\n",
    "    print(f\"{'Model':<30} | {'Mean LL':>8} | {'95% CI':>18} | {'Severity':>8} | {'95% CI':>18}\")\n",
    "    print(\"-\" * 95)\n",
    "    \n",
    "    for exp_name, ck_res in ck_results_all.items():\n",
    "        if k not in ck_res:\n",
    "            continue\n",
    "        r = ck_res[k]\n",
    "        ll_data = r[\"log_likelihoods\"]\n",
    "        \n",
    "        ll_ci = bootstrap_ci(ll_data)\n",
    "        \n",
    "        # Also bootstrap severity\n",
    "        # Need to recompute per-sample severity\n",
    "        # We already have mean severity; for bootstrap we need individual values\n",
    "        # Use mean LL CI as primary\n",
    "        print(f\"{exp_name:<30} | {r['mean_ll']:8.4f} | [{ll_ci[0]:8.4f}, {ll_ci[1]:8.4f}] | \"\n",
    "              f\"{r['severity']:8.2f} |                   \")\n",
    "\n",
    "# ============================================\n",
    "# PAIRED COMPARISON: Model vs Baselines\n",
    "# ============================================\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"PAIRED WILCOXON TESTS (vs Stationary Baseline)\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "baseline_name = \"Stationary (train)\"\n",
    "if baseline_name in ck_results_all:\n",
    "    for k in K_VALUES:\n",
    "        if k not in ck_results_all[baseline_name]:\n",
    "            continue\n",
    "        \n",
    "        baseline_ll = ck_results_all[baseline_name][k][\"log_likelihoods\"]\n",
    "        \n",
    "        print(f\"\\nk={k}:\")\n",
    "        for exp_name, ck_res in ck_results_all.items():\n",
    "            if exp_name == baseline_name or k not in ck_res:\n",
    "                continue\n",
    "            \n",
    "            model_ll = ck_res[k][\"log_likelihoods\"]\n",
    "            \n",
    "            # Ensure same length\n",
    "            n_min = min(len(model_ll), len(baseline_ll))\n",
    "            if n_min < 10:\n",
    "                print(f\"  {exp_name}: insufficient samples ({n_min})\")\n",
    "                continue\n",
    "            \n",
    "            diff = model_ll[:n_min] - baseline_ll[:n_min]\n",
    "            \n",
    "            try:\n",
    "                stat, p_value = stats.wilcoxon(diff)\n",
    "                mean_diff = diff.mean()\n",
    "                direction = \"better\" if mean_diff > 0 else \"worse\"\n",
    "                sig = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"n.s.\"\n",
    "                print(f\"  {exp_name:<30} diff={mean_diff:+.4f} ({direction}) p={p_value:.4f} {sig}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {exp_name:<30} test failed: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*90}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-10",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SAVE ALL RESULTS\n",
    "# ============================================\n",
    "import os\n",
    "\n",
    "# Prepare serializable results\n",
    "def make_serializable(results):\n",
    "    \"\"\"Convert numpy arrays to lists for JSON serialization.\"\"\"\n",
    "    out = {}\n",
    "    for k, v in results.items():\n",
    "        if isinstance(v, dict):\n",
    "            out[str(k)] = {}\n",
    "            for kk, vv in v.items():\n",
    "                if kk == \"model_matrix\":\n",
    "                    continue  # Skip large matrices\n",
    "                if isinstance(vv, dict):\n",
    "                    out[str(k)][kk] = {}\n",
    "                    for kkk, vvv in vv.items():\n",
    "                        if isinstance(vvv, np.ndarray):\n",
    "                            out[str(k)][kk][kkk] = vvv.tolist()\n",
    "                        elif isinstance(vvv, (np.floating, np.integer)):\n",
    "                            out[str(k)][kk][kkk] = float(vvv)\n",
    "                        else:\n",
    "                            out[str(k)][kk][kkk] = vvv\n",
    "                elif isinstance(vv, (np.floating, np.integer)):\n",
    "                    out[str(k)][kk] = float(vv)\n",
    "                elif isinstance(vv, np.ndarray):\n",
    "                    out[str(k)][kk] = vv.tolist()\n",
    "                else:\n",
    "                    out[str(k)][kk] = vv\n",
    "        else:\n",
    "            out[str(k)] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "# Save per-experiment results\n",
    "for exp_name in all_experiments:\n",
    "    if exp_name in [\"Stationary (train)\", \"Marginal\"]:\n",
    "        continue\n",
    "    \n",
    "    exp_dir = results_dir / exp_name / \"metrics\"\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    save_data = {\n",
    "        \"k_values\": K_VALUES,\n",
    "        \"distributional_metrics\": make_serializable(results_all_experiments[exp_name]),\n",
    "    }\n",
    "    \n",
    "    if exp_name in ck_results_all:\n",
    "        ck_save = {}\n",
    "        for k, v in ck_results_all[exp_name].items():\n",
    "            ck_save[str(k)] = {\n",
    "                \"mean_ll\": v[\"mean_ll\"],\n",
    "                \"std_ll\": v[\"std_ll\"],\n",
    "                \"accuracy\": v[\"accuracy\"],\n",
    "                \"severity\": v[\"severity\"],\n",
    "                \"n_samples\": v[\"n_samples\"],\n",
    "            }\n",
    "        save_data[\"chapman_kolmogorov\"] = ck_save\n",
    "    \n",
    "    with open(exp_dir / \"multistep_evaluation.json\", \"w\") as f:\n",
    "        json.dump(save_data, f, indent=2)\n",
    "    print(f\"Saved: {exp_dir / 'multistep_evaluation.json'}\")\n",
    "\n",
    "# Save comparison summary\n",
    "summary_rows = []\n",
    "for exp_name, ck_res in ck_results_all.items():\n",
    "    for k in sorted(ck_res.keys()):\n",
    "        r = ck_res[k]\n",
    "        summary_rows.append({\n",
    "            \"Model\": exp_name,\n",
    "            \"k\": k,\n",
    "            \"Mean_LL\": r[\"mean_ll\"],\n",
    "            \"Accuracy\": r[\"accuracy\"],\n",
    "            \"Severity\": r[\"severity\"],\n",
    "            \"N_Samples\": r[\"n_samples\"],\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(\"results/multistep_summary.csv\", index=False)\n",
    "print(f\"\\nSaved: results/multistep_summary.csv\")\n",
    "print(f\"\\nAll figures saved to results/multistep_*.png\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
