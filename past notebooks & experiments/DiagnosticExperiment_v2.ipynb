{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# Go/No-Go Diagnostic v2 — Rigorous Signal Detection\n",
    "\n",
    "**Purpose:** Determine with defensible statistics whether there is *any* conditional signal\n",
    "in the current-state $X_t$ (Backward_Bin) about the next-state $Y_t$ (Forward_Bin)\n",
    "beyond the marginal baseline.\n",
    "\n",
    "**Improvements over v1:**\n",
    "1. Explicit alignment verification\n",
    "2. Proper MI permutation test (500 permutations, p-value, z-score)\n",
    "3. Fixed rebinning bug (backward returns use their own quantile edges)\n",
    "4. Tuned additive-smoothing conditional baseline (alpha sweep on VAL)\n",
    "5. Interpolated backoff baseline: $P_{mix}(y|x) = \\lambda_x \\cdot P_{cond}(y|x) + (1-\\lambda_x) \\cdot P_{marg}(y)$\n",
    "6. Multi-step evaluation ($k \\in \\{1,2,3,5,10\\}$) via $A^k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os, warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "EPS = 1e-12  # only for log(p + EPS) in evaluation, NOT for MI\n",
    "\n",
    "print(\"Imports ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-data",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Identical to `TransitionProbMatrix_NEWDATA.ipynb`. Same dataset, same temporal split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples=2368, n_states_orig=55\n",
      "Train: 1657, Val: 355, Test: 356\n"
     ]
    }
   ],
   "source": [
    "train_df  = pd.read_csv(\"dataset/train_diagnostic.csv\")\n",
    "labels_df = pd.read_csv(\"dataset/label_diagnostic.csv\")\n",
    "\n",
    "# Compute forward percent change from Price\n",
    "train_df[\"Percent_change_forward\"] = (\n",
    "    train_df[\"Price\"].shift(-1) / train_df[\"Price\"] - 1\n",
    ") * 100.0\n",
    "\n",
    "# Drop last row (forward return undefined)\n",
    "train_df = train_df.iloc[:-1].copy()\n",
    "labels_df = labels_df.iloc[:-1].copy()\n",
    "\n",
    "# States: 0-based\n",
    "s_curr_all = (train_df[\"Backward_Bin\"].values.astype(np.int64) - 1)\n",
    "y_all      = (labels_df[\"Forward_Bin\"].values.astype(np.int64) - 1)\n",
    "\n",
    "# Raw percent changes (for rebinning)\n",
    "pct_backward_all = train_df[\"Percent_change_backward\"].values.astype(np.float64)\n",
    "pct_forward_all  = train_df[\"Percent_change_forward\"].values.astype(np.float64)\n",
    "\n",
    "n_samples = len(s_curr_all)\n",
    "n_states_orig = int(max(s_curr_all.max(), y_all.max()) + 1)\n",
    "\n",
    "# Temporal split: 70 / 15 / 15\n",
    "T = n_samples\n",
    "train_end = int(0.7 * T)\n",
    "val_end   = int(0.85 * T)\n",
    "\n",
    "idx_train = np.arange(0,         train_end)\n",
    "idx_val   = np.arange(train_end, val_end)\n",
    "idx_test  = np.arange(val_end,   T)\n",
    "\n",
    "s_train, s_val, s_test = s_curr_all[idx_train], s_curr_all[idx_val], s_curr_all[idx_test]\n",
    "y_train, y_val, y_test = y_all[idx_train], y_all[idx_val], y_all[idx_test]\n",
    "\n",
    "print(f\"n_samples={n_samples}, n_states_orig={n_states_orig}\")\n",
    "print(f\"Train: {len(idx_train)}, Val: {len(idx_val)}, Test: {len(idx_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-alignment",
   "metadata": {},
   "source": [
    "## (1) Alignment Verification\n",
    "Confirm that `Backward_Bin[t]` = bin of return from $P_{t-1} \\to P_t$,\n",
    "and `Forward_Bin[t]` = bin of return from $P_t \\to P_{t+1}$.\n",
    "\n",
    "Also verify: `Forward_Bin[t] == Backward_Bin[t+1]` (the forward return at $t$ IS the backward return at $t+1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-alignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ALIGNMENT VERIFICATION (rows 10-14)\n",
      "====================================================================================================\n",
      "  t |       P_t |   bwd_pct | bwd_bin | check |   fwd_pct | fwd_bin | check\n",
      "---------------------------------------------------------------------------\n",
      " 10 |     57.04 |   -0.0701 |   27= 27 |    OK |    0.4208 |   32= 32 |    OK\n",
      " 11 |     57.28 |    0.4208 |   32= 32 |    OK |    3.8757 |   47= 47 |    OK\n",
      " 12 |     59.50 |    3.8757 |   47= 47 |    OK |   -1.0756 |   17= 17 |    OK\n",
      " 13 |     58.86 |   -1.0756 |   17= 17 |    OK |   -3.1091 |   10= 10 |    OK\n",
      " 14 |     57.03 |   -3.1091 |   10= 10 |    OK |    0.6663 |   35= 35 |    OK\n",
      "\n",
      "Cross-check: Forward_Bin[t] == Backward_Bin[t+1]?\n",
      "  2367/2367 match (100.0%)\n",
      "  PERFECT ALIGNMENT CONFIRMED.\n"
     ]
    }
   ],
   "source": [
    "# Python reimplementation of the R bin_return function\n",
    "def bin_return_py(x):\n",
    "    \"\"\"Replicate the R bin_return function exactly. Returns 1-based bin.\"\"\"\n",
    "    thresholds = [\n",
    "        (-np.inf, -10, 1), (-10, -9, 2), (-9, -8, 3), (-8, -7, 4),\n",
    "        (-7, -6, 5), (-6, -5, 6), (-5, -4.5, 7), (-4.5, -4, 8),\n",
    "        (-4, -3.5, 9), (-3.5, -3, 10), (-3, -2.5, 11), (-2.5, -2.05, 12),\n",
    "        (-2.05, -1.85, 13), (-1.85, -1.65, 14), (-1.65, -1.45, 15),\n",
    "        (-1.45, -1.25, 16), (-1.25, -1.05, 17), (-1.05, -0.95, 18),\n",
    "        (-0.95, -0.85, 19), (-0.85, -0.75, 20), (-0.75, -0.65, 21),\n",
    "        (-0.65, -0.55, 22), (-0.55, -0.45, 23), (-0.45, -0.35, 24),\n",
    "        (-0.35, -0.25, 25), (-0.25, -0.15, 26), (-0.15, -0.05, 27),\n",
    "        (-0.05, 0.05, 28), (0.05, 0.15, 29), (0.15, 0.25, 30),\n",
    "        (0.25, 0.35, 31), (0.35, 0.45, 32), (0.45, 0.55, 33),\n",
    "        (0.55, 0.65, 34), (0.65, 0.75, 35), (0.75, 0.85, 36),\n",
    "        (0.85, 0.95, 37), (0.95, 1.05, 38), (1.05, 1.25, 39),\n",
    "        (1.25, 1.45, 40), (1.45, 1.65, 41), (1.65, 1.85, 42),\n",
    "        (1.85, 2.05, 43), (2.05, 2.55, 44), (2.55, 3.05, 45),\n",
    "        (3.05, 3.55, 46), (3.55, 4.05, 47), (4.05, 4.55, 48),\n",
    "        (4.55, 5, 49), (5, 6, 50), (6, 7, 51), (7, 8, 52),\n",
    "        (8, 9, 53), (9, 10, 54),\n",
    "    ]\n",
    "    for lo, hi, b in thresholds:\n",
    "        if x >= lo and x < hi:\n",
    "            return b\n",
    "    if x > 10:\n",
    "        return 55\n",
    "    return None\n",
    "\n",
    "# Show 5 consecutive rows (indices 10-14)\n",
    "print(\"=\" * 100)\n",
    "print(\"ALIGNMENT VERIFICATION (rows 10-14)\")\n",
    "print(\"=\" * 100)\n",
    "prices = train_df[\"Price\"].values\n",
    "bwd_pct = train_df[\"Percent_change_backward\"].values\n",
    "fwd_pct = train_df[\"Percent_change_forward\"].values\n",
    "bwd_bin_csv = train_df[\"Backward_Bin\"].values  # 1-based from CSV\n",
    "fwd_bin_csv = labels_df[\"Forward_Bin\"].values   # 1-based from CSV\n",
    "\n",
    "header = f\"{'t':>3} | {'P_t':>9} | {'bwd_pct':>9} | {'bwd_bin':>7} | {'check':>5} | {'fwd_pct':>9} | {'fwd_bin':>7} | {'check':>5}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for t in range(10, 15):\n",
    "    # Manually compute returns\n",
    "    bwd_manual = (prices[t] - prices[t-1]) / prices[t-1] * 100 if t > 0 else float('nan')\n",
    "    fwd_manual = (prices[t+1] - prices[t]) / prices[t] * 100 if t < len(prices)-1 else float('nan')\n",
    "    bwd_bin_check = bin_return_py(bwd_pct[t])\n",
    "    fwd_bin_check = bin_return_py(fwd_pct[t])\n",
    "    print(f\"{t:3d} | {prices[t]:9.2f} | {bwd_pct[t]:9.4f} | {bwd_bin_csv[t]:4d}={bwd_bin_check:3d} | {'OK' if bwd_bin_csv[t]==bwd_bin_check else 'FAIL':>5} | \"\n",
    "          f\"{fwd_pct[t]:9.4f} | {fwd_bin_csv[t]:4d}={fwd_bin_check:3d} | {'OK' if fwd_bin_csv[t]==fwd_bin_check else 'FAIL':>5}\")\n",
    "\n",
    "# Key relationship: Forward_Bin[t] should equal Backward_Bin[t+1]\n",
    "print(\"\\nCross-check: Forward_Bin[t] == Backward_Bin[t+1]?\")\n",
    "matches = 0\n",
    "total = 0\n",
    "for t in range(len(fwd_bin_csv) - 1):\n",
    "    if fwd_bin_csv[t] == bwd_bin_csv[t+1]:\n",
    "        matches += 1\n",
    "    total += 1\n",
    "print(f\"  {matches}/{total} match ({matches/total*100:.1f}%)\")\n",
    "if matches == total:\n",
    "    print(\"  PERFECT ALIGNMENT CONFIRMED.\")\n",
    "else:\n",
    "    print(f\"  *** MISMATCH in {total-matches} rows! ***\")\n",
    "    # Show first mismatch\n",
    "    for t in range(len(fwd_bin_csv) - 1):\n",
    "        if fwd_bin_csv[t] != bwd_bin_csv[t+1]:\n",
    "            print(f\"    First mismatch at t={t}: fwd_bin[t]={fwd_bin_csv[t]}, bwd_bin[t+1]={bwd_bin_csv[t+1]}\")\n",
    "            print(f\"    fwd_pct[t]={fwd_pct[t]:.6f}, bwd_pct[t+1]={bwd_pct[t+1]:.6f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-helpers",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-helpers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers defined.\n"
     ]
    }
   ],
   "source": [
    "def compute_marginal(y, n_classes):\n",
    "    \"\"\"Compute marginal distribution from integer labels.\"\"\"\n",
    "    counts = np.bincount(y, minlength=n_classes).astype(np.float64)\n",
    "    return counts / counts.sum()\n",
    "\n",
    "\n",
    "def compute_joint_counts(s, y, n_x, n_y):\n",
    "    \"\"\"Compute raw joint count matrix C[x, y].\"\"\"\n",
    "    C = np.zeros((n_x, n_y), dtype=np.float64)\n",
    "    for si, yi in zip(s, y):\n",
    "        C[si, yi] += 1\n",
    "    return C\n",
    "\n",
    "\n",
    "def compute_mi_plugin(s, y, n_x, n_y):\n",
    "    \"\"\"Plugin MI estimator from raw counts. No epsilon smoothing.\n",
    "    MI = sum_{x,y} P_hat(x,y) * log(P_hat(x,y) / (P_hat(x)*P_hat(y)))\n",
    "    Skips cells where P_hat(x,y) = 0.\n",
    "    \"\"\"\n",
    "    C = compute_joint_counts(s, y, n_x, n_y)\n",
    "    N = C.sum()\n",
    "    P_joint = C / N\n",
    "    P_x = P_joint.sum(axis=1)\n",
    "    P_y = P_joint.sum(axis=0)\n",
    "    mi = 0.0\n",
    "    for i in range(n_x):\n",
    "        if P_x[i] == 0:\n",
    "            continue\n",
    "        for j in range(n_y):\n",
    "            if P_joint[i, j] > 0 and P_y[j] > 0:\n",
    "                mi += P_joint[i, j] * np.log(P_joint[i, j] / (P_x[i] * P_y[j]))\n",
    "    return mi\n",
    "\n",
    "\n",
    "def mi_permutation_test(s, y, n_x, n_y, n_perm=500):\n",
    "    \"\"\"MI permutation test. Returns dict with mi_real, perm stats, p-value, z-score.\"\"\"\n",
    "    mi_real = compute_mi_plugin(s, y, n_x, n_y)\n",
    "    mi_perm = np.zeros(n_perm)\n",
    "    for i in range(n_perm):\n",
    "        y_shuf = np.random.permutation(y)\n",
    "        mi_perm[i] = compute_mi_plugin(s, y_shuf, n_x, n_y)\n",
    "    p_value = (1 + np.sum(mi_perm >= mi_real)) / (n_perm + 1)\n",
    "    perm_mean = mi_perm.mean()\n",
    "    perm_std = mi_perm.std()\n",
    "    z_score = (mi_real - perm_mean) / perm_std if perm_std > 0 else 0.0\n",
    "    delta_mi = mi_real - perm_mean\n",
    "    return {\n",
    "        \"mi_real\": mi_real, \"perm_mean\": perm_mean, \"perm_std\": perm_std,\n",
    "        \"z_score\": z_score, \"p_value\": p_value,\n",
    "        \"delta_mi_nats\": delta_mi, \"delta_mi_bits\": delta_mi / np.log(2),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_conditional_additive(s, y, n_x, n_y, alpha):\n",
    "    \"\"\"P(y|x) = (C[x,y] + alpha) / (C[x,.] + alpha * n_y).\"\"\"\n",
    "    C = compute_joint_counts(s, y, n_x, n_y)\n",
    "    C_alpha = C + alpha\n",
    "    P = C_alpha / C_alpha.sum(axis=1, keepdims=True)\n",
    "    return P\n",
    "\n",
    "\n",
    "def compute_backoff_baseline(s_train, y_train, s_eval, n_x, n_y, alpha, tau, marginal):\n",
    "    \"\"\"Interpolated backoff: P_mix(y|x) = lambda_x * P_cond(y|x) + (1-lambda_x) * P_marg(y)\n",
    "    where lambda_x = count(x) / (count(x) + tau).\n",
    "    Returns: (N_eval, n_y) array of predicted distributions.\n",
    "    \"\"\"\n",
    "    C = compute_joint_counts(s_train, y_train, n_x, n_y)\n",
    "    C_alpha = C + alpha\n",
    "    P_cond = C_alpha / C_alpha.sum(axis=1, keepdims=True)\n",
    "    state_counts = C.sum(axis=1)  # count(x)\n",
    "    lam = state_counts / (state_counts + tau)  # (n_x,)\n",
    "    # For each eval sample: mix conditional and marginal\n",
    "    P_mix = np.zeros((len(s_eval), n_y), dtype=np.float64)\n",
    "    for i, sx in enumerate(s_eval):\n",
    "        P_mix[i] = lam[sx] * P_cond[sx] + (1 - lam[sx]) * marginal\n",
    "    return P_mix\n",
    "\n",
    "\n",
    "def mean_log_likelihood(pred_dist, y_true):\n",
    "    \"\"\"Mean log-likelihood: mean(log(P[y_true])).\"\"\"\n",
    "    N = len(y_true)\n",
    "    probs = pred_dist[np.arange(N), y_true]\n",
    "    return np.log(probs + EPS).mean()\n",
    "\n",
    "\n",
    "def accuracy(pred_dist, y_true):\n",
    "    return (pred_dist.argmax(axis=1) == y_true).mean()\n",
    "\n",
    "\n",
    "def severity(pred_dist, y_true, n_classes):\n",
    "    bins = np.arange(n_classes, dtype=np.float64)\n",
    "    expected = (pred_dist * bins[np.newaxis, :]).sum(axis=1)\n",
    "    return np.abs(expected - y_true.astype(np.float64)).mean()\n",
    "\n",
    "\n",
    "print(\"Helpers defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-rebin",
   "metadata": {},
   "source": [
    "## Rebinning (Fixed)\n",
    "**Bug fix from v1:** backward returns now use their own quantile edges\n",
    "(fit on train backward returns), not forward return edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-rebin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebinning functions defined.\n"
     ]
    }
   ],
   "source": [
    "def rebin_quantile(pct_values, pct_train_for_edges, n_bins):\n",
    "    \"\"\"Rebin using quantile edges fit on the given training data.\"\"\"\n",
    "    edges = np.quantile(pct_train_for_edges, np.linspace(0, 1, n_bins + 1))\n",
    "    edges[0] = -np.inf\n",
    "    edges[-1] = np.inf\n",
    "    edges = np.unique(edges)\n",
    "    actual_n = len(edges) - 1\n",
    "    bins = np.clip(np.digitize(pct_values, edges) - 1, 0, actual_n - 1)\n",
    "    return bins, edges, actual_n\n",
    "\n",
    "\n",
    "def prepare_bins(n_bins, pct_fwd_all, pct_bwd_all, pct_fwd_train, pct_bwd_train,\n",
    "                 s_orig, y_orig):\n",
    "    \"\"\"Prepare rebinned states and labels for a given n_bins.\n",
    "    For n_bins=55: use original CSV bins.\n",
    "    Otherwise: quantile-based, with SEPARATE edges for forward and backward.\n",
    "    \"\"\"\n",
    "    if n_bins == 55:\n",
    "        return s_orig.copy(), y_orig.copy(), 55, \"original_fixed\"\n",
    "    else:\n",
    "        y_new, _, n_y = rebin_quantile(pct_fwd_all, pct_fwd_train, n_bins)\n",
    "        s_new, _, n_s = rebin_quantile(pct_bwd_all, pct_bwd_train, n_bins)  # FIXED: own edges\n",
    "        actual_n = max(n_y, n_s)\n",
    "        return s_new, y_new, actual_n, \"quantile\"\n",
    "\n",
    "\n",
    "# Precompute train-only percent changes\n",
    "pct_fwd_train = pct_forward_all[idx_train]\n",
    "pct_bwd_train = pct_backward_all[idx_train]  # FIXED: was missing in v1\n",
    "\n",
    "print(\"Rebinning functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-mi",
   "metadata": {},
   "source": [
    "## (2) MI Permutation Test — All Bin Counts\n",
    "500 permutations. Reports MI_real, permutation mean/std, z-score, p-value, deltaMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-mi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_bins=25 (quantile): running 500 permutations... done.\n",
      "  MI_real=0.253800, perm_mean=0.192654, perm_std=0.010699\n",
      "  deltaMI=0.061146 nats (0.088214 bits)\n",
      "  z-score=5.72, p-value=0.0020\n",
      "  -> Statistically significant signal (p<0.01, deltaMI>0.001)\n",
      "\n",
      "n_bins=35 (quantile): running 500 permutations... done.\n",
      "  MI_real=0.468648, perm_mean=0.410466, perm_std=0.013763\n",
      "  deltaMI=0.058182 nats (0.083940 bits)\n",
      "  z-score=4.23, p-value=0.0020\n",
      "  -> Statistically significant signal (p<0.01, deltaMI>0.001)\n",
      "\n",
      "n_bins=40 (quantile): running 500 permutations... done.\n",
      "  MI_real=0.577981, perm_mean=0.534409, perm_std=0.013769\n",
      "  deltaMI=0.043572 nats (0.062861 bits)\n",
      "  z-score=3.16, p-value=0.0020\n",
      "  -> Statistically significant signal (p<0.01, deltaMI>0.001)\n",
      "\n",
      "n_bins=55 (original_fixed): running 500 permutations... done.\n",
      "  MI_real=0.659427, perm_mean=0.628043, perm_std=0.012749\n",
      "  deltaMI=0.031384 nats (0.045278 bits)\n",
      "  z-score=2.46, p-value=0.0100\n",
      "  -> Statistically significant signal (p<0.01, deltaMI>0.001)\n",
      "\n",
      "====================================================================================================\n",
      "MI PERMUTATION TEST SUMMARY\n",
      "====================================================================================================\n",
      " n_bins         method  mi_real  perm_mean  perm_std  delta_mi_nats  delta_mi_bits  z_score  p_value  avg_samples_per_state  states_lt5\n",
      "     25       quantile 0.253800   0.192654  0.010699       0.061146       0.088214 5.715092 0.001996              66.280000           0\n",
      "     35       quantile 0.468648   0.410466  0.013763       0.058182       0.083940 4.227378 0.001996              47.342857           0\n",
      "     40       quantile 0.577981   0.534409  0.013769       0.043572       0.062861 3.164496 0.001996              41.425000           0\n",
      "     55 original_fixed 0.659427   0.628043  0.012749       0.031384       0.045278 2.461734 0.009980              31.264151          11\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "N_PERM = 500\n",
    "N_BINS_LIST = [25, 35, 40, 55]\n",
    "\n",
    "mi_results = []\n",
    "\n",
    "for n_bins in N_BINS_LIST:\n",
    "    s_all, y_all_rb, n_st, method = prepare_bins(\n",
    "        n_bins, pct_forward_all, pct_backward_all,\n",
    "        pct_fwd_train, pct_bwd_train, s_curr_all, y_all)\n",
    "    s_tr = s_all[idx_train]\n",
    "    y_tr = y_all_rb[idx_train]\n",
    "\n",
    "    print(f\"\\nn_bins={n_bins} ({method}): running {N_PERM} permutations...\", end=\" \")\n",
    "    res = mi_permutation_test(s_tr, y_tr, n_st, n_st, n_perm=N_PERM)\n",
    "    res[\"n_bins\"] = n_bins\n",
    "    res[\"method\"] = method\n",
    "\n",
    "    # State coverage\n",
    "    sc = np.bincount(s_tr, minlength=n_st)\n",
    "    res[\"avg_samples_per_state\"] = sc[sc > 0].mean()\n",
    "    res[\"states_lt5\"] = int((sc < 5).sum())\n",
    "\n",
    "    mi_results.append(res)\n",
    "    print(f\"done.\")\n",
    "    print(f\"  MI_real={res['mi_real']:.6f}, perm_mean={res['perm_mean']:.6f}, \"\n",
    "          f\"perm_std={res['perm_std']:.6f}\")\n",
    "    print(f\"  deltaMI={res['delta_mi_nats']:.6f} nats ({res['delta_mi_bits']:.6f} bits)\")\n",
    "    print(f\"  z-score={res['z_score']:.2f}, p-value={res['p_value']:.4f}\")\n",
    "    if res['p_value'] < 0.01 and res['delta_mi_nats'] > 0.001:\n",
    "        print(f\"  -> Statistically significant signal (p<0.01, deltaMI>0.001)\")\n",
    "    elif res['p_value'] < 0.01:\n",
    "        print(f\"  -> Statistically detectable but economically weak (deltaMI={res['delta_mi_nats']:.6f})\")\n",
    "    else:\n",
    "        print(f\"  -> NOT significant (p={res['p_value']:.4f})\")\n",
    "\n",
    "df_mi = pd.DataFrame(mi_results)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MI PERMUTATION TEST SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "cols = [\"n_bins\", \"method\", \"mi_real\", \"perm_mean\", \"perm_std\",\n",
    "        \"delta_mi_nats\", \"delta_mi_bits\", \"z_score\", \"p_value\",\n",
    "        \"avg_samples_per_state\", \"states_lt5\"]\n",
    "print(df_mi[cols].to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-baselines",
   "metadata": {},
   "source": [
    "## (3) Conditional Baselines — Properly Tuned\n",
    "\n",
    "Three baselines at $k=1$:\n",
    "- **A) Marginal**: $P(y)$ from TRAIN\n",
    "- **B) Additive smoothing**: $P(y|x) = (C[x,y] + \\alpha) / (C[x,.] + \\alpha \\cdot n_{states})$, $\\alpha$ tuned on VAL\n",
    "- **C) Backoff**: $P_{mix}(y|x) = \\lambda_x \\cdot P_{cond}(y|x) + (1-\\lambda_x) \\cdot P_{marg}(y)$,\n",
    "  $\\lambda_x = count(x)/(count(x)+\\tau)$, with $\\alpha$ and $\\tau$ tuned on VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-baselines",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_bins=25: marginal_LL=-3.218758\n",
      "  B) Additive: alpha=10.0, test_LL=-3.216606, delta=+0.002152\n",
      "  C) Backoff:  alpha=10.0, tau=500, test_LL=-3.217329, delta=+0.001429\n",
      "\n",
      "n_bins=35: marginal_LL=-3.554692\n",
      "  B) Additive: alpha=10.0, test_LL=-3.562570, delta=-0.007878\n",
      "  C) Backoff:  alpha=5.0, tau=200, test_LL=-3.555981, delta=-0.001290\n",
      "\n",
      "n_bins=40: marginal_LL=-3.688565\n",
      "  B) Additive: alpha=10.0, test_LL=-3.695031, delta=-0.006467\n",
      "  C) Backoff:  alpha=10.0, tau=100, test_LL=-3.689438, delta=-0.000873\n",
      "\n",
      "n_bins=55: marginal_LL=-3.692749\n",
      "  B) Additive: alpha=1.0, test_LL=-3.891563, delta=-0.198814\n",
      "  C) Backoff:  alpha=0.001, tau=500, test_LL=-3.690809, delta=+0.001940\n",
      "\n",
      "==============================================================================================================\n",
      "BASELINE COMPARISON (k=1, TEST)\n",
      "==============================================================================================================\n",
      " n_bins         method  marginal_LL_test  additive_alpha  additive_LL_test  additive_delta  additive_acc  backoff_alpha  backoff_tau  backoff_LL_test  backoff_delta  backoff_acc\n",
      "     25       quantile         -3.218758       10.000000         -3.216606        0.002152      0.050562      10.000000          500        -3.217329       0.001429     0.058989\n",
      "     35       quantile         -3.554692       10.000000         -3.562570       -0.007878      0.025281       5.000000          200        -3.555981      -0.001290     0.022472\n",
      "     40       quantile         -3.688565       10.000000         -3.695031       -0.006467      0.022472      10.000000          100        -3.689438      -0.000873     0.019663\n",
      "     55 original_fixed         -3.692749        1.000000         -3.891563       -0.198814      0.042135       0.001000          500        -3.690809       0.001940     0.039326\n",
      "==============================================================================================================\n",
      "\n",
      "Best backoff delta: +0.001940 nats at n_bins=55\n",
      "  Backoff baseline BEATS marginal on test.\n"
     ]
    }
   ],
   "source": [
    "ALPHA_GRID = [0, 1e-3, 1e-2, 1e-1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "TAU_GRID   = [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for n_bins in N_BINS_LIST:\n",
    "    s_all, y_all_rb, n_st, method = prepare_bins(\n",
    "        n_bins, pct_forward_all, pct_backward_all,\n",
    "        pct_fwd_train, pct_bwd_train, s_curr_all, y_all)\n",
    "    s_tr = s_all[idx_train]; s_va = s_all[idx_val]; s_te = s_all[idx_test]\n",
    "    y_tr = y_all_rb[idx_train]; y_va = y_all_rb[idx_val]; y_te = y_all_rb[idx_test]\n",
    "\n",
    "    marginal = compute_marginal(y_tr, n_st)\n",
    "\n",
    "    # --- Baseline A: Marginal ---\n",
    "    pred_marg_val  = np.tile(marginal, (len(y_va), 1))\n",
    "    pred_marg_test = np.tile(marginal, (len(y_te), 1))\n",
    "    ll_marg_val  = mean_log_likelihood(pred_marg_val, y_va)\n",
    "    ll_marg_test = mean_log_likelihood(pred_marg_test, y_te)\n",
    "\n",
    "    # --- Baseline B: Additive smoothing, alpha tuned on VAL ---\n",
    "    best_alpha, best_alpha_ll = None, -np.inf\n",
    "    for alpha in ALPHA_GRID:\n",
    "        if alpha == 0:\n",
    "            # Zero smoothing: skip states with 0 counts (assign marginal)\n",
    "            C = compute_joint_counts(s_tr, y_tr, n_st, n_st)\n",
    "            row_sums = C.sum(axis=1)\n",
    "            P_cond = np.zeros_like(C)\n",
    "            for i in range(n_st):\n",
    "                if row_sums[i] > 0:\n",
    "                    P_cond[i] = C[i] / row_sums[i]\n",
    "                else:\n",
    "                    P_cond[i] = marginal\n",
    "        else:\n",
    "            P_cond = compute_conditional_additive(s_tr, y_tr, n_st, n_st, alpha)\n",
    "        pred_val = P_cond[s_va]\n",
    "        ll_val = mean_log_likelihood(pred_val, y_va)\n",
    "        if ll_val > best_alpha_ll:\n",
    "            best_alpha_ll = ll_val\n",
    "            best_alpha = alpha\n",
    "\n",
    "    # Evaluate best alpha on test\n",
    "    if best_alpha == 0:\n",
    "        C = compute_joint_counts(s_tr, y_tr, n_st, n_st)\n",
    "        row_sums = C.sum(axis=1)\n",
    "        P_cond_best = np.zeros_like(C)\n",
    "        for i in range(n_st):\n",
    "            P_cond_best[i] = C[i] / row_sums[i] if row_sums[i] > 0 else marginal\n",
    "    else:\n",
    "        P_cond_best = compute_conditional_additive(s_tr, y_tr, n_st, n_st, best_alpha)\n",
    "    ll_additive_test = mean_log_likelihood(P_cond_best[s_te], y_te)\n",
    "    acc_additive_test = accuracy(P_cond_best[s_te], y_te)\n",
    "\n",
    "    # --- Baseline C: Backoff, 2-stage tuning ---\n",
    "    # Stage 1: fix alpha = best from B, tune tau\n",
    "    best_tau, best_tau_ll = None, -np.inf\n",
    "    alpha_for_backoff = best_alpha if best_alpha > 0 else 1e-3\n",
    "    for tau in TAU_GRID:\n",
    "        pred_val = compute_backoff_baseline(\n",
    "            s_tr, y_tr, s_va, n_st, n_st, alpha_for_backoff, tau, marginal)\n",
    "        ll_val = mean_log_likelihood(pred_val, y_va)\n",
    "        if ll_val > best_tau_ll:\n",
    "            best_tau_ll = ll_val\n",
    "            best_tau = tau\n",
    "\n",
    "    # Stage 2: refine alpha with best tau\n",
    "    best_backoff_alpha, best_backoff_ll = alpha_for_backoff, best_tau_ll\n",
    "    for alpha in ALPHA_GRID:\n",
    "        if alpha == 0:\n",
    "            alpha = 1e-3  # avoid division issues\n",
    "        pred_val = compute_backoff_baseline(\n",
    "            s_tr, y_tr, s_va, n_st, n_st, alpha, best_tau, marginal)\n",
    "        ll_val = mean_log_likelihood(pred_val, y_va)\n",
    "        if ll_val > best_backoff_ll:\n",
    "            best_backoff_ll = ll_val\n",
    "            best_backoff_alpha = alpha\n",
    "\n",
    "    # Evaluate backoff on test\n",
    "    pred_backoff_test = compute_backoff_baseline(\n",
    "        s_tr, y_tr, s_te, n_st, n_st, best_backoff_alpha, best_tau, marginal)\n",
    "    ll_backoff_test = mean_log_likelihood(pred_backoff_test, y_te)\n",
    "    acc_backoff_test = accuracy(pred_backoff_test, y_te)\n",
    "\n",
    "    delta_additive = ll_additive_test - ll_marg_test\n",
    "    delta_backoff  = ll_backoff_test - ll_marg_test\n",
    "\n",
    "    print(f\"\\nn_bins={n_bins}: marginal_LL={ll_marg_test:.6f}\")\n",
    "    print(f\"  B) Additive: alpha={best_alpha}, test_LL={ll_additive_test:.6f}, delta={delta_additive:+.6f}\")\n",
    "    print(f\"  C) Backoff:  alpha={best_backoff_alpha}, tau={best_tau}, \"\n",
    "          f\"test_LL={ll_backoff_test:.6f}, delta={delta_backoff:+.6f}\")\n",
    "\n",
    "    baseline_results.append({\n",
    "        \"n_bins\": n_bins, \"method\": method,\n",
    "        \"marginal_LL_test\": ll_marg_test,\n",
    "        \"additive_alpha\": best_alpha,\n",
    "        \"additive_LL_test\": ll_additive_test,\n",
    "        \"additive_delta\": delta_additive,\n",
    "        \"additive_acc\": acc_additive_test,\n",
    "        \"backoff_alpha\": best_backoff_alpha,\n",
    "        \"backoff_tau\": best_tau,\n",
    "        \"backoff_LL_test\": ll_backoff_test,\n",
    "        \"backoff_delta\": delta_backoff,\n",
    "        \"backoff_acc\": acc_backoff_test,\n",
    "    })\n",
    "\n",
    "df_baselines = pd.DataFrame(baseline_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"BASELINE COMPARISON (k=1, TEST)\")\n",
    "print(\"=\" * 110)\n",
    "print(df_baselines.to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "print(\"=\" * 110)\n",
    "\n",
    "# Identify best configuration\n",
    "best_row = df_baselines.loc[df_baselines[\"backoff_delta\"].idxmax()]\n",
    "print(f\"\\nBest backoff delta: {best_row['backoff_delta']:+.6f} nats at n_bins={int(best_row['n_bins'])}\")\n",
    "if best_row['backoff_delta'] > 0:\n",
    "    print(\"  Backoff baseline BEATS marginal on test.\")\n",
    "else:\n",
    "    print(\"  Backoff baseline does NOT beat marginal on test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-multistep",
   "metadata": {},
   "source": [
    "## (4) Multi-Step Evaluation ($k > 1$)\n",
    "\n",
    "For stationary baselines:\n",
    "- **Marginal**: $\\hat{\\pi}_{t+k} = P_{marg}(y)$ for all $k$.\n",
    "- **Backoff**: $\\hat{\\pi}_{t+k} = e_{x_t} \\cdot A^k$ where $A$ is the transition matrix from the backoff baseline, evaluated at each state.\n",
    "\n",
    "Note: $A^k$ is the matrix power of the $k$-step transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-multistep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_bins=55 (original_fixed), alpha=0.001, tau=500.0\n",
      "  k= 1: n=355, marg_LL=-3.692661, backoff_LL=-3.687368, delta=+0.005294\n",
      "  k= 2: n=354, marg_LL=-3.693314, backoff_LL=-3.692620, delta=+0.000694\n",
      "  k= 3: n=353, marg_LL=-3.694515, backoff_LL=-3.693811, delta=+0.000704\n",
      "  k= 5: n=351, marg_LL=-3.693570, backoff_LL=-3.692869, delta=+0.000701\n",
      "  k=10: n=346, marg_LL=-3.698555, backoff_LL=-3.697894, delta=+0.000661\n",
      "\n",
      "==========================================================================================\n",
      "MULTI-STEP EVALUATION SUMMARY\n",
      "==========================================================================================\n",
      " n_bins  k  n_valid  marginal_LL  backoff_LL  delta_LL\n",
      "     55  1      355    -3.692661   -3.687368  0.005294\n",
      "     55  2      354    -3.693314   -3.692620  0.000694\n",
      "     55  3      353    -3.694515   -3.693811  0.000704\n",
      "     55  5      351    -3.693570   -3.692869  0.000701\n",
      "     55 10      346    -3.698555   -3.697894  0.000661\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "K_LIST = [1, 2, 3, 5, 10]\n",
    "\n",
    "# Use the best n_bins configuration from the backoff results\n",
    "best_nb = int(df_baselines.loc[df_baselines[\"backoff_delta\"].idxmax(), \"n_bins\"])\n",
    "best_alpha_ms = float(df_baselines.loc[df_baselines[\"backoff_delta\"].idxmax(), \"backoff_alpha\"])\n",
    "best_tau_ms   = float(df_baselines.loc[df_baselines[\"backoff_delta\"].idxmax(), \"backoff_tau\"])\n",
    "\n",
    "# Also run for n_bins=55 (original) for comparison\n",
    "multistep_nbins_list = sorted(set([best_nb, 55]))\n",
    "\n",
    "multistep_results = []\n",
    "\n",
    "for n_bins in multistep_nbins_list:\n",
    "    s_all, y_all_rb, n_st, method = prepare_bins(\n",
    "        n_bins, pct_forward_all, pct_backward_all,\n",
    "        pct_fwd_train, pct_bwd_train, s_curr_all, y_all)\n",
    "    s_tr = s_all[idx_train]; y_tr = y_all_rb[idx_train]\n",
    "\n",
    "    marginal = compute_marginal(y_tr, n_st)\n",
    "\n",
    "    # Build transition matrix A for backoff baseline\n",
    "    # Use the tuned hyperparams from the best config, or re-tune for this n_bins\n",
    "    row = df_baselines[df_baselines[\"n_bins\"] == n_bins]\n",
    "    if len(row) > 0:\n",
    "        alpha_ms = float(row[\"backoff_alpha\"].values[0])\n",
    "        tau_ms   = float(row[\"backoff_tau\"].values[0])\n",
    "    else:\n",
    "        alpha_ms = best_alpha_ms\n",
    "        tau_ms   = best_tau_ms\n",
    "\n",
    "    # Build A: each row A[x,:] = lambda_x * P_cond(y|x) + (1-lambda_x) * P_marg(y)\n",
    "    C = compute_joint_counts(s_tr, y_tr, n_st, n_st)\n",
    "    C_alpha = C + alpha_ms\n",
    "    P_cond = C_alpha / C_alpha.sum(axis=1, keepdims=True)\n",
    "    state_counts = C.sum(axis=1)\n",
    "    lam = state_counts / (state_counts + tau_ms)\n",
    "    A = np.zeros((n_st, n_st), dtype=np.float64)\n",
    "    for i in range(n_st):\n",
    "        A[i] = lam[i] * P_cond[i] + (1 - lam[i]) * marginal\n",
    "\n",
    "    print(f\"\\nn_bins={n_bins} ({method}), alpha={alpha_ms}, tau={tau_ms}\")\n",
    "\n",
    "    for k in K_LIST:\n",
    "        # Need k-step-ahead pairs: (s_t, y_{t+k})\n",
    "        # From test set: indices val_end to T-1, we need y at t+k\n",
    "        # But y_{t+k} requires index t+k to exist\n",
    "        valid_mask = (idx_test + k) < T\n",
    "        idx_t = idx_test[valid_mask]\n",
    "        idx_tk = idx_t + k\n",
    "        s_t = s_all[idx_t]\n",
    "        y_tk = y_all_rb[idx_tk]\n",
    "        n_valid = len(s_t)\n",
    "\n",
    "        if n_valid < 10:\n",
    "            print(f\"  k={k}: too few samples ({n_valid}), skipping\")\n",
    "            continue\n",
    "\n",
    "        # Marginal prediction\n",
    "        pred_marg = np.tile(marginal, (n_valid, 1))\n",
    "        ll_marg = mean_log_likelihood(pred_marg, y_tk)\n",
    "\n",
    "        # Backoff k-step: pi_{t+k} = e_{s_t} @ A^k\n",
    "        Ak = np.linalg.matrix_power(A, k)\n",
    "        pred_backoff = Ak[s_t]\n",
    "        ll_backoff = mean_log_likelihood(pred_backoff, y_tk)\n",
    "\n",
    "        delta = ll_backoff - ll_marg\n",
    "\n",
    "        multistep_results.append({\n",
    "            \"n_bins\": n_bins, \"k\": k, \"n_valid\": n_valid,\n",
    "            \"marginal_LL\": ll_marg, \"backoff_LL\": ll_backoff,\n",
    "            \"delta_LL\": delta,\n",
    "        })\n",
    "        print(f\"  k={k:2d}: n={n_valid}, marg_LL={ll_marg:.6f}, \"\n",
    "              f\"backoff_LL={ll_backoff:.6f}, delta={delta:+.6f}\")\n",
    "\n",
    "df_multistep = pd.DataFrame(multistep_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"MULTI-STEP EVALUATION SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(df_multistep.to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-conclusion",
   "metadata": {},
   "source": [
    "## (5) Final Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-conclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "FINAL CONCLUSION — Is there conditional signal beyond marginal?\n",
      "==========================================================================================\n",
      "\n",
      "(i) MI Permutation Test:\n",
      "  n_bins=25: p=0.0020, deltaMI=0.061146 nats, z=5.72 -> SIGNIFICANT signal\n",
      "  n_bins=35: p=0.0020, deltaMI=0.058182 nats, z=4.23 -> SIGNIFICANT signal\n",
      "  n_bins=40: p=0.0020, deltaMI=0.043572 nats, z=3.16 -> SIGNIFICANT signal\n",
      "  n_bins=55: p=0.0100, deltaMI=0.031384 nats, z=2.46 -> SIGNIFICANT signal\n",
      "\n",
      "(ii) Tuned Backoff Baseline (k=1, TEST):\n",
      "  n_bins=25: best delta=+0.002152 nats (additive) -> BEATS marginal\n",
      "  n_bins=35: best delta=-0.001290 nats (backoff) -> does NOT beat marginal\n",
      "  n_bins=40: best delta=-0.000873 nats (backoff) -> does NOT beat marginal\n",
      "  n_bins=55: best delta=+0.001940 nats (backoff) -> BEATS marginal\n",
      "\n",
      "(iii) Multi-Step (k>1):\n",
      "  n_bins=55, k=1: delta=+0.005294 -> BEATS marginal\n",
      "  n_bins=55, k=2: delta=+0.000694 -> BEATS marginal\n",
      "  n_bins=55, k=3: delta=+0.000704 -> BEATS marginal\n",
      "  n_bins=55, k=5: delta=+0.000701 -> BEATS marginal\n",
      "  n_bins=55, k=10: delta=+0.000661 -> BEATS marginal\n",
      "\n",
      "==========================================================================================\n",
      "VERDICT\n",
      "==========================================================================================\n",
      "  YES — Conditional signal EXISTS.\n",
      "  Best improvement: +0.002152 nats/sample at n_bins=25.\n",
      "  MI permutation test confirms statistical significance.\n",
      "  However, the effect is SMALL (0.002152 nats).\n",
      "  This may be too weak for a neural model to reliably capture.\n",
      "\n",
      "  Multi-step note: Some k>1 horizons show positive delta,\n",
      "  suggesting persistence effects at longer horizons.\n",
      "\n",
      "==========================================================================================\n",
      "All results saved to results/diagnostics_v2/ (timestamp: 20260213_223116)\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"results/diagnostics_v2\", exist_ok=True)\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "df_mi.to_csv(f\"results/diagnostics_v2/mi_permutation_{ts}.csv\", index=False)\n",
    "df_baselines.to_csv(f\"results/diagnostics_v2/baselines_k1_{ts}.csv\", index=False)\n",
    "df_multistep.to_csv(f\"results/diagnostics_v2/multistep_{ts}.csv\", index=False)\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"FINAL CONCLUSION — Is there conditional signal beyond marginal?\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# (i) MI permutation test\n",
    "print(\"\\n(i) MI Permutation Test:\")\n",
    "any_mi_sig = False\n",
    "for _, row in df_mi.iterrows():\n",
    "    nb = int(row['n_bins'])\n",
    "    p = row['p_value']\n",
    "    delta = row['delta_mi_nats']\n",
    "    z = row['z_score']\n",
    "    if p < 0.01 and delta > 0.001:\n",
    "        print(f\"  n_bins={nb}: p={p:.4f}, deltaMI={delta:.6f} nats, z={z:.2f} \"\n",
    "              f\"-> SIGNIFICANT signal\")\n",
    "        any_mi_sig = True\n",
    "    elif p < 0.01:\n",
    "        print(f\"  n_bins={nb}: p={p:.4f}, deltaMI={delta:.6f} nats, z={z:.2f} \"\n",
    "              f\"-> Detectable but tiny\")\n",
    "        any_mi_sig = True\n",
    "    else:\n",
    "        print(f\"  n_bins={nb}: p={p:.4f}, deltaMI={delta:.6f} nats, z={z:.2f} \"\n",
    "              f\"-> NOT significant\")\n",
    "\n",
    "# (ii) Tuned backoff baseline\n",
    "print(\"\\n(ii) Tuned Backoff Baseline (k=1, TEST):\")\n",
    "best_delta_ll = -np.inf\n",
    "best_nb_ll = None\n",
    "any_beats_marginal = False\n",
    "for _, row in df_baselines.iterrows():\n",
    "    nb = int(row['n_bins'])\n",
    "    d_add = row['additive_delta']\n",
    "    d_bk  = row['backoff_delta']\n",
    "    best_d = max(d_add, d_bk)\n",
    "    label = 'backoff' if d_bk >= d_add else 'additive'\n",
    "    if best_d > 0:\n",
    "        print(f\"  n_bins={nb}: best delta={best_d:+.6f} nats ({label}) -> BEATS marginal\")\n",
    "        any_beats_marginal = True\n",
    "    else:\n",
    "        print(f\"  n_bins={nb}: best delta={best_d:+.6f} nats ({label}) -> does NOT beat marginal\")\n",
    "    if best_d > best_delta_ll:\n",
    "        best_delta_ll = best_d\n",
    "        best_nb_ll = nb\n",
    "\n",
    "# (iii) Multi-step\n",
    "print(\"\\n(iii) Multi-Step (k>1):\")\n",
    "any_multistep_sig = False\n",
    "for _, row in df_multistep.iterrows():\n",
    "    nb = int(row['n_bins'])\n",
    "    k = int(row['k'])\n",
    "    d = row['delta_LL']\n",
    "    if d > 0:\n",
    "        print(f\"  n_bins={nb}, k={k}: delta={d:+.6f} -> BEATS marginal\")\n",
    "        any_multistep_sig = True\n",
    "    elif k <= 3:  # only report small k\n",
    "        print(f\"  n_bins={nb}, k={k}: delta={d:+.6f}\")\n",
    "\n",
    "# Overall verdict\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "if any_beats_marginal and any_mi_sig:\n",
    "    print(f\"  YES — Conditional signal EXISTS.\")\n",
    "    print(f\"  Best improvement: {best_delta_ll:+.6f} nats/sample at n_bins={best_nb_ll}.\")\n",
    "    print(f\"  MI permutation test confirms statistical significance.\")\n",
    "    if best_delta_ll < 0.01:\n",
    "        print(f\"  However, the effect is SMALL ({best_delta_ll:.6f} nats).\")\n",
    "        print(f\"  This may be too weak for a neural model to reliably capture.\")\n",
    "    else:\n",
    "        print(f\"  Effect size is non-trivial. Neural model should be able to exploit this.\")\n",
    "elif any_mi_sig and not any_beats_marginal:\n",
    "    print(f\"  WEAK — MI shows statistically detectable dependence,\")\n",
    "    print(f\"  but even the best-tuned count-based baseline cannot beat marginal on test.\")\n",
    "    print(f\"  The signal is too weak to be useful for prediction.\")\n",
    "elif any_beats_marginal and not any_mi_sig:\n",
    "    print(f\"  AMBIGUOUS — Backoff baseline beats marginal on test (delta={best_delta_ll:+.6f}),\")\n",
    "    print(f\"  but MI permutation test is not significant. Could be finite-sample artifact.\")\n",
    "else:\n",
    "    print(f\"  NO — No detectable conditional signal.\")\n",
    "    print(f\"  MI permutation test: not significant.\")\n",
    "    print(f\"  Best tuned baseline: delta={best_delta_ll:+.6f} nats (does not beat marginal).\")\n",
    "    print(f\"  The current-state bin carries NO useful information about the next-state bin.\")\n",
    "\n",
    "if any_multistep_sig:\n",
    "    print(f\"\\n  Multi-step note: Some k>1 horizons show positive delta,\")\n",
    "    print(f\"  suggesting persistence effects at longer horizons.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(f\"All results saved to results/diagnostics_v2/ (timestamp: {ts})\")\n",
    "print(\"=\" * 90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat453",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
