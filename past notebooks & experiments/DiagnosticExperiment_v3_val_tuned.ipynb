{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# Diagnostic v3 — Val-Tuned Validation of Conditional Signal\n",
    "\n",
    "**Purpose:** Verify whether the conditional lift reported in v2 is real or an artifact\n",
    "of implicit test-set selection.\n",
    "\n",
    "### What was wrong with v2?\n",
    "In v2, per-`n_bins` hyperparameters (alpha, tau) were correctly tuned on VAL.\n",
    "However, the **selection of which `n_bins` is best** was made by inspecting\n",
    "`backoff_delta` on TEST. This is a subtle form of test-set optimism:\n",
    "when you report \"best delta = +0.002 at n_bins=25\" after looking at all 4 test deltas,\n",
    "you are implicitly picking the luckiest n_bins on test.\n",
    "\n",
    "### Protocol in this notebook\n",
    "1. ALL hyperparameters (alpha, tau) tuned on VALIDATION only.\n",
    "2. **`n_bins` selection** also made on VALIDATION only.\n",
    "3. TEST evaluated **exactly once** with frozen parameters.\n",
    "4. Bootstrap confidence intervals on test delta_LL.\n",
    "5. Multi-step (k>1) with frozen params from step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os, warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "EPS = 1e-12  # only for log(p + EPS), not for MI or smoothing\n",
    "\n",
    "print(\"Imports ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-data",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Same dataset and split as v2 / `TransitionProbMatrix_NEWDATA.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples=2368, n_states_orig=55\n",
      "Train: 1657 [0..1656]\n",
      "Val:   355 [1657..2011]\n",
      "Test:  356 [2012..2367]\n"
     ]
    }
   ],
   "source": [
    "train_df  = pd.read_csv(\"dataset/train_diagnostic.csv\")\n",
    "labels_df = pd.read_csv(\"dataset/label_diagnostic.csv\")\n",
    "\n",
    "# Compute forward percent change from Price\n",
    "train_df[\"Percent_change_forward\"] = (\n",
    "    train_df[\"Price\"].shift(-1) / train_df[\"Price\"] - 1\n",
    ") * 100.0\n",
    "\n",
    "# Drop last row (forward return undefined)\n",
    "train_df = train_df.iloc[:-1].copy()\n",
    "labels_df = labels_df.iloc[:-1].copy()\n",
    "\n",
    "# States: 0-based\n",
    "s_curr_all = (train_df[\"Backward_Bin\"].values.astype(np.int64) - 1)\n",
    "y_all      = (labels_df[\"Forward_Bin\"].values.astype(np.int64) - 1)\n",
    "\n",
    "# Raw percent changes (for rebinning)\n",
    "pct_backward_all = train_df[\"Percent_change_backward\"].values.astype(np.float64)\n",
    "pct_forward_all  = train_df[\"Percent_change_forward\"].values.astype(np.float64)\n",
    "\n",
    "n_samples = len(s_curr_all)\n",
    "n_states_orig = int(max(s_curr_all.max(), y_all.max()) + 1)\n",
    "\n",
    "# Temporal split: 70 / 15 / 15  (identical to v2)\n",
    "T = n_samples\n",
    "train_end = int(0.7 * T)   # 1657\n",
    "val_end   = int(0.85 * T)  # 2012\n",
    "\n",
    "idx_train = np.arange(0,         train_end)\n",
    "idx_val   = np.arange(train_end, val_end)\n",
    "idx_test  = np.arange(val_end,   T)\n",
    "\n",
    "pct_fwd_train = pct_forward_all[idx_train]\n",
    "pct_bwd_train = pct_backward_all[idx_train]\n",
    "\n",
    "print(f\"n_samples={n_samples}, n_states_orig={n_states_orig}\")\n",
    "print(f\"Train: {len(idx_train)} [{idx_train[0]}..{idx_train[-1]}]\")\n",
    "print(f\"Val:   {len(idx_val)} [{idx_val[0]}..{idx_val[-1]}]\")\n",
    "print(f\"Test:  {len(idx_test)} [{idx_test[0]}..{idx_test[-1]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-helpers",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Copied from v2 — no changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-helpers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers defined.\n"
     ]
    }
   ],
   "source": [
    "def compute_marginal(y, n_classes):\n",
    "    \"\"\"Compute marginal distribution from integer labels.\"\"\"\n",
    "    counts = np.bincount(y, minlength=n_classes).astype(np.float64)\n",
    "    return counts / counts.sum()\n",
    "\n",
    "\n",
    "def compute_joint_counts(s, y, n_x, n_y):\n",
    "    \"\"\"Compute raw joint count matrix C[x, y].\"\"\"\n",
    "    C = np.zeros((n_x, n_y), dtype=np.float64)\n",
    "    for si, yi in zip(s, y):\n",
    "        C[si, yi] += 1\n",
    "    return C\n",
    "\n",
    "\n",
    "def compute_conditional_additive(s, y, n_x, n_y, alpha, marginal_fallback=None):\n",
    "    \"\"\"P(y|x) = (C[x,y] + alpha) / (C[x,.] + alpha * n_y).\n",
    "    If alpha=0 and a row has zero counts, use marginal_fallback.\n",
    "    All returned probabilities are strictly > 0 when alpha > 0.\n",
    "    \"\"\"\n",
    "    C = compute_joint_counts(s, y, n_x, n_y)\n",
    "    if alpha == 0:\n",
    "        row_sums = C.sum(axis=1)\n",
    "        P = np.zeros_like(C)\n",
    "        for i in range(n_x):\n",
    "            if row_sums[i] > 0:\n",
    "                P[i] = C[i] / row_sums[i]\n",
    "            elif marginal_fallback is not None:\n",
    "                P[i] = marginal_fallback\n",
    "            else:\n",
    "                P[i] = 1.0 / n_y  # uniform fallback\n",
    "        return P\n",
    "    C_alpha = C + alpha\n",
    "    return C_alpha / C_alpha.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def build_backoff_matrix(s_train, y_train, n_x, n_y, alpha, tau, marginal):\n",
    "    \"\"\"Build backoff transition matrix A[x, y] = lam_x * P_cond(y|x) + (1-lam_x) * P_marg(y)\n",
    "    where lam_x = count(x) / (count(x) + tau).\n",
    "    Returns: A (n_x, n_y), lam (n_x,), P_cond (n_x, n_y)\n",
    "    \"\"\"\n",
    "    C = compute_joint_counts(s_train, y_train, n_x, n_y)\n",
    "    C_alpha = C + alpha\n",
    "    P_cond = C_alpha / C_alpha.sum(axis=1, keepdims=True)\n",
    "    state_counts = C.sum(axis=1)\n",
    "    lam = state_counts / (state_counts + tau)\n",
    "    A = np.zeros((n_x, n_y), dtype=np.float64)\n",
    "    for i in range(n_x):\n",
    "        A[i] = lam[i] * P_cond[i] + (1 - lam[i]) * marginal\n",
    "    return A, lam, P_cond\n",
    "\n",
    "\n",
    "def predict_backoff(A, s_eval):\n",
    "    \"\"\"Return per-sample predicted distributions from backoff matrix.\"\"\"\n",
    "    return A[s_eval]\n",
    "\n",
    "\n",
    "def mean_log_likelihood(pred_dist, y_true):\n",
    "    N = len(y_true)\n",
    "    probs = pred_dist[np.arange(N), y_true]\n",
    "    return np.log(probs + EPS).mean()\n",
    "\n",
    "\n",
    "def accuracy_score(pred_dist, y_true):\n",
    "    return (pred_dist.argmax(axis=1) == y_true).mean()\n",
    "\n",
    "\n",
    "def severity_score(pred_dist, y_true, n_classes):\n",
    "    bins = np.arange(n_classes, dtype=np.float64)\n",
    "    expected = (pred_dist * bins[np.newaxis, :]).sum(axis=1)\n",
    "    return np.abs(expected - y_true.astype(np.float64)).mean()\n",
    "\n",
    "\n",
    "def rebin_quantile(pct_values, pct_train_for_edges, n_bins):\n",
    "    \"\"\"Rebin using quantile edges fit on the given training data.\"\"\"\n",
    "    edges = np.quantile(pct_train_for_edges, np.linspace(0, 1, n_bins + 1))\n",
    "    edges[0] = -np.inf\n",
    "    edges[-1] = np.inf\n",
    "    edges = np.unique(edges)\n",
    "    actual_n = len(edges) - 1\n",
    "    bins = np.clip(np.digitize(pct_values, edges) - 1, 0, actual_n - 1)\n",
    "    return bins, edges, actual_n\n",
    "\n",
    "\n",
    "def prepare_bins(n_bins):\n",
    "    \"\"\"Prepare rebinned states and labels for a given n_bins.\n",
    "    For n_bins=55: original CSV bins.\n",
    "    Otherwise: quantile-based, SEPARATE edges for fwd/bwd, fit on TRAIN only.\n",
    "    \"\"\"\n",
    "    if n_bins == 55:\n",
    "        return s_curr_all.copy(), y_all.copy(), 55, \"original_fixed\"\n",
    "    y_new, _, n_y = rebin_quantile(pct_forward_all, pct_fwd_train, n_bins)\n",
    "    s_new, _, n_s = rebin_quantile(pct_backward_all, pct_bwd_train, n_bins)\n",
    "    actual_n = max(n_y, n_s)\n",
    "    return s_new, y_new, actual_n, \"quantile\"\n",
    "\n",
    "\n",
    "print(\"Helpers defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-tuning",
   "metadata": {},
   "source": [
    "## A+B) Hyperparameter Tuning on VALIDATION Only\n",
    "\n",
    "For each `n_bins` in {25, 35, 40, 55}:\n",
    "1. **Additive baseline**: sweep alpha, pick best by VAL LL.\n",
    "2. **Backoff baseline**: sweep (alpha, tau) jointly, pick best by VAL LL.\n",
    "3. Record VAL LL for both, plus marginal VAL LL.\n",
    "\n",
    "Then select the **overall best (model, n_bins)** configuration by VAL LL.\n",
    "\n",
    "**TEST is not touched in this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_bins=25 (quantile):  marginal_val=-3.218704\n",
      "  Best additive: alpha=10.0, val_LL=-3.231259, delta_val=-0.012555\n",
      "  Best backoff:  alpha=10.0, tau=1000, val_LL=-3.219013, delta_val=-0.000310\n",
      "n_bins=35 (quantile):  marginal_val=-3.554669\n",
      "  Best additive: alpha=10.0, val_LL=-3.557619, delta_val=-0.002949\n",
      "  Best backoff:  alpha=0.1, tau=1000, val_LL=-3.554212, delta_val=+0.000458\n",
      "n_bins=40 (quantile):  marginal_val=-3.688670\n",
      "  Best additive: alpha=10.0, val_LL=-3.690263, delta_val=-0.001593\n",
      "  Best backoff:  alpha=0.1, tau=1000, val_LL=-3.688277, delta_val=+0.000394\n",
      "n_bins=55 (original_fixed):  marginal_val=-3.682208\n",
      "  Best additive: alpha=1.0, val_LL=-3.882289, delta_val=-0.200081\n",
      "  Best backoff:  alpha=1e-06, tau=500, val_LL=-3.678258, delta_val=+0.003950\n",
      "\n",
      "==========================================================================================\n",
      "VAL-SELECTED BEST CONFIGURATIONS (no test data used)\n",
      "==========================================================================================\n",
      "Marginal:  n_bins=25, val_LL=-3.218704\n",
      "Additive:  n_bins=25, alpha=10.0, val_LL=-3.231259, delta_val=-0.012555\n",
      "Backoff:   n_bins=25, alpha=10.0, tau=1000.0, val_LL=-3.219013, delta_val=-0.000310\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "N_BINS_LIST = [25, 35, 40, 55]\n",
    "ALPHA_GRID = [1e-6, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 5.0, 10.0]\n",
    "TAU_GRID   = [10, 50, 100, 200, 500, 1000]\n",
    "\n",
    "tuning_rows = []  # store all (n_bins, model, params, val_LL)\n",
    "\n",
    "for n_bins in N_BINS_LIST:\n",
    "    s_all, y_all_rb, n_st, method = prepare_bins(n_bins)\n",
    "    s_tr = s_all[idx_train]; s_va = s_all[idx_val]\n",
    "    y_tr = y_all_rb[idx_train]; y_va = y_all_rb[idx_val]\n",
    "\n",
    "    marginal = compute_marginal(y_tr, n_st)\n",
    "\n",
    "    # Marginal VAL LL\n",
    "    pred_marg_val = np.tile(marginal, (len(y_va), 1))\n",
    "    ll_marg_val = mean_log_likelihood(pred_marg_val, y_va)\n",
    "\n",
    "    tuning_rows.append({\n",
    "        \"n_bins\": n_bins, \"n_st\": n_st, \"method\": method,\n",
    "        \"model\": \"marginal\", \"alpha\": None, \"tau\": None,\n",
    "        \"val_LL\": ll_marg_val, \"delta_val\": 0.0,\n",
    "    })\n",
    "\n",
    "    # --- Additive: sweep alpha on VAL ---\n",
    "    best_add = {\"val_LL\": -np.inf}\n",
    "    for alpha in ALPHA_GRID:\n",
    "        P_cond = compute_conditional_additive(s_tr, y_tr, n_st, n_st, alpha, marginal)\n",
    "        ll_val = mean_log_likelihood(P_cond[s_va], y_va)\n",
    "        row = {\n",
    "            \"n_bins\": n_bins, \"n_st\": n_st, \"method\": method,\n",
    "            \"model\": \"additive\", \"alpha\": alpha, \"tau\": None,\n",
    "            \"val_LL\": ll_val, \"delta_val\": ll_val - ll_marg_val,\n",
    "        }\n",
    "        tuning_rows.append(row)\n",
    "        if ll_val > best_add[\"val_LL\"]:\n",
    "            best_add = row.copy()\n",
    "\n",
    "    # --- Backoff: sweep (alpha, tau) jointly on VAL ---\n",
    "    best_bk = {\"val_LL\": -np.inf}\n",
    "    for alpha in ALPHA_GRID:\n",
    "        for tau in TAU_GRID:\n",
    "            A, _, _ = build_backoff_matrix(s_tr, y_tr, n_st, n_st, alpha, tau, marginal)\n",
    "            pred_val = A[s_va]\n",
    "            ll_val = mean_log_likelihood(pred_val, y_va)\n",
    "            row = {\n",
    "                \"n_bins\": n_bins, \"n_st\": n_st, \"method\": method,\n",
    "                \"model\": \"backoff\", \"alpha\": alpha, \"tau\": tau,\n",
    "                \"val_LL\": ll_val, \"delta_val\": ll_val - ll_marg_val,\n",
    "            }\n",
    "            tuning_rows.append(row)\n",
    "            if ll_val > best_bk[\"val_LL\"]:\n",
    "                best_bk = row.copy()\n",
    "\n",
    "    print(f\"n_bins={n_bins} ({method}):  marginal_val={ll_marg_val:.6f}\")\n",
    "    print(f\"  Best additive: alpha={best_add['alpha']}, val_LL={best_add['val_LL']:.6f}, \"\n",
    "          f\"delta_val={best_add['delta_val']:+.6f}\")\n",
    "    print(f\"  Best backoff:  alpha={best_bk['alpha']}, tau={best_bk['tau']}, \"\n",
    "          f\"val_LL={best_bk['val_LL']:.6f}, delta_val={best_bk['delta_val']:+.6f}\")\n",
    "\n",
    "df_tuning = pd.DataFrame(tuning_rows)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Select OVERALL BEST configuration by VAL LL (across all n_bins)\n",
    "# ---------------------------------------------------------------\n",
    "# Best additive across all n_bins\n",
    "df_add = df_tuning[df_tuning[\"model\"] == \"additive\"]\n",
    "best_add_overall = df_add.loc[df_add[\"val_LL\"].idxmax()]\n",
    "\n",
    "# Best backoff across all n_bins\n",
    "df_bk = df_tuning[df_tuning[\"model\"] == \"backoff\"]\n",
    "best_bk_overall = df_bk.loc[df_bk[\"val_LL\"].idxmax()]\n",
    "\n",
    "# Best marginal across all n_bins (for reference)\n",
    "df_marg = df_tuning[df_tuning[\"model\"] == \"marginal\"]\n",
    "best_marg_overall = df_marg.loc[df_marg[\"val_LL\"].idxmax()]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"VAL-SELECTED BEST CONFIGURATIONS (no test data used)\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"Marginal:  n_bins={int(best_marg_overall['n_bins'])}, \"\n",
    "      f\"val_LL={best_marg_overall['val_LL']:.6f}\")\n",
    "print(f\"Additive:  n_bins={int(best_add_overall['n_bins'])}, \"\n",
    "      f\"alpha={best_add_overall['alpha']}, \"\n",
    "      f\"val_LL={best_add_overall['val_LL']:.6f}, \"\n",
    "      f\"delta_val={best_add_overall['delta_val']:+.6f}\")\n",
    "print(f\"Backoff:   n_bins={int(best_bk_overall['n_bins'])}, \"\n",
    "      f\"alpha={best_bk_overall['alpha']}, tau={best_bk_overall['tau']}, \"\n",
    "      f\"val_LL={best_bk_overall['val_LL']:.6f}, \"\n",
    "      f\"delta_val={best_bk_overall['delta_val']:+.6f}\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-test",
   "metadata": {},
   "source": [
    "## C) Test Evaluation — Frozen Parameters\n",
    "\n",
    "Evaluate the val-selected configurations on TEST exactly once.\n",
    "Also report ALL n_bins with their val-best params for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "k=1 RESULTS: ALL n_bins, val-tuned params, evaluated on TEST\n",
      "========================================================================================================================\n",
      " n_bins    model     alpha         tau    val_LL   test_LL  delta_test  test_acc  test_sev\n",
      "     25 marginal         -           - -3.218704 -3.218758    0.000000  0.036517  5.924157\n",
      "     25 additive 10.000000           - -3.231259 -3.216606    0.002152  0.050562  5.936565\n",
      "     25  backoff 10.000000 1000.000000 -3.219013 -3.217956    0.000802  0.067416  5.924923\n",
      "     35 marginal         -           - -3.554669 -3.554692    0.000000  0.030899  8.311798\n",
      "     35 additive 10.000000           - -3.557619 -3.562570   -0.007878  0.025281  8.324209\n",
      "     35  backoff  0.100000 1000.000000 -3.554212 -3.555950   -0.001259  0.022472  8.316177\n",
      "     40 marginal         -           - -3.688670 -3.688565    0.000000  0.028090  9.522104\n",
      "     40 additive 10.000000           - -3.690263 -3.695031   -0.006467  0.022472  9.522161\n",
      "     40  backoff  0.100000 1000.000000 -3.688277 -3.689787   -0.001223  0.019663  9.521996\n",
      "     55 marginal         -           - -3.682208 -3.692749    0.000000  0.042135  8.266910\n",
      "     55 additive  1.000000           - -3.882289 -3.891563   -0.198814  0.042135  8.317989\n",
      "     55  backoff  0.000001  500.000000 -3.678258 -3.690809    0.001940  0.039326  8.271117\n",
      "========================================================================================================================\n",
      "\n",
      "Val-selected best additive: n_bins=25, alpha=10.0\n",
      "Val-selected best backoff:  n_bins=25, alpha=10.0, tau=1000.0\n",
      "\n",
      "  Val-best additive -> test delta = +0.002152 nats\n",
      "  Val-best backoff  -> test delta = +0.000802 nats\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Evaluate ALL n_bins with their val-best params on TEST\n",
    "# (for full transparency, not just the globally best n_bins)\n",
    "# ---------------------------------------------------------------\n",
    "test_results = []\n",
    "\n",
    "for n_bins in N_BINS_LIST:\n",
    "    s_all, y_all_rb, n_st, method = prepare_bins(n_bins)\n",
    "    s_tr = s_all[idx_train]; s_va = s_all[idx_val]; s_te = s_all[idx_test]\n",
    "    y_tr = y_all_rb[idx_train]; y_va = y_all_rb[idx_val]; y_te = y_all_rb[idx_test]\n",
    "\n",
    "    marginal = compute_marginal(y_tr, n_st)\n",
    "\n",
    "    # Marginal\n",
    "    pred_marg_val  = np.tile(marginal, (len(y_va), 1))\n",
    "    pred_marg_test = np.tile(marginal, (len(y_te), 1))\n",
    "    ll_marg_val  = mean_log_likelihood(pred_marg_val, y_va)\n",
    "    ll_marg_test = mean_log_likelihood(pred_marg_test, y_te)\n",
    "\n",
    "    test_results.append({\n",
    "        \"n_bins\": n_bins, \"model\": \"marginal\",\n",
    "        \"alpha\": \"-\", \"tau\": \"-\",\n",
    "        \"val_LL\": ll_marg_val, \"test_LL\": ll_marg_test,\n",
    "        \"delta_test\": 0.0,\n",
    "        \"test_acc\": accuracy_score(pred_marg_test, y_te),\n",
    "        \"test_sev\": severity_score(pred_marg_test, y_te, n_st),\n",
    "    })\n",
    "\n",
    "    # Best additive for this n_bins (from val tuning)\n",
    "    sub_add = df_tuning[(df_tuning[\"model\"] == \"additive\") & (df_tuning[\"n_bins\"] == n_bins)]\n",
    "    best_a = sub_add.loc[sub_add[\"val_LL\"].idxmax()]\n",
    "    alpha_a = best_a[\"alpha\"]\n",
    "    P_cond_a = compute_conditional_additive(s_tr, y_tr, n_st, n_st, alpha_a, marginal)\n",
    "    pred_add_val  = P_cond_a[s_va]\n",
    "    pred_add_test = P_cond_a[s_te]\n",
    "    ll_add_val  = mean_log_likelihood(pred_add_val, y_va)\n",
    "    ll_add_test = mean_log_likelihood(pred_add_test, y_te)\n",
    "\n",
    "    test_results.append({\n",
    "        \"n_bins\": n_bins, \"model\": \"additive\",\n",
    "        \"alpha\": alpha_a, \"tau\": \"-\",\n",
    "        \"val_LL\": ll_add_val, \"test_LL\": ll_add_test,\n",
    "        \"delta_test\": ll_add_test - ll_marg_test,\n",
    "        \"test_acc\": accuracy_score(pred_add_test, y_te),\n",
    "        \"test_sev\": severity_score(pred_add_test, y_te, n_st),\n",
    "    })\n",
    "\n",
    "    # Best backoff for this n_bins (from val tuning)\n",
    "    sub_bk = df_tuning[(df_tuning[\"model\"] == \"backoff\") & (df_tuning[\"n_bins\"] == n_bins)]\n",
    "    best_b = sub_bk.loc[sub_bk[\"val_LL\"].idxmax()]\n",
    "    alpha_b = best_b[\"alpha\"]\n",
    "    tau_b   = best_b[\"tau\"]\n",
    "    A_bk, _, _ = build_backoff_matrix(s_tr, y_tr, n_st, n_st, alpha_b, tau_b, marginal)\n",
    "    pred_bk_val  = A_bk[s_va]\n",
    "    pred_bk_test = A_bk[s_te]\n",
    "    ll_bk_val  = mean_log_likelihood(pred_bk_val, y_va)\n",
    "    ll_bk_test = mean_log_likelihood(pred_bk_test, y_te)\n",
    "\n",
    "    test_results.append({\n",
    "        \"n_bins\": n_bins, \"model\": \"backoff\",\n",
    "        \"alpha\": alpha_b, \"tau\": tau_b,\n",
    "        \"val_LL\": ll_bk_val, \"test_LL\": ll_bk_test,\n",
    "        \"delta_test\": ll_bk_test - ll_marg_test,\n",
    "        \"test_acc\": accuracy_score(pred_bk_test, y_te),\n",
    "        \"test_sev\": severity_score(pred_bk_test, y_te, n_st),\n",
    "    })\n",
    "\n",
    "df_test = pd.DataFrame(test_results)\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"k=1 RESULTS: ALL n_bins, val-tuned params, evaluated on TEST\")\n",
    "print(\"=\" * 120)\n",
    "print(df_test.to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Highlight the val-selected best\n",
    "print(f\"\\nVal-selected best additive: n_bins={int(best_add_overall['n_bins'])}, alpha={best_add_overall['alpha']}\")\n",
    "print(f\"Val-selected best backoff:  n_bins={int(best_bk_overall['n_bins'])}, alpha={best_bk_overall['alpha']}, tau={best_bk_overall['tau']}\")\n",
    "\n",
    "# Extract the test delta for the val-selected-best configs\n",
    "# Additive\n",
    "sel_add = df_test[(df_test[\"model\"] == \"additive\") &\n",
    "                  (df_test[\"n_bins\"] == int(best_add_overall[\"n_bins\"]))]\n",
    "delta_add_test = sel_add[\"delta_test\"].values[0]\n",
    "print(f\"\\n  Val-best additive -> test delta = {delta_add_test:+.6f} nats\")\n",
    "\n",
    "# Backoff\n",
    "sel_bk = df_test[(df_test[\"model\"] == \"backoff\") &\n",
    "                 (df_test[\"n_bins\"] == int(best_bk_overall[\"n_bins\"]))]\n",
    "delta_bk_test = sel_bk[\"delta_test\"].values[0]\n",
    "print(f\"  Val-best backoff  -> test delta = {delta_bk_test:+.6f} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-bootstrap",
   "metadata": {},
   "source": [
    "## D) Bootstrap Confidence Intervals on Test delta_LL\n",
    "\n",
    "1000 bootstrap resamples over test indices.\n",
    "Reports 95% CI for delta_LL = conditional_LL - marginal_LL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-bootstrap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_bins=25:\n",
      "  Additive delta: +0.002152  95% CI: [-0.013127, +0.017896]  excludes 0? no\n",
      "  Backoff  delta: +0.000802  95% CI: [-0.000194, +0.001874]  excludes 0? no\n",
      "n_bins=35:\n",
      "  Additive delta: -0.007878  95% CI: [-0.018710, +0.002723]  excludes 0? no\n",
      "  Backoff  delta: -0.001259  95% CI: [-0.005194, +0.002618]  excludes 0? no\n",
      "n_bins=40:\n",
      "  Additive delta: -0.006467  95% CI: [-0.016744, +0.002877]  excludes 0? no\n",
      "  Backoff  delta: -0.001223  95% CI: [-0.005252, +0.002545]  excludes 0? no\n",
      "n_bins=55:\n",
      "  Additive delta: -0.198814  95% CI: [-0.260549, -0.138552]  excludes 0? YES\n",
      "  Backoff  delta: +0.001940  95% CI: [-0.005578, +0.009597]  excludes 0? no\n",
      "\n",
      "==============================================================================================================\n",
      "BOOTSTRAP 95% CI FOR TEST delta_LL (k=1)\n",
      "==============================================================================================================\n",
      " n_bins  add_delta_mean  add_CI_lo  add_CI_hi add_CI_excludes_0  bk_delta_mean  bk_CI_lo  bk_CI_hi bk_CI_excludes_0\n",
      "     25        0.002152  -0.013127   0.017896                no       0.000802 -0.000194  0.001874               no\n",
      "     35       -0.007878  -0.018710   0.002723                no      -0.001259 -0.005194  0.002618               no\n",
      "     40       -0.006467  -0.016744   0.002877                no      -0.001223 -0.005252  0.002545               no\n",
      "     55       -0.198814  -0.260549  -0.138552               YES       0.001940 -0.005578  0.009597               no\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "N_BOOT = 1000\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "boot_results = []\n",
    "\n",
    "for n_bins in N_BINS_LIST:\n",
    "    s_all, y_all_rb, n_st, method = prepare_bins(n_bins)\n",
    "    s_tr = s_all[idx_train]; s_te = s_all[idx_test]\n",
    "    y_tr = y_all_rb[idx_train]; y_te = y_all_rb[idx_test]\n",
    "    N_te = len(y_te)\n",
    "\n",
    "    marginal = compute_marginal(y_tr, n_st)\n",
    "\n",
    "    # Val-best additive for this n_bins\n",
    "    sub_add = df_tuning[(df_tuning[\"model\"] == \"additive\") & (df_tuning[\"n_bins\"] == n_bins)]\n",
    "    alpha_a = sub_add.loc[sub_add[\"val_LL\"].idxmax(), \"alpha\"]\n",
    "    P_cond_a = compute_conditional_additive(s_tr, y_tr, n_st, n_st, alpha_a, marginal)\n",
    "\n",
    "    # Val-best backoff for this n_bins\n",
    "    sub_bk = df_tuning[(df_tuning[\"model\"] == \"backoff\") & (df_tuning[\"n_bins\"] == n_bins)]\n",
    "    best_b = sub_bk.loc[sub_bk[\"val_LL\"].idxmax()]\n",
    "    alpha_b, tau_b = best_b[\"alpha\"], best_b[\"tau\"]\n",
    "    A_bk, _, _ = build_backoff_matrix(s_tr, y_tr, n_st, n_st, alpha_b, tau_b, marginal)\n",
    "\n",
    "    # Per-sample log-likelihoods on test\n",
    "    ll_marg_per = np.log(marginal[y_te] + EPS)\n",
    "    ll_add_per  = np.log(P_cond_a[s_te, y_te] + EPS)\n",
    "    ll_bk_per   = np.log(A_bk[s_te, y_te] + EPS)\n",
    "\n",
    "    # Per-sample deltas\n",
    "    delta_add_per = ll_add_per - ll_marg_per\n",
    "    delta_bk_per  = ll_bk_per  - ll_marg_per\n",
    "\n",
    "    # Bootstrap\n",
    "    boot_add = np.zeros(N_BOOT)\n",
    "    boot_bk  = np.zeros(N_BOOT)\n",
    "    for b in range(N_BOOT):\n",
    "        idx_b = rng.randint(0, N_te, size=N_te)\n",
    "        boot_add[b] = delta_add_per[idx_b].mean()\n",
    "        boot_bk[b]  = delta_bk_per[idx_b].mean()\n",
    "\n",
    "    ci_add = np.percentile(boot_add, [2.5, 97.5])\n",
    "    ci_bk  = np.percentile(boot_bk, [2.5, 97.5])\n",
    "\n",
    "    boot_results.append({\n",
    "        \"n_bins\": n_bins,\n",
    "        \"add_delta_mean\": delta_add_per.mean(),\n",
    "        \"add_CI_lo\": ci_add[0], \"add_CI_hi\": ci_add[1],\n",
    "        \"add_CI_excludes_0\": \"YES\" if (ci_add[0] > 0 or ci_add[1] < 0) else \"no\",\n",
    "        \"bk_delta_mean\": delta_bk_per.mean(),\n",
    "        \"bk_CI_lo\": ci_bk[0], \"bk_CI_hi\": ci_bk[1],\n",
    "        \"bk_CI_excludes_0\": \"YES\" if (ci_bk[0] > 0 or ci_bk[1] < 0) else \"no\",\n",
    "    })\n",
    "    print(f\"n_bins={n_bins}:\")\n",
    "    print(f\"  Additive delta: {delta_add_per.mean():+.6f}  95% CI: [{ci_add[0]:+.6f}, {ci_add[1]:+.6f}]  \"\n",
    "          f\"excludes 0? {'YES' if (ci_add[0] > 0 or ci_add[1] < 0) else 'no'}\")\n",
    "    print(f\"  Backoff  delta: {delta_bk_per.mean():+.6f}  95% CI: [{ci_bk[0]:+.6f}, {ci_bk[1]:+.6f}]  \"\n",
    "          f\"excludes 0? {'YES' if (ci_bk[0] > 0 or ci_bk[1] < 0) else 'no'}\")\n",
    "\n",
    "df_boot = pd.DataFrame(boot_results)\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"BOOTSTRAP 95% CI FOR TEST delta_LL (k=1)\")\n",
    "print(\"=\" * 110)\n",
    "print(df_boot.to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "print(\"=\" * 110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-multistep",
   "metadata": {},
   "source": [
    "## E) Multi-Step Evaluation ($k \\in \\{1,2,3,5,10\\}$)\n",
    "\n",
    "Using val-frozen hyperparameters. Reports both VAL and TEST LL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-multistep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "MULTI-STEP EVALUATION (val-tuned params, reported on both VAL and TEST)\n",
      "========================================================================================================================\n",
      " n_bins  k split  n_valid  marginal_LL  backoff_LL  delta_LL     alpha         tau\n",
      "     25  1   val      355    -3.218704   -3.219013 -0.000310 10.000000 1000.000000\n",
      "     25  1  test      356    -3.218758   -3.217956  0.000802 10.000000 1000.000000\n",
      "     25  2   val      355    -3.218704   -3.218710 -0.000006 10.000000 1000.000000\n",
      "     25  2  test      355    -3.218746   -3.218750 -0.000004 10.000000 1000.000000\n",
      "     25  3   val      355    -3.218704   -3.218714 -0.000010 10.000000 1000.000000\n",
      "     25  3  test      354    -3.218734   -3.218740 -0.000006 10.000000 1000.000000\n",
      "     25  5   val      355    -3.218704   -3.218714 -0.000010 10.000000 1000.000000\n",
      "     25  5  test      352    -3.218752   -3.218757 -0.000006 10.000000 1000.000000\n",
      "     25 10   val      355    -3.218704   -3.218714 -0.000010 10.000000 1000.000000\n",
      "     25 10  test      347    -3.218776   -3.218780 -0.000004 10.000000 1000.000000\n",
      "     35  1   val      355    -3.554669   -3.554212  0.000458  0.100000 1000.000000\n",
      "     35  1  test      356    -3.554692   -3.555950 -0.001259  0.100000 1000.000000\n",
      "     35  2   val      355    -3.554669   -3.554607  0.000062  0.100000 1000.000000\n",
      "     35  2  test      355    -3.554669   -3.554658  0.000012  0.100000 1000.000000\n",
      "     35  3   val      355    -3.554669   -3.554670 -0.000000  0.100000 1000.000000\n",
      "     35  3  test      354    -3.554647   -3.554653 -0.000006  0.100000 1000.000000\n",
      "     35  5   val      355    -3.554669   -3.554670 -0.000001  0.100000 1000.000000\n",
      "     35  5  test      352    -3.554662   -3.554668 -0.000006  0.100000 1000.000000\n",
      "     35 10   val      355    -3.554492   -3.554493 -0.000002  0.100000 1000.000000\n",
      "     35 10  test      347    -3.554790   -3.554795 -0.000006  0.100000 1000.000000\n",
      "     40  1   val      355    -3.688670   -3.688277  0.000394  0.100000 1000.000000\n",
      "     40  1  test      356    -3.688565   -3.689787 -0.001223  0.100000 1000.000000\n",
      "     40  2   val      355    -3.688670   -3.688625  0.000046  0.100000 1000.000000\n",
      "     40  2  test      355    -3.688603   -3.688580  0.000022  0.100000 1000.000000\n",
      "     40  3   val      355    -3.688738   -3.688750 -0.000011  0.100000 1000.000000\n",
      "     40  3  test      354    -3.688573   -3.688578 -0.000005  0.100000 1000.000000\n",
      "     40  5   val      355    -3.688738   -3.688749 -0.000011  0.100000 1000.000000\n",
      "     40  5  test      352    -3.688512   -3.688518 -0.000006  0.100000 1000.000000\n",
      "     40 10   val      355    -3.688806   -3.688818 -0.000012  0.100000 1000.000000\n",
      "     40 10  test      347    -3.688497   -3.688503 -0.000005  0.100000 1000.000000\n",
      "     55  1   val      355    -3.682208   -3.678258  0.003950  0.000001  500.000000\n",
      "     55  1  test      356    -3.692749   -3.690809  0.001940  0.000001  500.000000\n",
      "     55  2   val      355    -3.682662   -3.681774  0.000889  0.000001  500.000000\n",
      "     55  2  test      355    -3.692661   -3.691944  0.000718  0.000001  500.000000\n",
      "     55  3   val      355    -3.682662   -3.681827  0.000836  0.000001  500.000000\n",
      "     55  3  test      354    -3.693314   -3.692589  0.000726  0.000001  500.000000\n",
      "     55  5   val      355    -3.678990   -3.678133  0.000857  0.000001  500.000000\n",
      "     55  5  test      352    -3.695487   -3.694783  0.000704  0.000001  500.000000\n",
      "     55 10   val      355    -3.677331   -3.676484  0.000847  0.000001  500.000000\n",
      "     55 10  test      347    -3.697557   -3.696884  0.000673  0.000001  500.000000\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "K_LIST = [1, 2, 3, 5, 10]\n",
    "multistep_results = []\n",
    "\n",
    "for n_bins in N_BINS_LIST:\n",
    "    s_all, y_all_rb, n_st, method = prepare_bins(n_bins)\n",
    "    s_tr = s_all[idx_train]; y_tr = y_all_rb[idx_train]\n",
    "\n",
    "    marginal = compute_marginal(y_tr, n_st)\n",
    "\n",
    "    # Val-best backoff for this n_bins\n",
    "    sub_bk = df_tuning[(df_tuning[\"model\"] == \"backoff\") & (df_tuning[\"n_bins\"] == n_bins)]\n",
    "    best_b = sub_bk.loc[sub_bk[\"val_LL\"].idxmax()]\n",
    "    alpha_b, tau_b = best_b[\"alpha\"], best_b[\"tau\"]\n",
    "\n",
    "    A_bk, _, _ = build_backoff_matrix(s_tr, y_tr, n_st, n_st, alpha_b, tau_b, marginal)\n",
    "\n",
    "    for k in K_LIST:\n",
    "        Ak = np.linalg.matrix_power(A_bk, k)\n",
    "\n",
    "        # y_all_rb[t] = Forward_Bin at t = state at t+1.\n",
    "        # So \"true state at absolute time t+k\" = y_all_rb[t + k - 1]\n",
    "        # Given anchor at time t (where s_all[t] is current state),\n",
    "        # the realized state k steps ahead = y_all_rb[t + k - 1].\n",
    "        # But the transition A maps from s_t to s_{t+1} = y_all_rb[t],\n",
    "        # so A^k maps s_t to s_{t+k}. The realized s_{t+k} = y_all_rb[t+k-1].\n",
    "        # Wait — let's be precise:\n",
    "        #   s_all[t] = Backward_Bin[t] = state of return (P_{t-1} -> P_t)\n",
    "        #   y_all_rb[t] = Forward_Bin[t] = state of return (P_t -> P_{t+1})\n",
    "        #   Because Forward_Bin[t] == Backward_Bin[t+1] (verified in v2),\n",
    "        #   y_all_rb[t] == s_all[t+1].\n",
    "        #   So A maps s_all[t] -> y_all_rb[t] = s_all[t+1].\n",
    "        #   A^k maps s_all[t] -> s_all[t+k] = y_all_rb[t+k-1].\n",
    "        #   True label for k-step = y_all_rb[t + k - 1].\n",
    "\n",
    "        for split_name, idx_split in [(\"val\", idx_val), (\"test\", idx_test)]:\n",
    "            # For k-step: anchor at t, target at t+k-1 in y_all_rb\n",
    "            # We need t + k - 1 < T  (i.e., y_all_rb[t+k-1] exists)\n",
    "            if k == 1:\n",
    "                # A^1 maps s_t -> y_all_rb[t] (same-row target)\n",
    "                valid = idx_split  # all valid since y_all_rb[t] always exists\n",
    "                s_anchor = s_all[valid]\n",
    "                y_target = y_all_rb[valid]\n",
    "            else:\n",
    "                # A^k maps s_t -> y_all_rb[t + k - 1]\n",
    "                valid_mask = (idx_split + k - 1) < T\n",
    "                valid = idx_split[valid_mask]\n",
    "                s_anchor = s_all[valid]\n",
    "                y_target = y_all_rb[valid + k - 1]\n",
    "\n",
    "            n_valid = len(s_anchor)\n",
    "            if n_valid < 10:\n",
    "                continue\n",
    "\n",
    "            # Marginal\n",
    "            pred_marg = np.tile(marginal, (n_valid, 1))\n",
    "            ll_marg = mean_log_likelihood(pred_marg, y_target)\n",
    "\n",
    "            # Backoff k-step\n",
    "            pred_bk = Ak[s_anchor]\n",
    "            ll_bk = mean_log_likelihood(pred_bk, y_target)\n",
    "\n",
    "            multistep_results.append({\n",
    "                \"n_bins\": n_bins, \"k\": k, \"split\": split_name,\n",
    "                \"n_valid\": n_valid,\n",
    "                \"marginal_LL\": ll_marg, \"backoff_LL\": ll_bk,\n",
    "                \"delta_LL\": ll_bk - ll_marg,\n",
    "                \"alpha\": alpha_b, \"tau\": tau_b,\n",
    "            })\n",
    "\n",
    "df_ms = pd.DataFrame(multistep_results)\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"MULTI-STEP EVALUATION (val-tuned params, reported on both VAL and TEST)\")\n",
    "print(\"=\" * 120)\n",
    "display_cols = [\"n_bins\", \"k\", \"split\", \"n_valid\", \"marginal_LL\", \"backoff_LL\", \"delta_LL\", \"alpha\", \"tau\"]\n",
    "print(df_ms[display_cols].to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-summary",
   "metadata": {},
   "source": [
    "## F) Final Summary Table + Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================================\n",
      "SUMMARY TABLE (val-tuned hyperparameters, test evaluated once)\n",
      "==================================================================================================================================\n",
      "   model  n_bins  k     alpha         tau    val_LL   test_LL  delta_test     CI_lo     CI_hi\n",
      "marginal      25  1         -           - -3.218704 -3.218758    0.000000         -         -\n",
      "additive      25  1 10.000000           - -3.231259 -3.216606    0.002152 -0.013127  0.017896\n",
      " backoff      25  1 10.000000 1000.000000 -3.219013 -3.217956    0.000802 -0.000194  0.001874\n",
      "marginal      35  1         -           - -3.554669 -3.554692    0.000000         -         -\n",
      "additive      35  1 10.000000           - -3.557619 -3.562570   -0.007878 -0.018710  0.002723\n",
      " backoff      35  1  0.100000 1000.000000 -3.554212 -3.555950   -0.001259 -0.005194  0.002618\n",
      "marginal      40  1         -           - -3.688670 -3.688565    0.000000         -         -\n",
      "additive      40  1 10.000000           - -3.690263 -3.695031   -0.006467 -0.016744  0.002877\n",
      " backoff      40  1  0.100000 1000.000000 -3.688277 -3.689787   -0.001223 -0.005252  0.002545\n",
      "marginal      55  1         -           - -3.682208 -3.692749    0.000000         -         -\n",
      "additive      55  1  1.000000           - -3.882289 -3.891563   -0.198814 -0.260549 -0.138552\n",
      " backoff      55  1  0.000001  500.000000 -3.678258 -3.690809    0.001940 -0.005578  0.009597\n",
      " backoff      25  2 10.000000 1000.000000 -3.218710 -3.218750   -0.000004         -         -\n",
      " backoff      25  3 10.000000 1000.000000 -3.218714 -3.218740   -0.000006         -         -\n",
      " backoff      25  5 10.000000 1000.000000 -3.218714 -3.218757   -0.000006         -         -\n",
      " backoff      25 10 10.000000 1000.000000 -3.218714 -3.218780   -0.000004         -         -\n",
      " backoff      35  2  0.100000 1000.000000 -3.554607 -3.554658    0.000012         -         -\n",
      " backoff      35  3  0.100000 1000.000000 -3.554670 -3.554653   -0.000006         -         -\n",
      " backoff      35  5  0.100000 1000.000000 -3.554670 -3.554668   -0.000006         -         -\n",
      " backoff      35 10  0.100000 1000.000000 -3.554493 -3.554795   -0.000006         -         -\n",
      " backoff      40  2  0.100000 1000.000000 -3.688625 -3.688580    0.000022         -         -\n",
      " backoff      40  3  0.100000 1000.000000 -3.688750 -3.688578   -0.000005         -         -\n",
      " backoff      40  5  0.100000 1000.000000 -3.688749 -3.688518   -0.000006         -         -\n",
      " backoff      40 10  0.100000 1000.000000 -3.688818 -3.688503   -0.000005         -         -\n",
      " backoff      55  2  0.000001  500.000000 -3.681774 -3.691944    0.000718         -         -\n",
      " backoff      55  3  0.000001  500.000000 -3.681827 -3.692589    0.000726         -         -\n",
      " backoff      55  5  0.000001  500.000000 -3.678133 -3.694783    0.000704         -         -\n",
      " backoff      55 10  0.000001  500.000000 -3.676484 -3.696884    0.000673         -         -\n",
      "==================================================================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "FINAL CONCLUSION\n",
      "==========================================================================================\n",
      "\n",
      "[v2 Audit] In DiagnosticExperiment_v2.ipynb:\n",
      "  - alpha and tau were correctly tuned on VALIDATION per n_bins.\n",
      "  - However, the 'best n_bins' was selected by inspecting TEST deltas,\n",
      "    which is a form of implicit test-set selection.\n",
      "  - v3 corrects this by selecting n_bins on VAL only.\n",
      "\n",
      "[k=1 Results]\n",
      "  n_bins=25, additive: delta=+0.002152 nats, 95% CI=[-0.013127, +0.017896] (CI includes 0)\n",
      "  n_bins=25, backoff: delta=+0.000802 nats, 95% CI=[-0.000194, +0.001874] (CI includes 0)\n",
      "  n_bins=55, backoff: delta=+0.001940 nats, 95% CI=[-0.005578, +0.009597] (CI includes 0)\n",
      "\n",
      "[Multi-step]\n",
      "  n_bins=25, k=1: delta=+0.000802 nats\n",
      "  n_bins=35, k=2: delta=+0.000012 nats\n",
      "  n_bins=40, k=2: delta=+0.000022 nats\n",
      "  n_bins=55, k=1: delta=+0.001940 nats\n",
      "  n_bins=55, k=2: delta=+0.000718 nats\n",
      "  n_bins=55, k=3: delta=+0.000726 nats\n",
      "  n_bins=55, k=5: delta=+0.000704 nats\n",
      "  n_bins=55, k=10: delta=+0.000673 nats\n",
      "\n",
      "==========================================================================================\n",
      "VERDICT\n",
      "==========================================================================================\n",
      "  INCONCLUSIVE — Some conditional baselines show positive test delta,\n",
      "  but 95% bootstrap CI includes zero for all of them.\n",
      "  The observed lift is within sampling noise.\n",
      "  Cannot reject the null hypothesis that current state is uninformative.\n",
      "\n",
      "All results saved to results/diagnostics_v3/ (timestamp: 20260213_224427)\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Build single summary table\n",
    "# ---------------------------------------------------------------\n",
    "summary_rows = []\n",
    "\n",
    "for n_bins in N_BINS_LIST:\n",
    "    # k=1 test results with bootstrap CI\n",
    "    boot_row = df_boot[df_boot[\"n_bins\"] == n_bins].iloc[0]\n",
    "\n",
    "    for model_name in [\"marginal\", \"additive\", \"backoff\"]:\n",
    "        test_row = df_test[(df_test[\"n_bins\"] == n_bins) & (df_test[\"model\"] == model_name)].iloc[0]\n",
    "        ms_test_k1 = df_ms[(df_ms[\"n_bins\"] == n_bins) & (df_ms[\"k\"] == 1) & (df_ms[\"split\"] == \"test\")]\n",
    "        ms_val_k1  = df_ms[(df_ms[\"n_bins\"] == n_bins) & (df_ms[\"k\"] == 1) & (df_ms[\"split\"] == \"val\")]\n",
    "\n",
    "        ci_lo, ci_hi = None, None\n",
    "        if model_name == \"additive\":\n",
    "            ci_lo = boot_row[\"add_CI_lo\"]\n",
    "            ci_hi = boot_row[\"add_CI_hi\"]\n",
    "        elif model_name == \"backoff\":\n",
    "            ci_lo = boot_row[\"bk_CI_lo\"]\n",
    "            ci_hi = boot_row[\"bk_CI_hi\"]\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"n_bins\": n_bins,\n",
    "            \"k\": 1,\n",
    "            \"alpha\": test_row[\"alpha\"],\n",
    "            \"tau\": test_row[\"tau\"],\n",
    "            \"val_LL\": test_row[\"val_LL\"],\n",
    "            \"test_LL\": test_row[\"test_LL\"],\n",
    "            \"delta_test\": test_row[\"delta_test\"],\n",
    "            \"CI_lo\": ci_lo if ci_lo is not None else \"-\",\n",
    "            \"CI_hi\": ci_hi if ci_hi is not None else \"-\",\n",
    "        })\n",
    "\n",
    "# Add multi-step for backoff (test only, k>1)\n",
    "for _, ms_row in df_ms[(df_ms[\"split\"] == \"test\") & (df_ms[\"k\"] > 1)].iterrows():\n",
    "    # Find corresponding val row\n",
    "    ms_val = df_ms[(df_ms[\"n_bins\"] == ms_row[\"n_bins\"]) &\n",
    "                   (df_ms[\"k\"] == ms_row[\"k\"]) &\n",
    "                   (df_ms[\"split\"] == \"val\")]\n",
    "    val_ll = ms_val[\"backoff_LL\"].values[0] if len(ms_val) > 0 else None\n",
    "    summary_rows.append({\n",
    "        \"model\": \"backoff\",\n",
    "        \"n_bins\": int(ms_row[\"n_bins\"]),\n",
    "        \"k\": int(ms_row[\"k\"]),\n",
    "        \"alpha\": ms_row[\"alpha\"],\n",
    "        \"tau\": ms_row[\"tau\"],\n",
    "        \"val_LL\": val_ll,\n",
    "        \"test_LL\": ms_row[\"backoff_LL\"],\n",
    "        \"delta_test\": ms_row[\"delta_LL\"],\n",
    "        \"CI_lo\": \"-\", \"CI_hi\": \"-\",\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Save\n",
    "os.makedirs(\"results/diagnostics_v3\", exist_ok=True)\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df_tuning.to_csv(f\"results/diagnostics_v3/tuning_grid_{ts}.csv\", index=False)\n",
    "df_test.to_csv(f\"results/diagnostics_v3/test_k1_{ts}.csv\", index=False)\n",
    "df_boot.to_csv(f\"results/diagnostics_v3/bootstrap_ci_{ts}.csv\", index=False)\n",
    "df_ms.to_csv(f\"results/diagnostics_v3/multistep_{ts}.csv\", index=False)\n",
    "df_summary.to_csv(f\"results/diagnostics_v3/summary_{ts}.csv\", index=False)\n",
    "\n",
    "print(\"=\" * 130)\n",
    "print(\"SUMMARY TABLE (val-tuned hyperparameters, test evaluated once)\")\n",
    "print(\"=\" * 130)\n",
    "print(df_summary.to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\n",
    "print(\"=\" * 130)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# FINAL CONCLUSION\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"FINAL CONCLUSION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# v2 audit\n",
    "print(\"\\n[v2 Audit] In DiagnosticExperiment_v2.ipynb:\")\n",
    "print(\"  - alpha and tau were correctly tuned on VALIDATION per n_bins.\")\n",
    "print(\"  - However, the 'best n_bins' was selected by inspecting TEST deltas,\")\n",
    "print(\"    which is a form of implicit test-set selection.\")\n",
    "print(\"  - v3 corrects this by selecting n_bins on VAL only.\")\n",
    "\n",
    "# Check: does any conditional baseline beat marginal on test, with CI excluding 0?\n",
    "print(\"\\n[k=1 Results]\")\n",
    "any_sig_beat = False\n",
    "any_beat = False\n",
    "for _, brow in df_boot.iterrows():\n",
    "    nb = int(brow[\"n_bins\"])\n",
    "    for model_name, d_mean, ci_lo, ci_hi, excl in [\n",
    "        (\"additive\", brow[\"add_delta_mean\"], brow[\"add_CI_lo\"], brow[\"add_CI_hi\"], brow[\"add_CI_excludes_0\"]),\n",
    "        (\"backoff\",  brow[\"bk_delta_mean\"],  brow[\"bk_CI_lo\"],  brow[\"bk_CI_hi\"],  brow[\"bk_CI_excludes_0\"]),\n",
    "    ]:\n",
    "        if d_mean > 0:\n",
    "            any_beat = True\n",
    "            sig = \" (CI excludes 0)\" if excl == \"YES\" else \" (CI includes 0)\"\n",
    "            print(f\"  n_bins={nb}, {model_name}: delta={d_mean:+.6f} nats, \"\n",
    "                  f\"95% CI=[{ci_lo:+.6f}, {ci_hi:+.6f}]{sig}\")\n",
    "            if excl == \"YES\":\n",
    "                any_sig_beat = True\n",
    "\n",
    "if not any_beat:\n",
    "    print(\"  No conditional baseline beats marginal on test at any n_bins.\")\n",
    "\n",
    "# Multi-step check\n",
    "print(\"\\n[Multi-step]\")\n",
    "ms_test = df_ms[df_ms[\"split\"] == \"test\"]\n",
    "any_ms_beat = False\n",
    "for _, mrow in ms_test[ms_test[\"delta_LL\"] > 0].iterrows():\n",
    "    nb = int(mrow[\"n_bins\"]); k = int(mrow[\"k\"])\n",
    "    print(f\"  n_bins={nb}, k={k}: delta={mrow['delta_LL']:+.6f} nats\")\n",
    "    any_ms_beat = True\n",
    "if not any_ms_beat:\n",
    "    print(\"  No positive delta at any k.\")\n",
    "\n",
    "# Verdict\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "if any_sig_beat:\n",
    "    # Find the significant one\n",
    "    print(\"  YES — At least one conditional baseline beats marginal on test\")\n",
    "    print(\"  with 95% bootstrap CI excluding zero.\")\n",
    "    print(\"  The effect is real under val-tuned hyperparameters.\")\n",
    "    # But is it practically meaningful?\n",
    "    best_d = max(\n",
    "        df_boot[df_boot[\"add_CI_excludes_0\"] == \"YES\"][\"add_delta_mean\"].max() if len(df_boot[df_boot[\"add_CI_excludes_0\"] == \"YES\"]) > 0 else -np.inf,\n",
    "        df_boot[df_boot[\"bk_CI_excludes_0\"] == \"YES\"][\"bk_delta_mean\"].max() if len(df_boot[df_boot[\"bk_CI_excludes_0\"] == \"YES\"]) > 0 else -np.inf,\n",
    "    )\n",
    "    if best_d < 0.01:\n",
    "        print(f\"  However, the effect is TINY ({best_d:.6f} nats/sample).\")\n",
    "        print(\"  Practically: this is unlikely to be exploitable by a neural model.\")\n",
    "    else:\n",
    "        print(f\"  Effect size: {best_d:.6f} nats/sample — potentially exploitable.\")\n",
    "elif any_beat:\n",
    "    print(\"  INCONCLUSIVE — Some conditional baselines show positive test delta,\")\n",
    "    print(\"  but 95% bootstrap CI includes zero for all of them.\")\n",
    "    print(\"  The observed lift is within sampling noise.\")\n",
    "    print(\"  Cannot reject the null hypothesis that current state is uninformative.\")\n",
    "else:\n",
    "    print(\"  NO — No conditional baseline beats marginal on test\")\n",
    "    print(\"  under val-tuned hyperparameters.\")\n",
    "    print(\"  The current-state bin carries no exploitable information about the next-state bin.\")\n",
    "\n",
    "print(f\"\\nAll results saved to results/diagnostics_v3/ (timestamp: {ts})\")\n",
    "print(\"=\" * 90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat453",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
